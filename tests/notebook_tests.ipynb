{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to fix a line\n",
    "def fix_line(line):\n",
    "    return re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', line)\n",
    "\n",
    "# Process the file line by line\n",
    "with open('dblpv13.json', 'r') as infile, open('dblpv13_fixed.json', 'w') as outfile:\n",
    "    for line in infile:\n",
    "        outfile.write(fix_line(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39mexcept\u001b[39;00m ijson\u001b[39m.\u001b[39mJSONError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid JSON file: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m validate_large_json(\u001b[39m'\u001b[39;49m\u001b[39mdblpv13_fixed.json\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 3\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m parser \u001b[39m=\u001b[39m ijson\u001b[39m.\u001b[39mparse(f)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m prefix, event, value \u001b[39min\u001b[39;00m parser:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mThe JSON file is valid.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\myenv2\\Lib\\site-packages\\ijson\\utils.py:53\u001b[0m, in \u001b[0;36mcoros2gen\u001b[1;34m(source, *coro_pipeline)\u001b[0m\n\u001b[0;32m     51\u001b[0m f \u001b[39m=\u001b[39m chain(events, \u001b[39m*\u001b[39mcoro_pipeline)\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 53\u001b[0m     \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m source:\n\u001b[0;32m     54\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m             f\u001b[39m.\u001b[39msend(value)\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\myenv2\\Lib\\site-packages\\ijson\\common.py:219\u001b[0m, in \u001b[0;36mfile_source\u001b[1;34m(f, buf_size)\u001b[0m\n\u001b[0;32m    217\u001b[0m f \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mbytes_reader(f)\n\u001b[0;32m    218\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 219\u001b[0m     data \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mread(buf_size)\n\u001b[0;32m    220\u001b[0m     \u001b[39myield\u001b[39;00m data\n\u001b[0;32m    221\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\myenv2\\Lib\\site-packages\\ijson\\compat.py:32\u001b[0m, in \u001b[0;36mutf8reader.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\u001b[39mself\u001b[39m, n):\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstr_reader\u001b[39m.\u001b[39mread(n)\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def validate_large_json(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        parser = ijson.parse(f)\n",
    "        try:\n",
    "            for prefix, event, value in parser:\n",
    "                pass\n",
    "            print(\"The JSON file is valid.\")\n",
    "        except ijson.JSONError as e:\n",
    "            print(f\"Invalid JSON file: {e}\")\n",
    "\n",
    "validate_large_json('dblpv13_fixed.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docker run --name advdaba_labo2 `\n",
    "-p7474:7474 `\n",
    "-p7687:7687 `\n",
    "-v $HOME/neo4j/advDB-lab2/logs:/logs `\n",
    "-v $HOME/neo4j/advDB-lab2/data:/data `\n",
    "-v $HOME/neo4j/advDB-lab2/import:/var/lib/neo4j/import `\n",
    "--memory=\"3g\" `\n",
    "--env NEO4J_AUTH=neo4j/testtest `\n",
    "neo4j:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start processing...\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 2 (char 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W4sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m articles \u001b[39m=\u001b[39m stream_json_objects(file)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W4sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mwith\u001b[39;00m driver\u001b[39m.\u001b[39msession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W4sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39mfor\u001b[39;00m idx, item \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(articles):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W4sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m         \u001b[39mif\u001b[39;00m idx \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m max_line_to_load:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W4sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W4sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m obj\u001b[39m.\u001b[39mappend(stripped)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mif\u001b[39;00m depth \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39myield\u001b[39;00m json\u001b[39m.\u001b[39;49mloads(\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(obj))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     obj \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, s, _w\u001b[39m=\u001b[39mWHITESPACE\u001b[39m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[39m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mraw_decode(s, idx\u001b[39m=\u001b[39;49m_w(s, \u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[39m=\u001b[39m _w(s, end)\u001b[39m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExpecting value\u001b[39m\u001b[39m\"\u001b[39m, s, err\u001b[39m.\u001b[39mvalue) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[39mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 2 (char 1)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from neo4j import GraphDatabase\n",
    "import re\n",
    "\n",
    "max_line_to_load = 10\n",
    "filename = \"dblpv13.json\"\n",
    "uri = \"bolt://0.0.0.0:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"testtest\"))\n",
    "\n",
    "def stream_json_objects(file):\n",
    "    depth = 0\n",
    "    obj = []\n",
    "    for line in file:\n",
    "        stripped = line.strip()\n",
    "        if not stripped:\n",
    "            continue\n",
    "\n",
    "        if stripped[0] == '{':\n",
    "            depth += 1\n",
    "        if stripped[-1] == '}':\n",
    "            depth -= 1\n",
    "        \n",
    "        obj.append(stripped)\n",
    "\n",
    "        if depth == 0:\n",
    "            yield json.loads(''.join(obj))\n",
    "            obj = []\n",
    "\n",
    "def corrected_json_lines(file):\n",
    "    for line in file:\n",
    "        yield re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', line)\n",
    "\n",
    "def add_article_and_related_data(tx, article):\n",
    "    # Create ARTICLE node\n",
    "    article_node = tx.run(\"MERGE (a:Article {_id: $id, title: $title}) RETURN a\",\n",
    "                          id=article[\"_id\"], title=article[\"title\"]).single()[\"a\"]\n",
    "\n",
    "    # Create AUTHORED relationships\n",
    "    for author_name in article.get(\"authors\", []):\n",
    "        author_node = tx.run(\"MERGE (a:Author {_id: $name, name: $name}) RETURN a\", \n",
    "                             name=author_name).single()[\"a\"]\n",
    "        tx.run(\"MERGE (a)-[:AUTHORED]->(b)\", a=author_node, b=article_node)\n",
    "\n",
    "    # Create CITES relationships\n",
    "    for cited_article_id in article.get(\"references\", []):\n",
    "        tx.run(\"\"\"\n",
    "        MATCH (a:Article), (b:Article)\n",
    "        WHERE a._id = $id AND b._id = $ref_id\n",
    "        MERGE (a)-[:CITES]->(b)\n",
    "        \"\"\", id=article[\"_id\"], ref_id=cited_article_id)\n",
    "\n",
    "print(\"Start processing...\")\n",
    "with open(filename, 'r') as file:\n",
    "    articles = stream_json_objects(file)\n",
    "    with driver.session() as session:\n",
    "        for idx, item in enumerate(articles):\n",
    "            if idx >= max_line_to_load:\n",
    "                break\n",
    "            if \"title\" not in item:\n",
    "                print(f\"Article {item['_id']} does not have a title. Skipping.\")\n",
    "                continue\n",
    "            try:\n",
    "                session.write_transaction(add_article_and_related_data, item)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article {item['_id']}: {e}\")\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def corrected_json_lines(file):\n",
    "    for line in file:\n",
    "        yield re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', line)\n",
    "\n",
    "with open('dblpv13.json', 'r') as json_file:\n",
    "    corrected_lines = corrected_json_lines(json_file)\n",
    "    \n",
    "    for line in corrected_lines:\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            # Process the JSON data here\n",
    "            print(data)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   article_id  \\\n",
      "0    53e99784b7602d9701f3e133   \n",
      "1    53e99784b7602d9701f3e133   \n",
      "2    53e99784b7602d9701f3e133   \n",
      "3    53e99784b7602d9701f3e133   \n",
      "4    53e99784b7602d9701f3e133   \n",
      "..                        ...   \n",
      "139  53e99784b7602d9701f3f95a   \n",
      "140  53e99784b7602d9701f3f95a   \n",
      "141  53e99784b7602d9701f3f95a   \n",
      "142  53e99784b7602d9701f3f95a   \n",
      "143  53e99784b7602d9701f3f95b   \n",
      "\n",
      "                                         article_title  \\\n",
      "0    The relationship between canopy parameters and...   \n",
      "1    The relationship between canopy parameters and...   \n",
      "2    The relationship between canopy parameters and...   \n",
      "3    The relationship between canopy parameters and...   \n",
      "4    The relationship between canopy parameters and...   \n",
      "..                                                 ...   \n",
      "139                                             FACETS   \n",
      "140                                             FACETS   \n",
      "141                                             FACETS   \n",
      "142                                             FACETS   \n",
      "143                                        Fisherfaces   \n",
      "\n",
      "                    author_id       author_name cited_article_id  \n",
      "0    53f45728dabfaec09f209538      Peijuan Wang             None  \n",
      "1    5601754345cedb3395e59457      Jiahua Zhang             None  \n",
      "2    53f38438dabfae4b34a08928       Donghui Xie             None  \n",
      "3    5601754345cedb3395e5945a         Yanyan Xu             None  \n",
      "4    53f43d25dabfaeecd6995149            Yun Xu             None  \n",
      "..                        ...               ...              ...  \n",
      "139  53f45ab1dabfaeb22f511541  Richard Groebner             None  \n",
      "140  53f4359fdabfaeee229a4700      Satish Balay             None  \n",
      "141  53f44990dabfaee4dc7ddf84   Lois C. McInnes             None  \n",
      "142  562cb37445cedb3398c9befe        Hong Zhang             None  \n",
      "143  53f46eefdabfaee02adb698d    Aleix Martinez             None  \n",
      "\n",
      "[144 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "filename = 'biggertest.json'\n",
    "\n",
    "# Load the data from the file\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create a list to hold consolidated data\n",
    "consolidated_data = []\n",
    "\n",
    "for article in data:\n",
    "    article_id = article['_id']\n",
    "    article_title = article['title']\n",
    "    authors = article.get('authors', [])\n",
    "    references = article.get('references', [])\n",
    "    \n",
    "    for author in authors:\n",
    "        # Handle the case when author name or _id is missing\n",
    "        author_name = author.get('name', None)  # Use None if 'name' key is missing\n",
    "        author_id = author.get('_id', None)  # Use None if '_id' key is missing\n",
    "        \n",
    "        # If no references, append article and author details only\n",
    "        if not references:\n",
    "            consolidated_data.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': article_title,\n",
    "                'author_id': author_id,\n",
    "                'author_name': author_name,\n",
    "                'cited_article_id': None\n",
    "            })\n",
    "        else:\n",
    "            for reference in references:\n",
    "                consolidated_data.append({\n",
    "                    'article_id': article_id,\n",
    "                    'article_title': article_title,\n",
    "                    'author_id': author_id,\n",
    "                    'author_name': author_name,\n",
    "                    'cited_article_id': reference\n",
    "                })\n",
    "    \n",
    "    # Handle case where there are no authors but there are references\n",
    "    if not authors and references:\n",
    "        for reference in references:\n",
    "            consolidated_data.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': article_title,\n",
    "                'author_id': None,\n",
    "                'author_name': None,\n",
    "                'cited_article_id': reference\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(consolidated_data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedSymbol",
     "evalue": "Unexpected symbol 'N' at 103",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\Lib\\site-packages\\ijson\\backends\\python.py:225\u001b[0m, in \u001b[0;36mparse_value\u001b[1;34m(target, multivalue, use_float)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     number \u001b[39m=\u001b[39m to_number(symbol)\n\u001b[0;32m    226\u001b[0m     \u001b[39mif\u001b[39;00m number \u001b[39m==\u001b[39m inf:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\Lib\\site-packages\\ijson\\common.py:199\u001b[0m, in \u001b[0;36minteger_or_decimal\u001b[1;34m(str_value)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m str_value \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m str_value \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mE\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m str_value):\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39;49m(str_value)\n\u001b[0;32m    200\u001b[0m \u001b[39mreturn\u001b[39;00m decimal\u001b[39m.\u001b[39mDecimal(str_value)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'N'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnexpectedSymbol\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X20sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X20sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     articles \u001b[39m=\u001b[39m ijson\u001b[39m.\u001b[39mitems(file, \u001b[39m'\u001b[39m\u001b[39mitem\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X20sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     \u001b[39mfor\u001b[39;00m article \u001b[39min\u001b[39;00m articles:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X20sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m         consolidated_data\u001b[39m.\u001b[39mextend(process_article(article))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X20sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(consolidated_data)\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\Lib\\site-packages\\ijson\\utils.py:55\u001b[0m, in \u001b[0;36mcoros2gen\u001b[1;34m(source, *coro_pipeline)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m source:\n\u001b[0;32m     54\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m         f\u001b[39m.\u001b[39;49msend(value)\n\u001b[0;32m     56\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m         \u001b[39mfor\u001b[39;00m event \u001b[39min\u001b[39;00m events:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\Lib\\site-packages\\ijson\\backends\\python.py:44\u001b[0m, in \u001b[0;36mutf8_encoder\u001b[1;34m(target)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39mraise\u001b[39;00m common\u001b[39m.\u001b[39mIncompleteJSONError(e)\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m sdata:\n\u001b[1;32m---> 44\u001b[0m     send(sdata)\n\u001b[0;32m     45\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m bdata:\n\u001b[0;32m     46\u001b[0m     target\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\Lib\\site-packages\\ijson\\backends\\python.py:103\u001b[0m, in \u001b[0;36mLexer\u001b[1;34m(target)\u001b[0m\n\u001b[0;32m    101\u001b[0m             match \u001b[39m=\u001b[39m LEXEME_RE\u001b[39m.\u001b[39msearch(buf, pos)\n\u001b[0;32m    102\u001b[0m             lexeme \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup()\n\u001b[1;32m--> 103\u001b[0m         send((discarded \u001b[39m+\u001b[39;49m match\u001b[39m.\u001b[39;49mstart(), lexeme))\n\u001b[0;32m    104\u001b[0m         pos \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mend()\n\u001b[0;32m    105\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m     \u001b[39m# Don't ask data from an already exhausted source\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\Lib\\site-packages\\ijson\\backends\\python.py:231\u001b[0m, in \u001b[0;36mparse_value\u001b[1;34m(target, multivalue, use_float)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mstartswith(symbol) \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mfalse\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mstartswith(symbol) \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mnull\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mstartswith(symbol):\n\u001b[0;32m    230\u001b[0m         \u001b[39mraise\u001b[39;00m common\u001b[39m.\u001b[39mIncompleteJSONError(\u001b[39m'\u001b[39m\u001b[39mIncomplete JSON content\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 231\u001b[0m     \u001b[39mraise\u001b[39;00m UnexpectedSymbol(symbol, pos)\n\u001b[0;32m    232\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     send((\u001b[39m'\u001b[39m\u001b[39mnumber\u001b[39m\u001b[39m'\u001b[39m, number))\n",
      "\u001b[1;31mUnexpectedSymbol\u001b[0m: Unexpected symbol 'N' at 103"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ijson\n",
    "\n",
    "filename = 'dblpv13.json'\n",
    "\n",
    "# Function to process an individual article from the JSON file\n",
    "def process_article(article):\n",
    "    data_list = []\n",
    "    article_id = article['_id']\n",
    "    article_title = article['title']\n",
    "    authors = article.get('authors', [])\n",
    "    references = article.get('references', [])\n",
    "\n",
    "    for author in authors:\n",
    "        author_name = author.get('name', None)\n",
    "        author_id = author.get('_id', None)\n",
    "\n",
    "        if not references:\n",
    "            data_list.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': article_title,\n",
    "                'author_id': author_id,\n",
    "                'author_name': author_name,\n",
    "                'cited_article_id': None\n",
    "            })\n",
    "        else:\n",
    "            for reference in references:\n",
    "                data_list.append({\n",
    "                    'article_id': article_id,\n",
    "                    'article_title': article_title,\n",
    "                    'author_id': author_id,\n",
    "                    'author_name': author_name,\n",
    "                    'cited_article_id': reference\n",
    "                })\n",
    "\n",
    "    if not authors and references:\n",
    "        for reference in references:\n",
    "            data_list.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': article_title,\n",
    "                'author_id': None,\n",
    "                'author_name': None,\n",
    "                'cited_article_id': reference\n",
    "            })\n",
    "\n",
    "    return data_list\n",
    "\n",
    "# Iteratively read and process the JSON file\n",
    "consolidated_data = []\n",
    "with open(filename, 'r') as file:\n",
    "    articles = ijson.items(file, 'item')\n",
    "    for article in articles:\n",
    "        consolidated_data.extend(process_article(article))\n",
    "\n",
    "df = pd.DataFrame(consolidated_data)\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 9\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X16sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m corrected_file \u001b[39m=\u001b[39m corrected_json_lines(file)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X16sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m articles \u001b[39m=\u001b[39m ijson\u001b[39m.\u001b[39mitems(corrected_file, \u001b[39m'\u001b[39m\u001b[39mitem\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X16sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39mfor\u001b[39;00m article \u001b[39min\u001b[39;00m articles:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X16sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39mprint\u001b[39m(article)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X16sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     consolidated_data\u001b[39m.\u001b[39mextend(process_article(article))\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\Lib\\site-packages\\ijson\\utils.py:55\u001b[0m, in \u001b[0;36mcoros2gen\u001b[1;34m(source, *coro_pipeline)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m source:\n\u001b[0;32m     54\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m         f\u001b[39m.\u001b[39;49msend(value)\n\u001b[0;32m     56\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m         \u001b[39mfor\u001b[39;00m event \u001b[39min\u001b[39;00m events:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\Lib\\site-packages\\ijson\\common.py:146\u001b[0m, in \u001b[0;36mitems_basecoro\u001b[1;34m(target, prefix, map_type)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[39mAn couroutine dispatching native Python objects constructed from the events\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39munder a given prefix.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     current, event, value \u001b[39m=\u001b[39m (\u001b[39myield\u001b[39;00m)\n\u001b[0;32m    147\u001b[0m     \u001b[39mif\u001b[39;00m current \u001b[39m==\u001b[39m prefix:\n\u001b[0;32m    148\u001b[0m         \u001b[39mif\u001b[39;00m event \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mstart_map\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstart_array\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ijson\n",
    "import re\n",
    "\n",
    "filename = 'dblpv13.json'\n",
    "\n",
    "# Function to preprocess JSON lines\n",
    "def corrected_json_lines(file):\n",
    "    for line in file:\n",
    "        yield re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', line)\n",
    "\n",
    "# Function to process an individual article from the JSON file\n",
    "def process_article(article):\n",
    "    data_list = []\n",
    "    article_id = article['_id']\n",
    "    article_title = article['title']\n",
    "    authors = article.get('authors', [])\n",
    "    references = article.get('references', [])\n",
    "\n",
    "    for author in authors:\n",
    "        author_name = author.get('name', None)\n",
    "        author_id = author.get('_id', None)\n",
    "\n",
    "        if not references:\n",
    "            data_list.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': article_title,\n",
    "                'author_id': author_id,\n",
    "                'author_name': author_name,\n",
    "                'cited_article_id': None\n",
    "            })\n",
    "        else:\n",
    "            for reference in references:\n",
    "                data_list.append({\n",
    "                    'article_id': article_id,\n",
    "                    'article_title': article_title,\n",
    "                    'author_id': author_id,\n",
    "                    'author_name': author_name,\n",
    "                    'cited_article_id': reference\n",
    "                })\n",
    "\n",
    "    if not authors and references:\n",
    "        for reference in references:\n",
    "            data_list.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': article_title,\n",
    "                'author_id': None,\n",
    "                'author_name': None,\n",
    "                'cited_article_id': reference\n",
    "            })\n",
    "\n",
    "    return data_list\n",
    "\n",
    "# Iteratively read and process the JSON file\n",
    "consolidated_data = []\n",
    "with open(filename, 'r') as file:\n",
    "    # Apply preprocessing on each line for MongoDB's NumberInt\n",
    "    corrected_file = corrected_json_lines(file)\n",
    "    \n",
    "    articles = ijson.items(corrected_file, 'item')\n",
    "    for article in articles:\n",
    "        print(article)\n",
    "        consolidated_data.extend(process_article(article))\n",
    "\n",
    "# df = pd.DataFrame(consolidated_data)\n",
    "# display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>author_id</th>\n",
       "      <th>author_name</th>\n",
       "      <th>cited_article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53e99784b7602d9701f3e133</td>\n",
       "      <td>The relationship between canopy parameters and...</td>\n",
       "      <td>53f45728dabfaec09f209538</td>\n",
       "      <td>Peijuan Wang</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53e99784b7602d9701f3e133</td>\n",
       "      <td>The relationship between canopy parameters and...</td>\n",
       "      <td>5601754345cedb3395e59457</td>\n",
       "      <td>Jiahua Zhang</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53e99784b7602d9701f3e133</td>\n",
       "      <td>The relationship between canopy parameters and...</td>\n",
       "      <td>53f38438dabfae4b34a08928</td>\n",
       "      <td>Donghui Xie</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53e99784b7602d9701f3e133</td>\n",
       "      <td>The relationship between canopy parameters and...</td>\n",
       "      <td>5601754345cedb3395e5945a</td>\n",
       "      <td>Yanyan Xu</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>53e99784b7602d9701f3e133</td>\n",
       "      <td>The relationship between canopy parameters and...</td>\n",
       "      <td>53f43d25dabfaeecd6995149</td>\n",
       "      <td>Yun Xu</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>53e99784b7602d9701f3f95a</td>\n",
       "      <td>FACETS</td>\n",
       "      <td>53f45ab1dabfaeb22f511541</td>\n",
       "      <td>Richard Groebner</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>53e99784b7602d9701f3f95a</td>\n",
       "      <td>FACETS</td>\n",
       "      <td>53f4359fdabfaeee229a4700</td>\n",
       "      <td>Satish Balay</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>53e99784b7602d9701f3f95a</td>\n",
       "      <td>FACETS</td>\n",
       "      <td>53f44990dabfaee4dc7ddf84</td>\n",
       "      <td>Lois C. McInnes</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>53e99784b7602d9701f3f95a</td>\n",
       "      <td>FACETS</td>\n",
       "      <td>562cb37445cedb3398c9befe</td>\n",
       "      <td>Hong Zhang</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>53e99784b7602d9701f3f95b</td>\n",
       "      <td>Fisherfaces</td>\n",
       "      <td>53f46eefdabfaee02adb698d</td>\n",
       "      <td>Aleix Martinez</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   article_id  \\\n",
       "0    53e99784b7602d9701f3e133   \n",
       "1    53e99784b7602d9701f3e133   \n",
       "2    53e99784b7602d9701f3e133   \n",
       "3    53e99784b7602d9701f3e133   \n",
       "4    53e99784b7602d9701f3e133   \n",
       "..                        ...   \n",
       "139  53e99784b7602d9701f3f95a   \n",
       "140  53e99784b7602d9701f3f95a   \n",
       "141  53e99784b7602d9701f3f95a   \n",
       "142  53e99784b7602d9701f3f95a   \n",
       "143  53e99784b7602d9701f3f95b   \n",
       "\n",
       "                                         article_title  \\\n",
       "0    The relationship between canopy parameters and...   \n",
       "1    The relationship between canopy parameters and...   \n",
       "2    The relationship between canopy parameters and...   \n",
       "3    The relationship between canopy parameters and...   \n",
       "4    The relationship between canopy parameters and...   \n",
       "..                                                 ...   \n",
       "139                                             FACETS   \n",
       "140                                             FACETS   \n",
       "141                                             FACETS   \n",
       "142                                             FACETS   \n",
       "143                                        Fisherfaces   \n",
       "\n",
       "                    author_id       author_name cited_article_id  \n",
       "0    53f45728dabfaec09f209538      Peijuan Wang             None  \n",
       "1    5601754345cedb3395e59457      Jiahua Zhang             None  \n",
       "2    53f38438dabfae4b34a08928       Donghui Xie             None  \n",
       "3    5601754345cedb3395e5945a         Yanyan Xu             None  \n",
       "4    53f43d25dabfaeecd6995149            Yun Xu             None  \n",
       "..                        ...               ...              ...  \n",
       "139  53f45ab1dabfaeb22f511541  Richard Groebner             None  \n",
       "140  53f4359fdabfaeee229a4700      Satish Balay             None  \n",
       "141  53f44990dabfaee4dc7ddf84   Lois C. McInnes             None  \n",
       "142  562cb37445cedb3398c9befe        Hong Zhang             None  \n",
       "143  53f46eefdabfaee02adb698d    Aleix Martinez             None  \n",
       "\n",
       "[144 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "filename = 'biggertest.json'\n",
    "\n",
    "# Load the data from the file\n",
    "with open(filename, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Create a list to hold consolidated data\n",
    "consolidated_data = []\n",
    "\n",
    "for article in data:\n",
    "    article_id = article['_id']\n",
    "    article_title = article['title']\n",
    "    authors = article.get('authors', [])\n",
    "    references = article.get('references', [])\n",
    "    \n",
    "    for author in authors:\n",
    "        # Handle the case when author name or _id is missing\n",
    "        author_name = author.get('name', None)  # Use None if 'name' key is missing\n",
    "        author_id = author.get('_id', None)  # Use None if '_id' key is missing\n",
    "        \n",
    "        # If no references, append article and author details only\n",
    "        if not references:\n",
    "            consolidated_data.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': article_title,\n",
    "                'author_id': author_id,\n",
    "                'author_name': author_name,\n",
    "                'cited_article_id': None\n",
    "            })\n",
    "        else:\n",
    "            for reference in references:\n",
    "                consolidated_data.append({\n",
    "                    'article_id': article_id,\n",
    "                    'article_title': article_title,\n",
    "                    'author_id': author_id,\n",
    "                    'author_name': author_name,\n",
    "                    'cited_article_id': reference\n",
    "                })\n",
    "    \n",
    "    # Handle case where there are no authors but there are references\n",
    "    if not authors and references:\n",
    "        for reference in references:\n",
    "            consolidated_data.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': article_title,\n",
    "                'author_id': None,\n",
    "                'author_name': None,\n",
    "                'cited_article_id': reference\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(consolidated_data)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 11\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m consolidated_data \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m     corrected_content \u001b[39m=\u001b[39m corrected_json_content(file)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m     articles \u001b[39m=\u001b[39m ijson\u001b[39m.\u001b[39mitems(corrected_content, \u001b[39m'\u001b[39m\u001b[39mitem\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     \u001b[39mfor\u001b[39;00m article \u001b[39min\u001b[39;00m articles:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m         \u001b[39m# consolidated_data.extend(process_article(article))\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 11\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcorrected_json_content\u001b[39m(file):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     corrected_lines \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(corrected_json_lines(file))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(corrected_lines)\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcorrected_json_lines\u001b[39m(file):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m file:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         \u001b[39myield\u001b[39;00m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNumberInt\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m((\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m, line)\n",
      "File \u001b[1;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ijson\n",
    "import re\n",
    "\n",
    "filename = 'dblpv13.json'\n",
    "\n",
    "# Function to preprocess JSON lines\n",
    "def corrected_json_content(file):\n",
    "    corrected_lines = list(corrected_json_lines(file))\n",
    "    return '\\n'.join(corrected_lines)\n",
    "\n",
    "def corrected_json_lines(file):\n",
    "    for line in file:\n",
    "        yield re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', line)\n",
    "\n",
    "# Function to process an individual article from the JSON file\n",
    "def process_article(article):\n",
    "    data_list = []\n",
    "    article_id = article['_id']\n",
    "    article_title = article['title']\n",
    "    authors = article.get('authors', [])\n",
    "    references = article.get('references', [])\n",
    "\n",
    "    for author in authors:\n",
    "        author_name = author.get('name', None)  # Use None if 'name' key is missing\n",
    "        author_id = author.get('_id', None)  # Use None if '_id' key is missing\n",
    "\n",
    "        # If no references, append article and author details only\n",
    "        if not references:\n",
    "            data_list.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': article_title,\n",
    "                'author_id': author_id,\n",
    "                'author_name': author_name,\n",
    "                'cited_article_id': None\n",
    "            })\n",
    "        else:\n",
    "            for reference in references:\n",
    "                data_list.append({\n",
    "                    'article_id': article_id,\n",
    "                    'article_title': article_title,\n",
    "                    'author_id': author_id,\n",
    "                    'author_name': author_name,\n",
    "                    'cited_article_id': reference\n",
    "                })\n",
    "\n",
    "    # Handle case where there are no authors but there are references\n",
    "    if not authors and references:\n",
    "        for reference in references:\n",
    "            data_list.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': article_title,\n",
    "                'author_id': None,\n",
    "                'author_name': None,\n",
    "                'cited_article_id': reference\n",
    "            })\n",
    "\n",
    "    return data_list\n",
    "\n",
    "# Iteratively read, preprocess, and process the JSON file\n",
    "consolidated_data = []\n",
    "with open(filename, 'r') as file:\n",
    "    corrected_content = corrected_json_content(file)\n",
    "    articles = ijson.items(corrected_content, 'item')\n",
    "    for article in articles:\n",
    "        # consolidated_data.extend(process_article(article))\n",
    "        print(article)\n",
    "\n",
    "# df = pd.DataFrame(consolidated_data)\n",
    "# display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X25sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m buffered_data \u001b[39m=\u001b[39m []\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X25sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X25sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m     articles \u001b[39m=\u001b[39m ijson\u001b[39m.\u001b[39mitems(corrected_json_content(file), \u001b[39m'\u001b[39m\u001b[39mitem\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X25sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m     \u001b[39mfor\u001b[39;00m article \u001b[39min\u001b[39;00m articles:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X25sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m         buffered_data\u001b[39m.\u001b[39mextend(process_article(article))\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X25sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcorrected_json_content\u001b[39m(file):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     corrected_lines \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(corrected_json_lines(file))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(corrected_lines)\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcorrected_json_lines\u001b[39m(file):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m file:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39myield\u001b[39;00m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNumberInt\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m((\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m, line)\n",
      "File \u001b[1;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ijson\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "import gc\n",
    "\n",
    "filename = 'dblpv13.json'\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"testtest\"))\n",
    "\n",
    "# Function to preprocess JSON lines\n",
    "def corrected_json_content(file):\n",
    "    corrected_lines = list(corrected_json_lines(file))\n",
    "    return '\\n'.join(corrected_lines)\n",
    "\n",
    "def corrected_json_lines(file):\n",
    "    for line in file:\n",
    "        yield re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', line)\n",
    "\n",
    "# Function to process an individual article from the JSON file\n",
    "def process_article(article):\n",
    "    data_list = []\n",
    "    article_id = article['_id']\n",
    "    article_title = article['title']\n",
    "    authors = article.get('authors', [])\n",
    "    references = article.get('references', [])\n",
    "\n",
    "    for author in authors:\n",
    "        author_name = author.get('name', None)\n",
    "        author_id = author.get('_id', None)\n",
    "\n",
    "        # If no references, append article and author details only\n",
    "        if not references:\n",
    "            data_list.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': article_title,\n",
    "                'author_id': author_id,\n",
    "                'author_name': author_name,\n",
    "                'cited_article_id': None\n",
    "            })\n",
    "        else:\n",
    "            for reference in references:\n",
    "                data_list.append({\n",
    "                    'article_id': article_id,\n",
    "                    'article_title': article_title,\n",
    "                    'author_id': author_id,\n",
    "                    'author_name': author_name,\n",
    "                    'cited_article_id': reference\n",
    "                })\n",
    "\n",
    "    # Handle case where there are no authors but there are references\n",
    "    if not authors and references:\n",
    "        for reference in references:\n",
    "            data_list.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': article_title,\n",
    "                'author_id': None,\n",
    "                'author_name': None,\n",
    "                'cited_article_id': reference\n",
    "            })\n",
    "\n",
    "    return data_list\n",
    "\n",
    "def add_article(tx, article_id, title, authors, cited_articles):\n",
    "    # Merge the current ARTICLE node with its title and _id properties\n",
    "    tx.run(\"MERGE (a:Article {_id: $id}) SET a.title = $title\", id=article_id, title=title)\n",
    "    \n",
    "    for author in authors:\n",
    "        if author:  \n",
    "            # Merge the AUTHOR node with its name and _id properties\n",
    "            tx.run(\"MERGE (a:Author {_id: $author_id, name: $name})\", author_id=author[\"_id\"], name=author[\"name\"])\n",
    "            # Merge the AUTHORED relationship between AUTHOR and ARTICLE\n",
    "            tx.run(\"\"\"\n",
    "                MATCH (a:Author {_id: $author_id}), (b:Article {_id: $article_id})\n",
    "                MERGE (a)-[:AUTHORED]->(b)\n",
    "            \"\"\", author_id=author[\"_id\"], article_id=article_id)\n",
    "            \n",
    "    for cited_article_id in cited_articles:\n",
    "        tx.run(\"\"\"\n",
    "            MERGE (a:Article {_id: $article_id})\n",
    "            MERGE (b:Article {_id: $cited_article_id})\n",
    "            MERGE (a)-[:CITES]->(b)\n",
    "        \"\"\", article_id=article_id, cited_article_id=cited_article_id)\n",
    "\n",
    "\n",
    "\n",
    "def send_chunk_to_neo4j(chunk):\n",
    "    with driver.session() as session:\n",
    "        for _, row in chunk.iterrows():\n",
    "            if row['author_name'] and row['author_id']:\n",
    "                authors = [{\"name\": row['author_name'], \"_id\": row['author_id']}]\n",
    "                if row['cited_article_id']:  # check if there's a valid cited_article_id\n",
    "                    session.execute_write(add_article, row['article_id'], row['article_title'], authors, [row['cited_article_id']])\n",
    "                else:\n",
    "                    session.execute_write(add_article, row['article_id'], row['article_title'], authors, [])\n",
    "\n",
    "chunk_size = 10  # adjust this based on your memory and performance needs\n",
    "buffered_data = []\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    articles = ijson.items(corrected_json_content(file), 'item')\n",
    "    for article in articles:\n",
    "        buffered_data.extend(process_article(article))\n",
    "        if len(buffered_data) >= chunk_size:\n",
    "            chunk_df = pd.DataFrame(buffered_data)\n",
    "            send_chunk_to_neo4j(chunk_df)\n",
    "            buffered_data = []  # reset the buffer\n",
    "            del chunk_df  # delete the DataFrame\n",
    "            gc.collect()  # collect garbage\n",
    "\n",
    "# Send remaining buffered data (if any)\n",
    "if buffered_data:\n",
    "    chunk_df = pd.DataFrame(buffered_data)\n",
    "    send_chunk_to_neo4j(chunk_df)\n",
    "    del chunk_df  # delete the DataFrame\n",
    "    gc.collect()  # collect garbage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "filename = 'dblpv13.json'\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"testtest\"))\n",
    "\n",
    "# Function to preprocess JSON lines\n",
    "def corrected_json_content(file):\n",
    "    corrected_lines = list(corrected_json_lines(file))\n",
    "    return '\\n'.join(corrected_lines)\n",
    "\n",
    "def corrected_json_lines(file):\n",
    "    for line in file:\n",
    "        yield re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', line)\n",
    "\n",
    "# Function to process an individual article from the JSON file\n",
    "def process_article(article):\n",
    "    data_list = []\n",
    "    article_id = article['_id']\n",
    "    article_title = article['title']\n",
    "    authors = article.get('authors', [])\n",
    "    references = article.get('references', [])\n",
    "\n",
    "    for author in authors:\n",
    "        author_name = author.get('name', None)\n",
    "        author_id = author.get('_id', None)\n",
    "\n",
    "        # If no references, append article and author details only\n",
    "        if not references:\n",
    "            data_list.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': article_title,\n",
    "                'author_id': author_id,\n",
    "                'author_name': author_name,\n",
    "                'cited_article_id': None\n",
    "            })\n",
    "        else:\n",
    "            for reference in references:\n",
    "                data_list.append({\n",
    "                    'article_id': article_id,\n",
    "                    'article_title': article_title,\n",
    "                    'author_id': author_id,\n",
    "                    'author_name': author_name,\n",
    "                    'cited_article_id': reference\n",
    "                })\n",
    "\n",
    "    # Handle case where there are no authors but there are references\n",
    "    if not authors and references:\n",
    "        for reference in references:\n",
    "            data_list.append({\n",
    "                'article_id': article_id,\n",
    "                'article_title': article_title,\n",
    "                'author_id': None,\n",
    "                'author_name': None,\n",
    "                'cited_article_id': reference\n",
    "            })\n",
    "\n",
    "    return data_list\n",
    "\n",
    "def add_article(tx, article_id, title, authors, cited_articles):\n",
    "    # Merge the current ARTICLE node with its title and _id properties\n",
    "    tx.run(\"MERGE (a:Article {_id: $id}) SET a.title = $title\", id=article_id, title=title)\n",
    "    \n",
    "    for author in authors:\n",
    "        if author:  \n",
    "            # Merge the AUTHOR node with its name and _id properties\n",
    "            tx.run(\"MERGE (a:Author {_id: $author_id, name: $name})\", author_id=author[\"_id\"], name=author[\"name\"])\n",
    "            # Merge the AUTHORED relationship between AUTHOR and ARTICLE\n",
    "            tx.run(\"\"\"\n",
    "                MATCH (a:Author {_id: $author_id}), (b:Article {_id: $article_id})\n",
    "                MERGE (a)-[:AUTHORED]->(b)\n",
    "            \"\"\", author_id=author[\"_id\"], article_id=article_id)\n",
    "            \n",
    "    for cited_article_id in cited_articles:\n",
    "        tx.run(\"\"\"\n",
    "            MERGE (a:Article {_id: $article_id})\n",
    "            MERGE (b:Article {_id: $cited_article_id})\n",
    "            MERGE (a)-[:CITES]->(b)\n",
    "        \"\"\", article_id=article_id, cited_article_id=cited_article_id)\n",
    "\n",
    "def articles_generator(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        articles = ijson.items(corrected_json_content(file), 'item')\n",
    "        for article in articles:\n",
    "            yield article\n",
    "\n",
    "def process_and_send_to_neo4j(article):\n",
    "    with driver.session() as session:\n",
    "        data_list = process_article(article)\n",
    "        for data in data_list:\n",
    "            if data['author_name'] and data['author_id']:\n",
    "                authors = [{\"name\": data['author_name'], \"_id\": data['author_id']}]\n",
    "                if data['cited_article_id']:\n",
    "                    session.execute_write(add_article, data['article_id'], data['article_title'], authors, [data['cited_article_id']])\n",
    "                else:\n",
    "                    session.execute_write(add_article, data['article_id'], data['article_title'], authors, [])\n",
    "\n",
    "for article in articles_generator(filename):\n",
    "    process_and_send_to_neo4j(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import re\n",
    "from neo4j import AsyncGraphDatabase\n",
    "import datetime\n",
    "import asyncio\n",
    "import gc\n",
    "\n",
    "filename = 'biggertest.json'\n",
    "uri = \"bolt://localhost:7687\"\n",
    "BATCH_SIZE = 5  # Adjust based on memory limits and performance\n",
    "semaphore = asyncio.Semaphore(2)  # Limit concurrent tasks\n",
    "\n",
    "async def async_session():\n",
    "    driver = AsyncGraphDatabase.driver(uri, auth=(\"neo4j\", \"testtest\"))\n",
    "    return driver.async_session(database=\"neo4j\")\n",
    "\n",
    "# Function to correct JSON lines on-the-fly\n",
    "def corrected_json_lines(file):\n",
    "    for line in file:\n",
    "        yield re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', line)\n",
    "\n",
    "# Function to process an individual article from the JSON file\n",
    "async def process_article(article):\n",
    "    article_id = article['_id']\n",
    "    article_title = article['title']\n",
    "    authors = article.get('authors', [])\n",
    "    references = article.get('references', [])\n",
    "\n",
    "    async with semaphore, await async_session() as session:\n",
    "        for author in authors:\n",
    "            if references:\n",
    "                for reference in references:\n",
    "                    await session.write_transaction(add_article, article_id, article_title, author, reference)\n",
    "            else:\n",
    "                await session.write_transaction(add_article, article_id, article_title, author)\n",
    "\n",
    "        if not authors and references:\n",
    "            for reference in references:\n",
    "                await session.write_transaction(add_article, article_id, article_title, None, reference)\n",
    "        elif not references:\n",
    "            await session.write_transaction(add_article, article_id, article_title)\n",
    "\n",
    "async def add_article(tx, article_id, title, author=None, cited_article_id=None):\n",
    "    if title:\n",
    "        tx.run(\"MERGE (a:Article {_id: $id}) SET a.title = $title\", id=article_id, title=title)\n",
    "    else:\n",
    "        tx.run(\"MERGE (a:Article {_id: $id})\", id=article_id)\n",
    "\n",
    "    if author:\n",
    "        tx.run(\"MERGE (a:Author {_id: $author_id, name: $name})\", author_id=author['_id'], name=author['name'])\n",
    "        tx.run(\"\"\"\n",
    "            MATCH (a:Author {_id: $author_id}), (b:Article {_id: $article_id})\n",
    "            MERGE (a)-[:AUTHORED]->(b)\n",
    "        \"\"\", author_id=author['_id'], article_id=article_id)\n",
    "\n",
    "    if cited_article_id:\n",
    "        tx.run(\"\"\"\n",
    "            MERGE (a:Article {_id: $article_id})\n",
    "            MERGE (b:Article {_id: $cited_article_id})\n",
    "            MERGE (a)-[:CITES]->(b)\n",
    "        \"\"\", article_id=article_id, cited_article_id=cited_article_id)\n",
    "\n",
    "def articles_generator(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        for article in ijson.items(file, 'item', use_float=True):\n",
    "            yield article\n",
    "\n",
    "async def process_and_send_to_neo4j(articles_batch):\n",
    "    tasks = [process_article(article) for article in articles_batch]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "async def main():\n",
    "    # start\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(f\"Processing started at {start_time}\")\n",
    "\n",
    "    # process articles in batches\n",
    "    articles_batch = []\n",
    "    tasks = []\n",
    "\n",
    "    for article in articles_generator(filename):\n",
    "        articles_batch.append(article)\n",
    "        if len(articles_batch) == BATCH_SIZE:\n",
    "            tasks.append(asyncio.create_task(process_and_send_to_neo4j(articles_batch)))\n",
    "            articles_batch = []\n",
    "\n",
    "    # process any remaining articles\n",
    "    if articles_batch:\n",
    "        tasks.append(asyncio.create_task(process_and_send_to_neo4j(articles_batch)))\n",
    "\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "    # end\n",
    "    end_time = datetime.datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Processing finished at {end_time}. Total time taken: {elapsed_time}\")\n",
    "\n",
    "def run_main():\n",
    "    asyncio.run(main())\n",
    "\n",
    "%memit run_main()\n",
    "\n",
    "# Write a file to indicate that the data has been imported\n",
    "with open('/tmp/data_imported', 'w') as f:\n",
    "    f.write('Data imported on {}\\n'.format(datetime.datetime.now()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes in the database: 158\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "from neo4j import AsyncGraphDatabase\n",
    "\n",
    "uri = \"bolt://localhost:7687\"\n",
    "auth = (\"neo4j\", \"testtest\")\n",
    "\n",
    "async def test_connection():\n",
    "    driver = AsyncGraphDatabase.driver(uri, auth=auth)\n",
    "    async with driver.session() as session: \n",
    "        result = await session.run(\"MATCH (n) RETURN count(n) AS node_count\")\n",
    "        record = await result.single()  # Await here\n",
    "        count = record[\"node_count\"]\n",
    "        print(f\"Number of nodes in the database: {count}\")\n",
    "\n",
    "    await driver.close()\n",
    "\n",
    "# Run the test\n",
    "await test_connection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing started at 2023-10-24 18:23:34.688560\n",
      "Error processing article 53e99784b7602d9701f3e3f5: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3e133: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3e151: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3e15d: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3e161: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3e162: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3e165: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3e922: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3e4f4: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3eaf2: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3eaf4: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3eaf6: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f177: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f326: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f327: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f4d1: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f4d2: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f352: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f0b9: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f8c1: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f8c2: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f8c3: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f8d2: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f411: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f414: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f908: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f909: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f913: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f5fe: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f600: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f92a: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f60c: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f60f: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f955: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f956: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f957: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f958: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f959: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f95a: 'Semaphore' object does not support the context manager protocol\n",
      "Error processing article 53e99784b7602d9701f3f95b: 'Semaphore' object does not support the context manager protocol\n",
      "Processing finished at 2023-10-24 18:23:35.174241. Total time taken: 0:00:00.485681\n"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "import datetime\n",
    "import asyncio\n",
    "import gc\n",
    "\n",
    "filename = 'biggertest.json'\n",
    "uri = \"bolt://localhost:7687\"\n",
    "BATCH_SIZE = 5  # Adjust based on memory limits and performance\n",
    "semaphore = asyncio.Semaphore(2)  # Limit concurrent tasks\n",
    "\n",
    "# Function to correct JSON lines on-the-fly\n",
    "def corrected_json_lines(file):\n",
    "    for line in file:\n",
    "        yield re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', line)\n",
    "\n",
    "# Function to process an individual article from the JSON file\n",
    "async def process_article(article, driver):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            article_id = article['_id']\n",
    "            article_title = article['title']\n",
    "            authors = article.get('authors', [])\n",
    "            references = article.get('references', [])\n",
    "\n",
    "            with driver.session() as session:\n",
    "                with semaphore:\n",
    "                    for author in authors:\n",
    "                        if references:\n",
    "                            for reference in references:\n",
    "                                session.write_transaction(add_article, article_id, article_title, author, reference)\n",
    "                        else:\n",
    "                            session.write_transaction(add_article, article_id, article_title, author)\n",
    "\n",
    "                    if not authors and references:\n",
    "                        for reference in references:\n",
    "                            session.write_transaction(add_article, article_id, article_title, None, reference)\n",
    "                    elif not references:\n",
    "                        session.write_transaction(add_article, article_id, article_title)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing article {article['_id']}: {e}\")\n",
    "\n",
    "async def add_article(tx, article_id, title, author=None, cited_article_id=None):\n",
    "    if title:\n",
    "        tx.run(\"MERGE (a:Article {_id: $id}) SET a.title = $title\", id=article_id, title=title)\n",
    "    else:\n",
    "        tx.run(\"MERGE (a:Article {_id: $id})\", id=article_id)\n",
    "\n",
    "    if author:\n",
    "        tx.run(\"MERGE (a:Author {_id: $author_id, name: $name})\", author_id=author['_id'], name=author['name'])\n",
    "        tx.run(\"\"\"\n",
    "            MATCH (a:Author {_id: $author_id}), (b:Article {_id: $article_id})\n",
    "            MERGE (a)-[:AUTHORED]->(b)\n",
    "        \"\"\", author_id=author['_id'], article_id=article_id)\n",
    "\n",
    "    if cited_article_id:\n",
    "        tx.run(\"\"\"\n",
    "            MERGE (a:Article {_id: $article_id})\n",
    "            MERGE (b:Article {_id: $cited_article_id})\n",
    "            MERGE (a)-[:CITES]->(b)\n",
    "        \"\"\", article_id=article_id, cited_article_id=cited_article_id)\n",
    "\n",
    "def articles_generator(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        for article in ijson.items(file, 'item', use_float=True):\n",
    "            yield article\n",
    "\n",
    "async def process_and_send_to_neo4j(articles_batch, driver):\n",
    "    tasks = [process_article(article, driver) for article in articles_batch]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "async def main():\n",
    "    driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"testtest\"))\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(f\"Processing started at {start_time}\")\n",
    "\n",
    "    articles_batch = []\n",
    "    tasks = []\n",
    "\n",
    "    for article in articles_generator(filename):\n",
    "        articles_batch.append(article)\n",
    "        if len(articles_batch) == BATCH_SIZE:\n",
    "            tasks.append(asyncio.create_task(process_and_send_to_neo4j(articles_batch, driver)))\n",
    "            articles_batch = []\n",
    "            gc.collect()\n",
    "\n",
    "    if articles_batch:\n",
    "        tasks.append(asyncio.create_task(process_and_send_to_neo4j(articles_batch, driver)))\n",
    "\n",
    "    await asyncio.gather(*tasks)\n",
    "    \n",
    "    end_time = datetime.datetime.now()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Processing finished at {end_time}. Total time taken: {elapsed_time}\")\n",
    "\n",
    "    driver.close()  # Ensure the driver is closed\n",
    "\n",
    "def run_main():\n",
    "    asyncio.run(main())\n",
    "\n",
    "run_main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedSymbol",
     "evalue": "Unexpected symbol 'N' at 103",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\backends\\python.py:225\u001b[0m, in \u001b[0;36mparse_value\u001b[1;34m(target, multivalue, use_float)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     number \u001b[39m=\u001b[39m to_number(symbol)\n\u001b[0;32m    226\u001b[0m     \u001b[39mif\u001b[39;00m number \u001b[39m==\u001b[39m inf:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\common.py:199\u001b[0m, in \u001b[0;36minteger_or_decimal\u001b[1;34m(str_value)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m str_value \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m str_value \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mE\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m str_value):\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39;49m(str_value)\n\u001b[0;32m    200\u001b[0m \u001b[39mreturn\u001b[39;00m decimal\u001b[39m.\u001b[39mDecimal(str_value)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'N'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnexpectedSymbol\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                 \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mprefix\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Call the function with the path to your JSON file\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m process_large_json(filename)\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 18\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(file_path, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     parser \u001b[39m=\u001b[39m ijson\u001b[39m.\u001b[39mparse(file)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m prefix, event, value \u001b[39min\u001b[39;00m parser:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39mif\u001b[39;00m (prefix\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.title\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m             prefix\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.authors.item._id\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m             prefix\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.authors.item.name\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m             prefix\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.references.item\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m             prefix\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m._id\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mprefix\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\utils.py:55\u001b[0m, in \u001b[0;36mcoros2gen\u001b[1;34m(source, *coro_pipeline)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m source:\n\u001b[0;32m     54\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m         f\u001b[39m.\u001b[39;49msend(value)\n\u001b[0;32m     56\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m         \u001b[39mfor\u001b[39;00m event \u001b[39min\u001b[39;00m events:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\backends\\python.py:44\u001b[0m, in \u001b[0;36mutf8_encoder\u001b[1;34m(target)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39mraise\u001b[39;00m common\u001b[39m.\u001b[39mIncompleteJSONError(e)\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m sdata:\n\u001b[1;32m---> 44\u001b[0m     send(sdata)\n\u001b[0;32m     45\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m bdata:\n\u001b[0;32m     46\u001b[0m     target\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\backends\\python.py:103\u001b[0m, in \u001b[0;36mLexer\u001b[1;34m(target)\u001b[0m\n\u001b[0;32m    101\u001b[0m             match \u001b[39m=\u001b[39m LEXEME_RE\u001b[39m.\u001b[39msearch(buf, pos)\n\u001b[0;32m    102\u001b[0m             lexeme \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup()\n\u001b[1;32m--> 103\u001b[0m         send((discarded \u001b[39m+\u001b[39;49m match\u001b[39m.\u001b[39;49mstart(), lexeme))\n\u001b[0;32m    104\u001b[0m         pos \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mend()\n\u001b[0;32m    105\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m     \u001b[39m# Don't ask data from an already exhausted source\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\backends\\python.py:231\u001b[0m, in \u001b[0;36mparse_value\u001b[1;34m(target, multivalue, use_float)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mstartswith(symbol) \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mfalse\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mstartswith(symbol) \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mnull\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mstartswith(symbol):\n\u001b[0;32m    230\u001b[0m         \u001b[39mraise\u001b[39;00m common\u001b[39m.\u001b[39mIncompleteJSONError(\u001b[39m'\u001b[39m\u001b[39mIncomplete JSON content\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 231\u001b[0m     \u001b[39mraise\u001b[39;00m UnexpectedSymbol(symbol, pos)\n\u001b[0;32m    232\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     send((\u001b[39m'\u001b[39m\u001b[39mnumber\u001b[39m\u001b[39m'\u001b[39m, number))\n",
      "\u001b[1;31mUnexpectedSymbol\u001b[0m: Unexpected symbol 'N' at 103"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "\n",
    "filename = 'dblpv13.json'\n",
    "\n",
    "def process_large_json(file_path):\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        parser = ijson.parse(file)\n",
    "        for prefix, event, value in parser:\n",
    "            if (prefix.endswith('.title') or \n",
    "                prefix.endswith('.authors.item._id') or \n",
    "                prefix.endswith('.authors.item.name') or \n",
    "                prefix.endswith('.references.item') or \n",
    "                prefix.endswith('._id')):\n",
    "                print(f'{prefix}: {value}')\n",
    "\n",
    "# Call the function with the path to your JSON file\n",
    "process_large_json(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': '25', 'other': 'some text (with parentheses)'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# Assuming data_str is a string containing your JSON data\n",
    "data_str = '''\n",
    "{\n",
    "    \"age\": \"NumberInt(25)\",\n",
    "    \"other\": \"some text (with parentheses)\"\n",
    "}\n",
    "'''\n",
    "\n",
    "def clean_number_int(data_str):\n",
    "    return re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', data_str)\n",
    "\n",
    "cleaned_data_str = clean_number_int(data_str)\n",
    "data = json.loads(cleaned_data_str)\n",
    "\n",
    "# Now data is a clean dictionary\n",
    "print(data)  # Output: {'age': '25', 'other': 'some text (with parentheses)'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age : 25\n",
      "other : some text (with parentheses)\n"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "import re\n",
    "import io\n",
    "\n",
    "# Assuming data_str is a string containing your JSON data\n",
    "data_str = '''\n",
    "{\n",
    "    \"age\": \"NumberInt(25)\",\n",
    "    \"other\": \"some text (with parentheses)\"\n",
    "}\n",
    "'''\n",
    "\n",
    "def clean_number_int(data_str):\n",
    "    return re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', data_str)\n",
    "\n",
    "# Clean the JSON data string\n",
    "cleaned_data_str = clean_number_int(data_str)\n",
    "\n",
    "# Convert the cleaned JSON string to a StringIO object\n",
    "cleaned_data_io = io.StringIO(cleaned_data_str)\n",
    "\n",
    "# Parse the cleaned JSON data with ijson\n",
    "parser = ijson.parse(cleaned_data_io)\n",
    "for prefix, event, value in parser:\n",
    "    if prefix:\n",
    "        print(f'{prefix} : {value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedSymbol",
     "evalue": "Unexpected symbol 'N' at 103",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\backends\\python.py:225\u001b[0m, in \u001b[0;36mparse_value\u001b[1;34m(target, multivalue, use_float)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     number \u001b[39m=\u001b[39m to_number(symbol)\n\u001b[0;32m    226\u001b[0m     \u001b[39mif\u001b[39;00m number \u001b[39m==\u001b[39m inf:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\common.py:199\u001b[0m, in \u001b[0;36minteger_or_decimal\u001b[1;34m(str_value)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m str_value \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m str_value \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mE\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m str_value):\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39;49m(str_value)\n\u001b[0;32m    200\u001b[0m \u001b[39mreturn\u001b[39;00m decimal\u001b[39m.\u001b[39mDecimal(str_value)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'N'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnexpectedSymbol\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         buffer \u001b[39m=\u001b[39m buffer\u001b[39m.\u001b[39mrstrip(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdblpv13.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m stream_mongo_json(file_path):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mprint\u001b[39m(item)  \u001b[39m# Each item here is a Python dictionary\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 18\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X30sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m parser \u001b[39m=\u001b[39m ijson\u001b[39m.\u001b[39mparse(\u001b[39mopen\u001b[39m(file_path, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m buffer \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m prefix, event, value \u001b[39min\u001b[39;00m parser:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m event \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mstart_map\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mstart_array\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         buffer \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m event \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mstart_map\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m[\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\utils.py:55\u001b[0m, in \u001b[0;36mcoros2gen\u001b[1;34m(source, *coro_pipeline)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m source:\n\u001b[0;32m     54\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m         f\u001b[39m.\u001b[39;49msend(value)\n\u001b[0;32m     56\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m         \u001b[39mfor\u001b[39;00m event \u001b[39min\u001b[39;00m events:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\backends\\python.py:44\u001b[0m, in \u001b[0;36mutf8_encoder\u001b[1;34m(target)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39mraise\u001b[39;00m common\u001b[39m.\u001b[39mIncompleteJSONError(e)\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m sdata:\n\u001b[1;32m---> 44\u001b[0m     send(sdata)\n\u001b[0;32m     45\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m bdata:\n\u001b[0;32m     46\u001b[0m     target\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\backends\\python.py:103\u001b[0m, in \u001b[0;36mLexer\u001b[1;34m(target)\u001b[0m\n\u001b[0;32m    101\u001b[0m             match \u001b[39m=\u001b[39m LEXEME_RE\u001b[39m.\u001b[39msearch(buf, pos)\n\u001b[0;32m    102\u001b[0m             lexeme \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup()\n\u001b[1;32m--> 103\u001b[0m         send((discarded \u001b[39m+\u001b[39;49m match\u001b[39m.\u001b[39;49mstart(), lexeme))\n\u001b[0;32m    104\u001b[0m         pos \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mend()\n\u001b[0;32m    105\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m     \u001b[39m# Don't ask data from an already exhausted source\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\backends\\python.py:231\u001b[0m, in \u001b[0;36mparse_value\u001b[1;34m(target, multivalue, use_float)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mstartswith(symbol) \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mfalse\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mstartswith(symbol) \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mnull\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mstartswith(symbol):\n\u001b[0;32m    230\u001b[0m         \u001b[39mraise\u001b[39;00m common\u001b[39m.\u001b[39mIncompleteJSONError(\u001b[39m'\u001b[39m\u001b[39mIncomplete JSON content\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 231\u001b[0m     \u001b[39mraise\u001b[39;00m UnexpectedSymbol(symbol, pos)\n\u001b[0;32m    232\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     send((\u001b[39m'\u001b[39m\u001b[39mnumber\u001b[39m\u001b[39m'\u001b[39m, number))\n",
      "\u001b[1;31mUnexpectedSymbol\u001b[0m: Unexpected symbol 'N' at 103"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "import bson\n",
    "import json\n",
    "\n",
    "def stream_mongo_json(file_path):\n",
    "    parser = ijson.parse(open(file_path, 'r', encoding='utf-8'))\n",
    "    buffer = \"\"\n",
    "    for prefix, event, value in parser:\n",
    "        if event in ('start_map', 'start_array'):\n",
    "            buffer += '{' if event == 'start_map' else '['\n",
    "        elif event in ('end_map', 'end_array'):\n",
    "            buffer += '}' if event == 'end_map' else ']'\n",
    "            if prefix == '':\n",
    "                # When we close the main object/array, parse and yield\n",
    "                yield bson.json_util.loads(buffer)\n",
    "                buffer = \"\"\n",
    "        elif event == 'map_key':\n",
    "            buffer += json.dumps(value) + ':'\n",
    "        else:\n",
    "            buffer += json.dumps(value) + ','\n",
    "        buffer = buffer.rstrip(',')\n",
    "\n",
    "file_path = 'dblpv13.json'\n",
    "for item in stream_mongo_json(file_path):\n",
    "    print(item)  # Each item here is a Python dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedSymbol",
     "evalue": "Unexpected symbol 'N' at 7581939",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\backends\\python.py:225\u001b[0m, in \u001b[0;36mparse_value\u001b[1;34m(target, multivalue, use_float)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     number \u001b[39m=\u001b[39m to_number(symbol)\n\u001b[0;32m    226\u001b[0m     \u001b[39mif\u001b[39;00m number \u001b[39m==\u001b[39m inf:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\common.py:199\u001b[0m, in \u001b[0;36minteger_or_decimal\u001b[1;34m(str_value)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m str_value \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m str_value \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mE\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m str_value):\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39;49m(str_value)\n\u001b[0;32m    200\u001b[0m \u001b[39mreturn\u001b[39;00m decimal\u001b[39m.\u001b[39mDecimal(str_value)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'N'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnexpectedSymbol\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 18\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X26sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Use ijson to parse the preprocessed file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X26sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m data \u001b[39m=\u001b[39m ijson\u001b[39m.\u001b[39mitems(preprocessed_file, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X26sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X26sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     title \u001b[39m=\u001b[39m item\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X26sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     authors \u001b[39m=\u001b[39m [author[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m author \u001b[39min\u001b[39;00m item\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mauthors\u001b[39m\u001b[39m'\u001b[39m, [])]\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\utils.py:55\u001b[0m, in \u001b[0;36mcoros2gen\u001b[1;34m(source, *coro_pipeline)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m source:\n\u001b[0;32m     54\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m         f\u001b[39m.\u001b[39;49msend(value)\n\u001b[0;32m     56\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m         \u001b[39mfor\u001b[39;00m event \u001b[39min\u001b[39;00m events:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\backends\\python.py:44\u001b[0m, in \u001b[0;36mutf8_encoder\u001b[1;34m(target)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39mraise\u001b[39;00m common\u001b[39m.\u001b[39mIncompleteJSONError(e)\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m sdata:\n\u001b[1;32m---> 44\u001b[0m     send(sdata)\n\u001b[0;32m     45\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m bdata:\n\u001b[0;32m     46\u001b[0m     target\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\backends\\python.py:103\u001b[0m, in \u001b[0;36mLexer\u001b[1;34m(target)\u001b[0m\n\u001b[0;32m    101\u001b[0m             match \u001b[39m=\u001b[39m LEXEME_RE\u001b[39m.\u001b[39msearch(buf, pos)\n\u001b[0;32m    102\u001b[0m             lexeme \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup()\n\u001b[1;32m--> 103\u001b[0m         send((discarded \u001b[39m+\u001b[39;49m match\u001b[39m.\u001b[39;49mstart(), lexeme))\n\u001b[0;32m    104\u001b[0m         pos \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mend()\n\u001b[0;32m    105\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m     \u001b[39m# Don't ask data from an already exhausted source\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\backends\\python.py:231\u001b[0m, in \u001b[0;36mparse_value\u001b[1;34m(target, multivalue, use_float)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mstartswith(symbol) \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mfalse\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mstartswith(symbol) \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mnull\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mstartswith(symbol):\n\u001b[0;32m    230\u001b[0m         \u001b[39mraise\u001b[39;00m common\u001b[39m.\u001b[39mIncompleteJSONError(\u001b[39m'\u001b[39m\u001b[39mIncomplete JSON content\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 231\u001b[0m     \u001b[39mraise\u001b[39;00m UnexpectedSymbol(symbol, pos)\n\u001b[0;32m    232\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     send((\u001b[39m'\u001b[39m\u001b[39mnumber\u001b[39m\u001b[39m'\u001b[39m, number))\n",
      "\u001b[1;31mUnexpectedSymbol\u001b[0m: Unexpected symbol 'N' at 7581939"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "import re\n",
    "\n",
    "filename = 'dblpv13.json'\n",
    "\n",
    "class PreprocessedFile:\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        self.buffer = b\"\"\n",
    "\n",
    "    def read(self, size=-1):\n",
    "        while True:\n",
    "            chunk = self.file.read(size)\n",
    "            if not chunk:\n",
    "                if self.buffer:\n",
    "                    result, self.buffer = self.buffer, b\"\"\n",
    "                    return result\n",
    "                else:\n",
    "                    return b\"\"\n",
    "            self.buffer += chunk\n",
    "            try:\n",
    "                modified_buffer = re.sub(b'NumberInt\\((\\d+)\\)', b'\\\\1', self.buffer)\n",
    "                modified_buffer = re.sub(b'NumberLong\\((\\d+)\\)', b'\\\\1', modified_buffer)\n",
    "                result, self.buffer = modified_buffer, b\"\"\n",
    "                return result\n",
    "            except re.error:\n",
    "                pass\n",
    "\n",
    "\n",
    "# Open the JSON filename to read it\n",
    "with open(filename, 'rb') as file:\n",
    "    # Create a PreprocessedFile object\n",
    "    preprocessed_file = PreprocessedFile(file)\n",
    "    \n",
    "    # Use ijson to parse the preprocessed file\n",
    "    data = ijson.items(preprocessed_file, '')\n",
    "    for item in data:\n",
    "        title = item.get('title')\n",
    "        authors = [author['name'] for author in item.get('authors', [])]\n",
    "        venue_name = item.get('venue', {}).get('name_d')\n",
    "        year = item.get('year')\n",
    "        \n",
    "        print(\"Title:\", title)\n",
    "        print(\"Authors:\", authors)\n",
    "        print(\"Venue Name:\", venue_name)\n",
    "        print(\"Year:\", year)\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 91 10\n",
      "An unexpected error occurred: too many values to unpack (expected 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\denis.iglesias\\AppData\\Local\\Temp\\ipykernel_40140\\1481585169.py\", line 15, in <module>\n",
      "    for prefix, event, value in parser:  # Unpack three values here\n",
      "  File \"c:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\Lib\\site-packages\\ijson\\utils.py\", line 55, in coros2gen\n",
      "    f.send(value)\n",
      "  File \"c:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\Lib\\site-packages\\ijson\\common.py\", line 72, in parse_basecoro\n",
      "    event, value = yield\n",
      "    ^^^^^^^^^^^^\n",
      "ValueError: too many values to unpack (expected 2)\n"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "import re\n",
    "\n",
    "filename = 'dblpv13.json'\n",
    "\n",
    "def preprocess_file(file):\n",
    "    for line in file:\n",
    "        # Use a raw byte string literal for the pattern to avoid SyntaxWarning\n",
    "        modified_line = re.sub(br'NumberInt\\((\\d+)\\)', br'\\1', line)\n",
    "        yield modified_line\n",
    "\n",
    "try:\n",
    "    with open(filename, 'rb') as f:\n",
    "        parser = ijson.parse(preprocess_file(f))\n",
    "        for prefix, event, value in parser:  # Unpack three values here\n",
    "            print(prefix, event, value)\n",
    "            if event == 'string':\n",
    "                print('Got string:', value)\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(\"An unexpected error occurred:\", e)\n",
    "    traceback.print_exc()  # This will print the full traceback\n",
    "except ValueError as e:\n",
    "    print(\"Value Error:\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mfor\u001b[39;00m article \u001b[39min\u001b[39;00m ijson\u001b[39m.\u001b[39mitems(stream, \u001b[39m'\u001b[39m\u001b[39mitem\u001b[39m\u001b[39m'\u001b[39m, use_float\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m             \u001b[39mprint\u001b[39m(article)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m articles_generator(filename2)\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39marticles_generator\u001b[39m(filename):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         stream \u001b[39m=\u001b[39m corrected_json_string(file)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39mprint\u001b[39m(stream[:\u001b[39m1000\u001b[39m])  \u001b[39m# printing the first 1000 characters\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mfor\u001b[39;00m article \u001b[39min\u001b[39;00m ijson\u001b[39m.\u001b[39mitems(stream, \u001b[39m'\u001b[39m\u001b[39mitem\u001b[39m\u001b[39m'\u001b[39m, use_float\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcorrected_json_string\u001b[39m(file):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(re\u001b[39m.\u001b[39;49msub(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mNumberInt\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m((\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39md+)\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m)\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m'\u001b[39;49m, line) \u001b[39mfor\u001b[39;49;00m line \u001b[39min\u001b[39;49;00m file)\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 18\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcorrected_json_string\u001b[39m(file):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X24sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNumberInt\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m((\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m, line) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m file)\n",
      "File \u001b[1;32m<frozen codecs>:319\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "import datetime\n",
    "\n",
    "filename1 = 'biggertest.json'\n",
    "filename2 = 'dblpv13.json'\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"testtest\"))\n",
    "\n",
    "def corrected_json_string(file):\n",
    "    return ''.join(re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', line) for line in file)\n",
    "\n",
    "\n",
    "def articles_generator(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        stream = corrected_json_string(file)\n",
    "        print(stream[:1000])  # printing the first 1000 characters\n",
    "        for article in ijson.items(stream, 'item', use_float=True):\n",
    "            print(article)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "articles_generator(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import re\n",
    "import io\n",
    "\n",
    "filename1 = 'biggertest.json'\n",
    "\n",
    "def clean_number_int(data_str):\n",
    "    return re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', data_str)\n",
    "\n",
    "# Read the JSON data from the file\n",
    "with open(filename1, 'r', encoding='utf-8') as file:\n",
    "    data_str = file.read()\n",
    "\n",
    "# Clean the JSON data string\n",
    "cleaned_data_str = clean_number_int(data_str)\n",
    "\n",
    "# Convert the cleaned JSON string to a StringIO object\n",
    "cleaned_data_io = io.StringIO(cleaned_data_str)\n",
    "\n",
    "# Parse the cleaned JSON data with ijson\n",
    "objects = ijson.items(cleaned_data_io, 'item')\n",
    "\n",
    "# Collect the data into a list of dictionaries\n",
    "data_list = [obj for obj in objects]\n",
    "\n",
    "# Print the data in a structured format\n",
    "for i, item in enumerate(data_list, 1):\n",
    "    print(f'Item {i}:')\n",
    "    for key, value in item.items():\n",
    "        print(f'  {key}: {value}')\n",
    "    print()  # Print a blank line between items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1:\n",
      "  _id: 53e99784b7602d9701f3e3f5\n",
      "  title: 3GIO.\n",
      "  authors: []\n",
      "  references: []\n",
      "\n",
      "Item 2:\n",
      "  _id: 53e99784b7602d9701f3e133\n",
      "  title: The relationship between canopy parameters and spectrum of winter wheat under different irrigations in Hebei Province.\n",
      "  authors: [{'_id': '53f45728dabfaec09f209538', 'name': 'Peijuan Wang'}, {'_id': '5601754345cedb3395e59457', 'name': 'Jiahua Zhang'}, {'_id': '53f38438dabfae4b34a08928', 'name': 'Donghui Xie'}, {'_id': '5601754345cedb3395e5945a', 'name': 'Yanyan Xu'}, {'_id': '53f43d25dabfaeecd6995149', 'name': 'Yun Xu'}]\n",
      "  references: []\n",
      "\n",
      "Item 3:\n",
      "  _id: 53e99784b7602d9701f3e151\n",
      "  title: A solution to the problem of touching and broken characters.\n",
      "  authors: [{'_id': '53f46797dabfaeb22f542630', 'name': 'Jairo Rocha'}, {'_id': '54328883dabfaeb4c6a8a699', 'name': 'Theo Pavlidis'}]\n",
      "  references: ['53e99cf5b7602d97025ace63', '557e8a7a6fee0fe990caa63d', '53e9a96cb7602d97032c459a', '53e9b929b7602d9704515791', '557e59ebf6678c77ea222447']\n",
      "\n",
      "Item 4:\n",
      "  _id: 53e99784b7602d9701f3e15d\n",
      "  title: Timing yield estimation using statistical static timing analysis\n",
      "  authors: [{'_id': '53f43b03dabfaedce555bf2a', 'name': 'Min Pan'}, {'_id': '53f45ee9dabfaee43ecda842', 'name': 'Chris C. N. Chu'}, {'_id': '53f42e8cdabfaee1c0a4274e', 'name': 'Hai Zhou'}]\n",
      "  references: ['53e9a8a9b7602d97031f6bb9', '599c7b6b601a182cd27360da', '53e9b443b7602d9703f3e52b', '53e9a6a6b7602d9702fdc57e', '599c7b6a601a182cd2735703', '53e9aad9b7602d970345afea', '5582821f0cf2bf7bae57ac18', '5e8911859fced0a24bb9a2ba', '53e9b002b7602d9703a5c932']\n",
      "\n",
      "Item 5:\n",
      "  _id: 53e99784b7602d9701f3e161\n",
      "  title: 360°\n",
      "  authors: [{'_id': '53f46946dabfaec09f24b4ed', 'name': 'Miguel Palma'}]\n",
      "  references: []\n",
      "\n",
      "Item 6:\n",
      "  _id: 53e99784b7602d9701f3e162\n",
      "  title: 300\n",
      "  authors: [{'_id': '53f43d95dabfaedf435b63fa', 'name': 'Maureen Squillace'}]\n",
      "  references: []\n",
      "\n",
      "Item 7:\n",
      "  _id: 53e99784b7602d9701f3e165\n",
      "  title: 34957+70764=105621\n",
      "  authors: [{'_id': '54484654dabfae87b7dfc077', 'name': 'Jon G. Hall'}]\n",
      "  references: []\n",
      "\n",
      "Item 8:\n",
      "  _id: 53e99784b7602d9701f3e922\n",
      "  title: International Conference on Nano/Micro Engineered and Molecular Systems\n",
      "  authors: [{'_id': '53f39e3edabfae4b34aa8c4a', 'name': 'Jungil Park'}, {'_id': '53f431bcdabfaee2a1cb41b5', 'name': 'Sunyoung Ahn'}, {'_id': '53f46ac3dabfaeee22a63eab', 'name': 'Youngmi Kim Pak'}, {'_id': '53f44f6adabfaedf435efcb8', 'name': 'James Jungho Pak'}]\n",
      "  references: []\n",
      "\n",
      "Item 9:\n",
      "  _id: 53e99784b7602d9701f3e4f4\n",
      "  title: 2BTextures\n",
      "  authors: [{'_id': '53f45ad4dabfaee1c0b3e206', 'name': 'Bonnie Mitchell'}]\n",
      "  references: []\n",
      "\n",
      "Item 10:\n",
      "  _id: 53e99784b7602d9701f3eaf2\n",
      "  title: 11MonkeysII\n",
      "  authors: [{'_id': '53f438d0dabfaeee229c1f1c', 'name': 'Naotaka Tanaka'}, {'_id': '53f47083dabfaeee22a79321', 'name': 'Mio Yamamoto'}]\n",
      "  references: []\n",
      "\n",
      "Item 11:\n",
      "  _id: 53e99784b7602d9701f3eaf4\n",
      "  title: 1may\n",
      "  authors: [{'_id': '53f4312edabfaee02ac9110a', 'name': 'Daniel Zdunczyk'}]\n",
      "  references: []\n",
      "\n",
      "Item 12:\n",
      "  _id: 53e99784b7602d9701f3eaf6\n",
      "  title: 1\n",
      "  authors: [{'_id': '560175b545cedb3395e59d80', 'name': 'C. Evans'}, {'_id': '53f42d5edabfaedce54c5021', 'name': 'S. Abu Rmeileh'}]\n",
      "  references: []\n",
      "\n",
      "Item 13:\n",
      "  _id: 53e99784b7602d9701f3f177\n",
      "  title: 6th\n",
      "  authors: [{'_id': '53f42d84dabfaee1c0a355c4', 'name': 'Damien Perritaz'}, {'_id': '53f46fa1dabfaec09f263a80', 'name': 'Christophe Salzmann'}, {'_id': '541023addabfae450f4d5127', 'name': 'Denis Gillet'}, {'_id': '53f42b9fdabfaedf434faea8', 'name': 'Olivier Naef'}, {'_id': '53f454cddabfaec09f2009b7', 'name': 'Jacques Bapst'}, {'_id': '53f47a8ddabfaee43ed42c17', 'name': 'Frédéric Barras'}, {'_id': '54096818dabfae8faa68bc76', 'name': 'Elena Mugellini'}, {'_id': '54865672dabfae8a11fb31aa', 'name': 'Omar Abou Khaled'}]\n",
      "  references: []\n",
      "\n",
      "Item 14:\n",
      "  _id: 53e99784b7602d9701f3f326\n",
      "  title: 458nm\n",
      "  authors: [{'_id': '53f44a4adabfaedd74dfe5c0', 'name': ' Institute of Animation, Visual Effects and Digital Postproduction'}]\n",
      "  references: []\n",
      "\n",
      "Item 15:\n",
      "  _id: 53e99784b7602d9701f3f327\n",
      "  title: 49\n",
      "  authors: [{'_id': '53f42fd0dabfaedd74d553b0', 'name': 'Ichiro Iwano'}]\n",
      "  references: []\n",
      "\n",
      "Item 16:\n",
      "  _id: 53e99784b7602d9701f3f4d1\n",
      "  title: 893\n",
      "  authors: [{'_id': '53f459b8dabfaeecd69f7b93', 'name': 'Anne Brotot'}]\n",
      "  references: []\n",
      "\n",
      "Item 17:\n",
      "  _id: 53e99784b7602d9701f3f4d2\n",
      "  title: 8848\n",
      "  authors: [{'_id': '53f463b3dabfaee4dc8430d5', 'name': 'Annabel Sebag'}]\n",
      "  references: []\n",
      "\n",
      "Item 18:\n",
      "  _id: 53e99784b7602d9701f3f352\n",
      "  title: 4IM.\n",
      "  authors: []\n",
      "  references: []\n",
      "\n",
      "Item 19:\n",
      "  _id: 53e99784b7602d9701f3f0b9\n",
      "  title: 7373170279850\n",
      "  authors: [{'_id': '53f42c7cdabfaedf43509437', 'name': 'Jean-marc Deshouillers'}, {'_id': '53f4d04ddabfaeeee2f8199d', 'name': 'François Hennecart'}, {'_id': '53f431fedabfaee43ec009a6', 'name': 'Bernard Landreau'}, {'_id': '53f46fe4dabfaeee22a770f3', 'name': 'I. Gusti Putu Purnaba'}]\n",
      "  references: []\n",
      "\n",
      "Item 20:\n",
      "  _id: 53e99784b7602d9701f3f8c1\n",
      "  title: Foreword\n",
      "  authors: [{'_id': '53f49508dabfaeb4c677b4a4', 'name': 'Hiromasa Habuchi'}, {'_id': '53f43522dabfaee4dc7780b2', 'name': None}, {'_id': '560175f745cedb3395e5a530', 'name': None}]\n",
      "  references: []\n",
      "\n",
      "Item 21:\n",
      "  _id: 53e99784b7602d9701f3f8c2\n",
      "  title: Foreword.\n",
      "  authors: [{'_id': '53f46210dabfaedf43634cba', 'name': 'El Mohajir, Mohammed'}, {'_id': None, 'name': 'Begdouri, Ahlame'}, {'_id': '560175f745cedb3395e5a536', 'name': 'El Mohajir, Badreddine'}, {'_id': '560175f745cedb3395e5a539', 'name': 'Zarghili, Arsalane'}]\n",
      "  references: []\n",
      "\n",
      "Item 22:\n",
      "  _id: 53e99784b7602d9701f3f8c3\n",
      "  title: Forward\n",
      "  authors: [{'_id': '53f4d429dabfaeedd3781ccf', 'name': 'J.R. Gat'}, {'_id': '53f459b8dabfaee0d9c00acb', 'name': 'J. Kushnir'}, {'_id': '54093fbcdabfae450f46f78e', 'name': 'A. Nissenbaum'}]\n",
      "  references: []\n",
      "\n",
      "Item 23:\n",
      "  _id: 53e99784b7602d9701f3f8d2\n",
      "  title: Foreword.\n",
      "  authors: [{'_id': '53f43c87dabfaedf435b42c8', 'name': 'McDonald Mark W'}, {'_id': '560175f745cedb3395e5a53c', 'name': 'McMullen Kevin P'}]\n",
      "  references: []\n",
      "\n",
      "Item 24:\n",
      "  _id: 53e99784b7602d9701f3f411\n",
      "  title: Using XML to Integrate Existing Software Systems into the Web\n",
      "  authors: [{'_id': '548a2e3ddabfae9b40134fbc', 'name': 'Harry M. Sneed'}]\n",
      "  references: ['53e9adbdb7602d97037be8a2', '53e9bb53b7602d9704792f33', '558aa425e4b0b32fcb37fff4', '558abd44e4b031bae1f9653a', '53e9a326b7602d9702c32229', '53e9b1d7b7602d9703c6ce7c', '558a7de784ae84d265bdee99', '53e9ae17b7602d9703828d13', '53e9aa4fb7602d97033bf9ad']\n",
      "\n",
      "Item 25:\n",
      "  _id: 53e99784b7602d9701f3f414\n",
      "  title: 90°\n",
      "  authors: [{'_id': '53f463b3dabfaee4dc8430d5', 'name': 'Annabel Sebag'}]\n",
      "  references: []\n",
      "\n",
      "Item 26:\n",
      "  _id: 53e99784b7602d9701f3f908\n",
      "  title: Foreword.\n",
      "  authors: [{'_id': '54328b5ddabfaeb4c6a8b965', 'name': 'W.R. Butt'}, {'_id': '53f4d12fdabfaef00ef80dde', 'name': 'D.R. London'}]\n",
      "  references: []\n",
      "\n",
      "Item 27:\n",
      "  _id: 53e99784b7602d9701f3f909\n",
      "  title: Foreword]\n",
      "  authors: [{'_id': None, 'name': 'P WEILLER'}]\n",
      "  references: []\n",
      "\n",
      "Item 28:\n",
      "  _id: 53e99784b7602d9701f3f913\n",
      "  title: Foreword\n",
      "  authors: [{'_id': '53f56155dabfae5bb7f8045d', 'name': 'Bird, Christian'}, {'_id': '53f56b5bdabfae6e20f805cd', 'name': 'Menzies, Tim'}, {'_id': '560175f845cedb3395e5a548', 'name': 'Zimmermann, Thomas'}, {'_id': None, 'name': None}]\n",
      "  references: []\n",
      "\n",
      "Item 29:\n",
      "  _id: 53e99784b7602d9701f3f5fe\n",
      "  title: Research on resource allocation for multi-tier web applications in a virtualization environment\n",
      "  authors: [{'_id': '53f46a22dabfaee0d9c3d5e5', 'name': 'Shuguo Yang'}]\n",
      "  references: ['53e9a073b7602d9702957efa', '53e9ad87b7602d970377bfb5', '53e9be51b7602d9704b11381', '53e9be04b7602d9704abb31d', '53e9992bb7602d9702169236', '53e998cdb7602d97021044db', '53e9afa6b7602d97039f6054', '53e99822b7602d9702044e60']\n",
      "\n",
      "Item 30:\n",
      "  _id: 53e99784b7602d9701f3f600\n",
      "  title: BUSTRAP – an efficient travel planner for metropolitans\n",
      "  authors: [{'_id': '560175ed45cedb3395e5a3a0', 'name': 'Sandeep Gupta'}, {'_id': '53f42d8bdabfaec22ba1a1e3', 'name': 'M. M. Gore'}]\n",
      "  references: []\n",
      "\n",
      "Item 31:\n",
      "  _id: 53e99784b7602d9701f3f92a\n",
      "  title: Foreword.\n",
      "  authors: [{'_id': None, 'name': 'Taleisnik Julio'}]\n",
      "  references: []\n",
      "\n",
      "Item 32:\n",
      "  _id: 53e99784b7602d9701f3f60c\n",
      "  title: Fairness\n",
      "  authors: [{'_id': '53f6339bdabfae2722c7163e', 'name': 'Nissim Francez'}]\n",
      "  references: []\n",
      "\n",
      "Item 33:\n",
      "  _id: 53e99784b7602d9701f3f60f\n",
      "  title: FOREWORD.\n",
      "  authors: [{'_id': None, 'name': 'Mahidol C'}, {'_id': '560175ed45cedb3395e5a3a6', 'name': 'Hostettmann K'}]\n",
      "  references: []\n",
      "\n",
      "Item 34:\n",
      "  _id: 53e99784b7602d9701f3f955\n",
      "  title: FIRST\n",
      "  authors: [{'_id': '53f4395ddabfaee2a1d02a9e', 'name': 'Michael D. Kelly'}, {'_id': '53f3aaabdabfae4b34af5123', 'name': 'Sean J. Geoghegan'}]\n",
      "  references: []\n",
      "\n",
      "Item 35:\n",
      "  _id: 53e99784b7602d9701f3f956\n",
      "  title: Formatics.\n",
      "  authors: [{'_id': '548999a7dabfae9b40134ba3', 'name': 'Paul Beynon-Davies'}]\n",
      "  references: []\n",
      "\n",
      "Item 36:\n",
      "  _id: 53e99784b7602d9701f3f957\n",
      "  title: Fault\n",
      "  authors: [{'_id': '53f3a30fdabfae4b34ac6874', 'name': 'S. V. Sandhya'}, {'_id': '53f42ce6dabfaec09f10dd58', 'name': 'H. A. Sanjay'}, {'_id': '53f45018dabfaee43eca4008', 'name': 'S. J. Netravathi'}, {'_id': '53f454a2dabfaeee22a2db26', 'name': 'M. V. Sowmyashree'}, {'_id': '53f42cecdabfaeb1a7b84856', 'name': 'R. N. Yogeshwari'}]\n",
      "  references: []\n",
      "\n",
      "Item 37:\n",
      "  _id: 53e99784b7602d9701f3f958\n",
      "  title: Flexonics.\n",
      "  authors: [{'_id': '560175f845cedb3395e5a54e', 'name': 'John Canny'}, {'_id': '53f42ef2dabfaee1c0a47ad5', 'name': 'Jeremy Risner'}, {'_id': '540548eddabfae92b41be5a4', 'name': 'Vivek Subramanian'}]\n",
      "  references: []\n",
      "\n",
      "Item 38:\n",
      "  _id: 53e99784b7602d9701f3f959\n",
      "  title: Frontmatter\n",
      "  authors: [{'_id': '53f42cf5dabfaeb2acfe425e', 'name': 'C. J. Gross'}, {'_id': '53f458aadabfaec09f20ed3a', 'name': 'W. Nazarewicz'}, {'_id': '53f45a48dabfaee2a1d82ade', 'name': 'K. P. Rykaczewski'}]\n",
      "  references: []\n",
      "\n",
      "Item 39:\n",
      "  _id: 53e99784b7602d9701f3f95a\n",
      "  title: FACETS\n",
      "  authors: [{'_id': '53f4671fdabfaee43ecf9787', 'name': 'John R. Cary'}, {'_id': '53f36fd8dabfae4b349bd343', 'name': 'Ammar Hakim'}, {'_id': '53f432c5dabfaedf4355a7ff', 'name': 'Mahmood Miah'}, {'_id': '53f428b2dabfaec22b9e0df5', 'name': 'Scott Kruger'}, {'_id': '53f4375ddabfaee1c0aa5fd8', 'name': 'Alexander Pletzer'}, {'_id': '53f4693bdabfaedd74e7532c', 'name': 'Svetlana G. Shasharina'}, {'_id': '53f431f3dabfaeb2ac023e95', 'name': 'Srinath Vadlamani'}, {'_id': '53f4669fdabfaeee22a54505', 'name': 'Ronald Cohen'}, {'_id': '53f47432dabfaeecd6a41d75', 'name': 'Thomas Epperly'}, {'_id': '53f456d3dabfaee02ad5a9ac', 'name': 'Tom Rognlien'}, {'_id': '53f356f2dabfae4b3495fbcb', 'name': 'Alexei Pankin'}, {'_id': '53f45ab1dabfaeb22f511541', 'name': 'Richard Groebner'}, {'_id': '53f4359fdabfaeee229a4700', 'name': 'Satish Balay'}, {'_id': '53f44990dabfaee4dc7ddf84', 'name': 'Lois C. McInnes'}, {'_id': '562cb37445cedb3398c9befe', 'name': 'Hong Zhang'}]\n",
      "  references: []\n",
      "\n",
      "Item 40:\n",
      "  _id: 53e99784b7602d9701f3f95b\n",
      "  title: Fisherfaces\n",
      "  authors: [{'_id': '53f46eefdabfaee02adb698d', 'name': 'Aleix Martinez'}]\n",
      "  references: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "import re\n",
    "import io\n",
    "\n",
    "filename1 = 'biggertest.json'\n",
    "\n",
    "def clean_number_int(data_str):\n",
    "    return re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', data_str)\n",
    "\n",
    "# Read the JSON data from the file\n",
    "with open(filename1, 'r', encoding='utf-8') as file:\n",
    "    data_str = file.read()\n",
    "\n",
    "# Clean the JSON data string\n",
    "cleaned_data_str = clean_number_int(data_str)\n",
    "\n",
    "# Convert the cleaned JSON string to a StringIO object\n",
    "cleaned_data_io = io.StringIO(cleaned_data_str)\n",
    "\n",
    "# Parse the cleaned JSON data with ijson\n",
    "objects = ijson.items(cleaned_data_io, 'item')\n",
    "\n",
    "# Collect the data into a list of dictionaries, filtering for desired fields\n",
    "filtered_data_list = []\n",
    "for obj in objects:\n",
    "    filtered_obj = {\n",
    "        '_id': obj.get('_id', None),\n",
    "        'title': obj.get('title', None),\n",
    "        'authors': [\n",
    "            {'_id': author.get('_id', None), 'name': author.get('name', None)}\n",
    "            for author in obj.get('authors', [])\n",
    "        ],\n",
    "        'references': obj.get('references', [])\n",
    "    }\n",
    "    filtered_data_list.append(filtered_obj)\n",
    "\n",
    "# Print the data in a structured format\n",
    "for i, item in enumerate(filtered_data_list, 1):\n",
    "    print(f'Item {i}:')\n",
    "    for key, value in item.items():\n",
    "        print(f'  {key}: {value}')\n",
    "    print()  # Print a blank line between items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Your add_article function here\n",
    "def add_article(tx, article_id, title, author=None, cited_articles=None):\n",
    "    tx.run(\n",
    "        \"MERGE (a:Article {_id: $id}) \"\n",
    "        \"ON CREATE SET a.title = $title \"\n",
    "        \"ON MATCH SET a.title = $title\",\n",
    "        id=article_id,\n",
    "        title=title\n",
    "    )\n",
    "    if author:\n",
    "        author_id = author.get('_id')\n",
    "        author_name = author.get('name')\n",
    "        if author_id and author_name:\n",
    "            tx.run(\n",
    "                \"MERGE (a:Author {_id: $author_id, name: $name})\", \n",
    "                author_id=author_id, \n",
    "                name=author_name\n",
    "            )\n",
    "            tx.run(\"\"\"\n",
    "                MATCH (a:Author {_id: $author_id}), (b:Article {_id: $article_id})\n",
    "                MERGE (a)-[:AUTHORED]->(b)\n",
    "            \"\"\", author_id=author_id, article_id=article_id)\n",
    "    if cited_articles:\n",
    "        for cited_article in cited_articles:\n",
    "            cited_article_id = cited_article.get('_id')\n",
    "            cited_article_title = cited_article.get('title')\n",
    "            if cited_article_id:\n",
    "                tx.run(\n",
    "                    \"MERGE (c:Article {_id: $cited_article_id}) \"\n",
    "                    \"ON CREATE SET c.title = $cited_article_title \"\n",
    "                    \"ON MATCH SET c.title = $cited_article_title\",\n",
    "                    cited_article_id=cited_article_id,\n",
    "                    cited_article_title=cited_article_title\n",
    "                )\n",
    "                tx.run(\"\"\"\n",
    "                    MATCH (a:Article {_id: $article_id}), (c:Article {_id: $cited_article_id})\n",
    "                    MERGE (a)-[:CITES]->(c)\n",
    "                \"\"\", article_id=article_id, cited_article_id=cited_article_id)\n",
    "\n",
    "def articles_generator(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        for article in ijson.items(file, 'item', use_float=True):\n",
    "            yield article\n",
    "\n",
    "def process_article(session, article):\n",
    "    article_id = article['_id']\n",
    "    title = article['title']\n",
    "    for author in article.get('authors', []):\n",
    "        session.execute_write(add_article, article_id, title, author=author)\n",
    "    # Create a list of dictionaries for cited articles, assuming titles are not available\n",
    "    cited_articles = [{'_id': cited_article_id, 'title': None} for cited_article_id in article.get('references', [])]\n",
    "    # Now pass the cited_articles list to add_article\n",
    "    session.execute_write(add_article, article_id, title, cited_articles=cited_articles)\n",
    "\n",
    "def process_and_send_to_neo4j(articles_batch):\n",
    "    with GraphDatabase.driver(\"neo4j://localhost:7687\", auth=(\"neo4j\", \"testtest\")) as driver:\n",
    "        with driver.session() as session:\n",
    "            for article in articles_batch:\n",
    "                process_article(session, article)\n",
    "\n",
    "def main(filename):\n",
    "    batch_size = 100  # Process articles in batches of 100 (adjust as needed)\n",
    "    articles_batch = []\n",
    "    for article in articles_generator(filename):\n",
    "        articles_batch.append(article)\n",
    "        if len(articles_batch) >= batch_size:\n",
    "            process_and_send_to_neo4j(articles_batch)\n",
    "            articles_batch = []\n",
    "    if articles_batch:  # Process any remaining articles\n",
    "        process_and_send_to_neo4j(articles_batch)\n",
    "\n",
    "# Run the main function with the filename as argument\n",
    "filename1 = 'biggertest.json'\n",
    "main(filename1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import re\n",
    "import io\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Your add_article function here\n",
    "def add_article(tx, article_id, title, authors=None, cited_articles=None):\n",
    "    tx.run(\n",
    "        \"MERGE (a:Article {_id: $id}) \"\n",
    "        \"ON CREATE SET a.title = $title \"\n",
    "        \"ON MATCH SET a.title = $title\",\n",
    "        id=article_id,\n",
    "        title=title\n",
    "    )\n",
    "    \n",
    "    if authors:\n",
    "        for author in authors:\n",
    "            author_id = author.get('_id')\n",
    "            author_name = author.get('name')\n",
    "            if author_id and author_name:\n",
    "                tx.run(\n",
    "                    \"MERGE (authorNode:Author {_id: $author_id, name: $name})\", \n",
    "                    author_id=author_id, \n",
    "                    name=author_name\n",
    "                )\n",
    "                tx.run(\"\"\"\n",
    "                    MATCH (authorNode:Author {_id: $author_id}), (a:Article {_id: $article_id})\n",
    "                    MERGE (authorNode)-[:AUTHORED]->(a)\n",
    "                \"\"\", author_id=author_id, article_id=article_id)\n",
    "\n",
    "    if cited_articles:\n",
    "        for cited_article in cited_articles:\n",
    "            cited_article_id = cited_article.get('_id')\n",
    "            cited_article_title = cited_article.get('title')\n",
    "            if cited_article_id:\n",
    "                tx.run(\n",
    "                    \"MERGE (c:Article {_id: $cited_article_id}) \"\n",
    "                    \"ON CREATE SET c.title = $cited_article_title \"\n",
    "                    \"ON MATCH SET c.title = $cited_article_title\",\n",
    "                    cited_article_id=cited_article_id,\n",
    "                    cited_article_title=cited_article_title\n",
    "                )\n",
    "                tx.run(\"\"\"\n",
    "                    MATCH (a:Article {_id: $article_id}), (c:Article {_id: $cited_article_id})\n",
    "                    MERGE (a)-[:CITES]->(c)\n",
    "                \"\"\", article_id=article_id, cited_article_id=cited_article_id)\n",
    "\n",
    "\n",
    "\n",
    "def articles_generator(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        for article in ijson.items(file, 'item', use_float=True):\n",
    "            yield article\n",
    "\n",
    "def process_article(session, article):\n",
    "    print(article)  # Debug line to print entire article data\n",
    "    article_id = article['_id']\n",
    "    title = article['title']\n",
    "    authors = article.get('authors')  # Get the authors from the 'authors' field\n",
    "    print(article_id, title, authors)  # Debug line to print the article data\n",
    "\n",
    "    # Check if 'references' is a key in the article dictionary\n",
    "    references = article.get('references')\n",
    "    if references is None:\n",
    "        references = []\n",
    "    # Create a list of dictionaries for cited articles, assuming titles are not available\n",
    "    cited_articles = [{'_id': cited_article_id, 'title': None} for cited_article_id in references]\n",
    "    #print(cited_articles)  # Debug line to print the cited_articles data\n",
    "\n",
    "    # Now pass the authors and cited_articles lists to add_article\n",
    "    session.execute_write(add_article, article_id, title, authors=authors, cited_articles=cited_articles)\n",
    "\n",
    "\n",
    "def process_and_send_to_neo4j(articles_batch):\n",
    "    with GraphDatabase.driver(\"neo4j://localhost:7687\", auth=(\"neo4j\", \"testtest\")) as driver:\n",
    "        with driver.session() as session:\n",
    "            for article in articles_batch:\n",
    "                process_article(session, article)\n",
    "\n",
    "class PreprocessedFile:\n",
    "    def __init__(self, filename):\n",
    "        self.generator = preprocess_json(filename)\n",
    "        self.buffer = ''\n",
    "\n",
    "    def read(self, size=-1):\n",
    "        while size < 0 or len(self.buffer) < size:\n",
    "            try:\n",
    "                self.buffer += next(self.generator)\n",
    "            except StopIteration:\n",
    "                # End of generator, return what's left in the buffer\n",
    "                break\n",
    "        if size < 0:\n",
    "            result, self.buffer = self.buffer, ''\n",
    "        else:\n",
    "            result, self.buffer = self.buffer[:size], self.buffer[size:]\n",
    "        return result\n",
    "\n",
    "def preprocess_json(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', line)\n",
    "            line = line.replace('NaN', 'null')\n",
    "            yield line\n",
    "\n",
    "def get_cleaned_data(filename):\n",
    "    preprocessed_file = PreprocessedFile(filename)\n",
    "    return ijson.parse(preprocessed_file)  # pass the custom file-like object to ijson.parse\n",
    "\n",
    "def main(filename, neo4j_uri, neo4j_user, neo4j_password):\n",
    "    driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        articles_batch = []  # Initialize an empty batch\n",
    "        batch_size = 100  # Adjust batch size as needed\n",
    "        key = None  # Initialize key to None before the loop\n",
    "        article = {}  # Initialize article to an empty dictionary before the loop\n",
    "        current_list = None  # Initialize current_list to None before the loop\n",
    "        current_dict = None  # Initialize current_dict to None before the loop\n",
    "        \n",
    "        for prefix, event, value in get_cleaned_data(filename):\n",
    "            if event == 'map_key':\n",
    "                key = value\n",
    "            elif key and prefix.endswith(f'.{key}'):\n",
    "                if event == 'start_array':\n",
    "                    current_list = []\n",
    "                elif event == 'end_array':\n",
    "                    article[key] = current_list\n",
    "                    current_list = None\n",
    "                elif current_list is not None:\n",
    "                    if event == 'start_map':\n",
    "                        current_dict = {}\n",
    "                    elif event == 'end_map':\n",
    "                        current_list.append(current_dict)\n",
    "                        current_dict = None\n",
    "                    elif current_dict is not None:\n",
    "                        current_dict[key] = value\n",
    "                    else:\n",
    "                        current_list.append(value)\n",
    "                else:\n",
    "                    article[key] = value\n",
    "            if prefix == 'item' and event == 'end_map':\n",
    "                articles_batch.append(article)  # Append the article to the batch\n",
    "                article = {}  # Reset the article for the next item\n",
    "                if len(articles_batch) >= batch_size:\n",
    "                    process_and_send_to_neo4j(articles_batch)  # Process the batch\n",
    "                    articles_batch = []  # Reset the batch\n",
    "        if articles_batch:  # Process any remaining articles\n",
    "            process_and_send_to_neo4j(articles_batch)  # Process the remaining batch\n",
    "    \n",
    "    driver.close()\n",
    "\n",
    "# Usage\n",
    "filename1 = 'biggertest.json'\n",
    "filename2 = 'dblpv13.json'\n",
    "neo4j_uri = 'bolt://localhost:7687'\n",
    "neo4j_user = 'neo4j'\n",
    "neo4j_password = 'testtest'\n",
    "main(filename1, neo4j_uri, neo4j_user, neo4j_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "BufferError",
     "evalue": "Existing exports of data: object cannot be re-sized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 29\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=208'>209</a>\u001b[0m \u001b[39mif\u001b[39;00m author_lists:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=209'>210</a>\u001b[0m     send_authors_to_neo4j(session, author_lists)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=210'>211</a>\u001b[0m \u001b[39m# add references to neo4j\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 29\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mif\u001b[39;00m author_id \u001b[39mand\u001b[39;00m author_name:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     session\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\"\"\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m        MERGE (authorNode:Author \u001b[39;49m\u001b[39m{\u001b[39;49m\u001b[39m_id: $author_id, name: $name})\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m        MERGE (a:Article \u001b[39;49m\u001b[39m{\u001b[39;49m\u001b[39m_id: $article_id, title: $title})\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m        MERGE (authorNode)-[:AUTHORED]->(a)\u001b[39;49m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m    \u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m, author_id\u001b[39m=\u001b[39;49mauthor_id, name\u001b[39m=\u001b[39;49mauthor_name, title\u001b[39m=\u001b[39;49marticle_title,article_id\u001b[39m=\u001b[39;49marticle_id)\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\work\\session.py:314\u001b[0m, in \u001b[0;36mSession.run\u001b[1;34m(self, query, parameters, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m parameters \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(parameters \u001b[39mor\u001b[39;00m {}, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 314\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_auto_result\u001b[39m.\u001b[39;49m_run(\n\u001b[0;32m    315\u001b[0m     query, parameters, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_config\u001b[39m.\u001b[39;49mdatabase,\n\u001b[0;32m    316\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_config\u001b[39m.\u001b[39;49mimpersonated_user, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_config\u001b[39m.\u001b[39;49mdefault_access_mode,\n\u001b[0;32m    317\u001b[0m     bookmarks, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_config\u001b[39m.\u001b[39;49mnotifications_min_severity,\n\u001b[0;32m    318\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_config\u001b[39m.\u001b[39;49mnotifications_disabled_categories,\n\u001b[0;32m    319\u001b[0m )\n\u001b[0;32m    321\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_auto_result\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\work\\result.py:166\u001b[0m, in \u001b[0;36mResult._run\u001b[1;34m(self, query, parameters, db, imp_user, access_mode, bookmarks, notifications_min_severity, notifications_disabled_categories)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39msend_all()\n\u001b[1;32m--> 166\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_attach()\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\work\\result.py:274\u001b[0m, in \u001b[0;36mResult._attach\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attached \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mfetch_message()\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:180\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 180\u001b[0m     func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    181\u001b[0m \u001b[39mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:848\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[39m# Receive exactly one message\u001b[39;00m\n\u001b[1;32m--> 848\u001b[0m tag, fields \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minbox\u001b[39m.\u001b[39;49mpop(\n\u001b[0;32m    849\u001b[0m     hydration_hooks\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresponses[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mhydration_hooks\n\u001b[0;32m    850\u001b[0m )\n\u001b[0;32m    851\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_message(tag, fields)\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:74\u001b[0m, in \u001b[0;36mInbox.pop\u001b[1;34m(self, hydration_hooks)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpop\u001b[39m(\u001b[39mself\u001b[39m, hydration_hooks):\n\u001b[1;32m---> 74\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer_one_chunk()\n\u001b[0;32m     75\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:53\u001b[0m, in \u001b[0;36mInbox._buffer_one_chunk\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mwhile\u001b[39;00m chunk_size \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     52\u001b[0m     \u001b[39m# Determine the chunk size and skip noop\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     receive_into_buffer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_socket, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer, \u001b[39m2\u001b[39;49m)\n\u001b[0;32m     54\u001b[0m     chunk_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer\u001b[39m.\u001b[39mpop_u16()\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:308\u001b[0m, in \u001b[0;36mreceive_into_buffer\u001b[1;34m(sock, buffer, n_bytes)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[39mwhile\u001b[39;00m buffer\u001b[39m.\u001b[39mused \u001b[39m<\u001b[39m end:\n\u001b[1;32m--> 308\u001b[0m     n \u001b[39m=\u001b[39m sock\u001b[39m.\u001b[39;49mrecv_into(view[buffer\u001b[39m.\u001b[39;49mused:end], end \u001b[39m-\u001b[39;49m buffer\u001b[39m.\u001b[39;49mused)\n\u001b[0;32m    309\u001b[0m     \u001b[39mif\u001b[39;00m n \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_async_compat\\network\\_bolt_socket.py:488\u001b[0m, in \u001b[0;36mBoltSocket.recv_into\u001b[1;34m(self, buffer, nbytes)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrecv_into\u001b[39m(\u001b[39mself\u001b[39m, buffer, nbytes):\n\u001b[1;32m--> 488\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_io(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_socket\u001b[39m.\u001b[39;49mrecv_into, buffer, nbytes)\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_async_compat\\network\\_bolt_socket.py:463\u001b[0m, in \u001b[0;36mBoltSocket._wait_for_io\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deadline \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    464\u001b[0m timeout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_socket\u001b[39m.\u001b[39mgettimeout()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mBufferError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\work\\session.py:215\u001b[0m, in \u001b[0;36mSession.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39msend_all()\n\u001b[1;32m--> 215\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mfetch_all()\n\u001b[0;32m    216\u001b[0m \u001b[39m# TODO: Investigate potential non graceful close states\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:865\u001b[0m, in \u001b[0;36mBolt.fetch_all\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m response\u001b[39m.\u001b[39mcomplete:\n\u001b[1;32m--> 865\u001b[0m     detail_delta, summary_delta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetch_message()\n\u001b[0;32m    866\u001b[0m     detail_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m detail_delta\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:848\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[39m# Receive exactly one message\u001b[39;00m\n\u001b[1;32m--> 848\u001b[0m tag, fields \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minbox\u001b[39m.\u001b[39;49mpop(\n\u001b[0;32m    849\u001b[0m     hydration_hooks\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresponses[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mhydration_hooks\n\u001b[0;32m    850\u001b[0m )\n\u001b[0;32m    851\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_message(tag, fields)\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:74\u001b[0m, in \u001b[0;36mInbox.pop\u001b[1;34m(self, hydration_hooks)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpop\u001b[39m(\u001b[39mself\u001b[39m, hydration_hooks):\n\u001b[1;32m---> 74\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer_one_chunk()\n\u001b[0;32m     75\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:58\u001b[0m, in \u001b[0;36mInbox._buffer_one_chunk\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m         log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39m[#\u001b[39m\u001b[39m%04X\u001b[39;00m\u001b[39m]  S: <NOOP>\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_port)\n\u001b[1;32m---> 58\u001b[0m receive_into_buffer(\n\u001b[0;32m     59\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_socket, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer, chunk_size \u001b[39m+\u001b[39;49m \u001b[39m2\u001b[39;49m\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     61\u001b[0m chunk_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer\u001b[39m.\u001b[39mpop_u16()\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:305\u001b[0m, in \u001b[0;36mreceive_into_buffer\u001b[1;34m(sock, buffer, n_bytes)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m>\u001b[39m \u001b[39mlen\u001b[39m(buffer\u001b[39m.\u001b[39mdata):\n\u001b[1;32m--> 305\u001b[0m     buffer\u001b[39m.\u001b[39mdata \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(end \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(buffer\u001b[39m.\u001b[39mdata))\n\u001b[0;32m    306\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mmemoryview\u001b[39m(buffer\u001b[39m.\u001b[39mdata) \u001b[39mas\u001b[39;00m view:\n",
      "\u001b[1;31mBufferError\u001b[0m: Existing exports of data: object cannot be re-sized",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mBufferError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 29\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=221'>222</a>\u001b[0m neo4j_user \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mneo4j\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=222'>223</a>\u001b[0m neo4j_password \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtesttest\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=223'>224</a>\u001b[0m main(filename2, neo4j_uri, neo4j_user, neo4j_password)\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\notebook_tests.ipynb Cellule 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=193'>194</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m(filename, neo4j_uri, neo4j_user, neo4j_password):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=194'>195</a>\u001b[0m     driver \u001b[39m=\u001b[39m GraphDatabase\u001b[39m.\u001b[39mdriver(neo4j_uri, auth\u001b[39m=\u001b[39m(neo4j_user, neo4j_password))\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=196'>197</a>\u001b[0m     \u001b[39mwith\u001b[39;00m driver\u001b[39m.\u001b[39msession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=197'>198</a>\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/notebook_tests.ipynb#X42sZmlsZQ%3D%3D?line=198'>199</a>\u001b[0m             one_article \u001b[39m=\u001b[39m get_cleaned_data(filename)\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\work\\session.py:127\u001b[0m, in \u001b[0;36mSession.__exit__\u001b[1;34m(self, exception_type, exception_value, traceback)\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state_failed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclose()\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\work\\session.py:226\u001b[0m, in \u001b[0;36mSession.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    225\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m--> 226\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_disconnect()\n\u001b[0;32m    228\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state_failed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_closed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\work\\session.py:142\u001b[0m, in \u001b[0;36mSession._disconnect\u001b[1;34m(self, sync)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_disconnect\u001b[39m(\u001b[39mself\u001b[39m, sync\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    141\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_disconnect(sync\u001b[39m=\u001b[39;49msync)\n\u001b[0;32m    143\u001b[0m     \u001b[39mexcept\u001b[39;00m asyncio\u001b[39m.\u001b[39mCancelledError:\n\u001b[0;32m    144\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_cancellation(message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_disconnect\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\work\\workspace.py:190\u001b[0m, in \u001b[0;36mWorkspace._disconnect\u001b[1;34m(self, sync)\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection:\n\u001b[1;32m--> 190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mrelease(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection)\n\u001b[0;32m    191\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection_access_mode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_pool.py:374\u001b[0m, in \u001b[0;36mIOPool.release\u001b[1;34m(self, *connections)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     log\u001b[39m.\u001b[39mdebug(\n\u001b[0;32m    371\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m[#\u001b[39m\u001b[39m%04X\u001b[39;00m\u001b[39m]  _: <POOL> release unclean connection \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    372\u001b[0m         connection\u001b[39m.\u001b[39mlocal_port, connection\u001b[39m.\u001b[39mconnection_id\n\u001b[0;32m    373\u001b[0m     )\n\u001b[1;32m--> 374\u001b[0m     connection\u001b[39m.\u001b[39;49mreset()\n\u001b[0;32m    375\u001b[0m \u001b[39mexcept\u001b[39;00m (Neo4jError, DriverError, BoltError) \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m    376\u001b[0m     log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39m[#\u001b[39m\u001b[39m%04X\u001b[39;00m\u001b[39m]  _: <POOL> failed to reset connection \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m               \u001b[39m\"\u001b[39m\u001b[39mon release: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, connection\u001b[39m.\u001b[39mlocal_port, exc)\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt5.py:330\u001b[0m, in \u001b[0;36mBolt5x0.reset\u001b[1;34m(self, dehydration_hooks, hydration_hooks)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append(\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\x0F\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    326\u001b[0m              response\u001b[39m=\u001b[39mResponse(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mreset\u001b[39m\u001b[39m\"\u001b[39m, hydration_hooks,\n\u001b[0;32m    327\u001b[0m                                on_failure\u001b[39m=\u001b[39mfail),\n\u001b[0;32m    328\u001b[0m              dehydration_hooks\u001b[39m=\u001b[39mdehydration_hooks)\n\u001b[0;32m    329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend_all()\n\u001b[1;32m--> 330\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetch_all()\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:865\u001b[0m, in \u001b[0;36mBolt.fetch_all\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    863\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponses[\u001b[39m0\u001b[39m]\n\u001b[0;32m    864\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m response\u001b[39m.\u001b[39mcomplete:\n\u001b[1;32m--> 865\u001b[0m     detail_delta, summary_delta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetch_message()\n\u001b[0;32m    866\u001b[0m     detail_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m detail_delta\n\u001b[0;32m    867\u001b[0m     summary_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m summary_delta\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:848\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    845\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m\n\u001b[0;32m    847\u001b[0m \u001b[39m# Receive exactly one message\u001b[39;00m\n\u001b[1;32m--> 848\u001b[0m tag, fields \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minbox\u001b[39m.\u001b[39;49mpop(\n\u001b[0;32m    849\u001b[0m     hydration_hooks\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresponses[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mhydration_hooks\n\u001b[0;32m    850\u001b[0m )\n\u001b[0;32m    851\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_message(tag, fields)\n\u001b[0;32m    852\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39midle_since \u001b[39m=\u001b[39m perf_counter()\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:74\u001b[0m, in \u001b[0;36mInbox.pop\u001b[1;34m(self, hydration_hooks)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpop\u001b[39m(\u001b[39mself\u001b[39m, hydration_hooks):\n\u001b[1;32m---> 74\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer_one_chunk()\n\u001b[0;32m     75\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m         size, tag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unpacker\u001b[39m.\u001b[39munpack_structure_header()\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:58\u001b[0m, in \u001b[0;36mInbox._buffer_one_chunk\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[39mif\u001b[39;00m chunk_size \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     56\u001b[0m         log\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39m[#\u001b[39m\u001b[39m%04X\u001b[39;00m\u001b[39m]  S: <NOOP>\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_port)\n\u001b[1;32m---> 58\u001b[0m receive_into_buffer(\n\u001b[0;32m     59\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_socket, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_buffer, chunk_size \u001b[39m+\u001b[39;49m \u001b[39m2\u001b[39;49m\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     61\u001b[0m chunk_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer\u001b[39m.\u001b[39mpop_u16()\n\u001b[0;32m     63\u001b[0m \u001b[39mif\u001b[39;00m chunk_size \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     64\u001b[0m     \u001b[39m# chunk_size was the end marker for the message\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:305\u001b[0m, in \u001b[0;36mreceive_into_buffer\u001b[1;34m(sock, buffer, n_bytes)\u001b[0m\n\u001b[0;32m    303\u001b[0m end \u001b[39m=\u001b[39m buffer\u001b[39m.\u001b[39mused \u001b[39m+\u001b[39m n_bytes\n\u001b[0;32m    304\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m>\u001b[39m \u001b[39mlen\u001b[39m(buffer\u001b[39m.\u001b[39mdata):\n\u001b[1;32m--> 305\u001b[0m     buffer\u001b[39m.\u001b[39mdata \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(end \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(buffer\u001b[39m.\u001b[39mdata))\n\u001b[0;32m    306\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mmemoryview\u001b[39m(buffer\u001b[39m.\u001b[39mdata) \u001b[39mas\u001b[39;00m view:\n\u001b[0;32m    307\u001b[0m     \u001b[39mwhile\u001b[39;00m buffer\u001b[39m.\u001b[39mused \u001b[39m<\u001b[39m end:\n",
      "\u001b[1;31mBufferError\u001b[0m: Existing exports of data: object cannot be re-sized"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "import re\n",
    "import io\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "def send_articles_to_neo4j(session, article_lists):\n",
    "    for article in article_lists:\n",
    "        article_id = article[0]\n",
    "        article_title = article[1]\n",
    "        \n",
    "        # Create the article node\n",
    "        session.run(\n",
    "            \"MERGE (a:Article {_id: $id}) \"\n",
    "            \"ON CREATE SET a.title = $title \"\n",
    "            \"ON MATCH SET a.title = $title\",\n",
    "            id=article_id,\n",
    "            title=article_title\n",
    "            )\n",
    "\n",
    "def send_authors_to_neo4j(session, author_lists):\n",
    "    for author in author_lists:\n",
    "        article_id = author[0]\n",
    "        article_title = author[1]\n",
    "        author_id = author[2]\n",
    "        author_name = author[3]\n",
    "        \n",
    "        if author_id and author_name:\n",
    "            session.run(\"\"\"\n",
    "                MERGE (authorNode:Author {_id: $author_id, name: $name})\n",
    "                MERGE (a:Article {_id: $article_id, title: $title})\n",
    "                MERGE (authorNode)-[:AUTHORED]->(a)\n",
    "            \"\"\", author_id=author_id, name=author_name, title=article_title,article_id=article_id)\n",
    "\n",
    "def send_references_to_neo4j(session, reference_lists):\n",
    "    for reference in reference_lists:\n",
    "        article_id = reference[0]\n",
    "        reference_article_id = reference[1]\n",
    "        \n",
    "        if reference_article_id:\n",
    "            session.run(\"\"\"\n",
    "                MERGE (a:Article {_id: $article_id})\n",
    "                MERGE (c:Article {_id: $reference_article_id})\n",
    "                MERGE (a)-[:CITES]->(c)\n",
    "            \"\"\", article_id=article_id, reference_article_id=reference_article_id)\n",
    "\n",
    "def articles_generator(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        for article in ijson.items(file, 'item', use_float=True):\n",
    "            yield article\n",
    "\n",
    "def prepare_author_lists(article):\n",
    "    \"\"\"\n",
    "    Prepare a list of lists for authors\n",
    "    \"\"\"\n",
    "    # Check if 'authors' key exists in the article dictionary\n",
    "    if 'authors' not in article or not article['authors']:\n",
    "        return []  # Return an empty list if there are no authors\n",
    "    try:\n",
    "        # Master list to collect the flattened data\n",
    "        author_lists = []\n",
    "\n",
    "        # Iterate through each author in the authors list\n",
    "        for author in article['authors']:\n",
    "            # Create a new list with _id, title, authors._id, and authors.name\n",
    "            author_data = [\n",
    "                article['_id'],\n",
    "                article['title'],\n",
    "                author['_id'],\n",
    "                author['name']\n",
    "            ]\n",
    "            # Append this list to the master list\n",
    "            author_lists.append(author_data)\n",
    "        return author_lists\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def prepare_article_lists(article):\n",
    "    \"\"\"\n",
    "    Create a new list with _id and title\n",
    "    \"\"\"\n",
    "    article_data = [\n",
    "        article['_id'],\n",
    "        article['title'],\n",
    "    ]\n",
    "    return [article_data]\n",
    "\n",
    "def prepare_references_lists(article):\n",
    "    \"\"\"\n",
    "    Prepare a list of lists of references\n",
    "    \"\"\"\n",
    "    # Check if 'authors' key exists in the article dictionary\n",
    "    if 'references' not in article or not article['references']:\n",
    "        return []  # Return an empty list if there are no authors\n",
    "    try:\n",
    "        # Master list to collect the flattened data\n",
    "        reference_lists = []\n",
    "\n",
    "        # Iterate through each reference in the references list\n",
    "        for references in article['references']:\n",
    "            # Create a new list with _id and references._id\n",
    "            reference_data = [\n",
    "                article['_id'],\n",
    "                references['references']  # Assume each reference has an _id key\n",
    "            ]\n",
    "            # Append this list to the master list\n",
    "            reference_lists.append(reference_data)\n",
    "        return reference_lists\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def preprocess_json(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', line)\n",
    "            line = line.replace('NaN', 'null')\n",
    "            yield line\n",
    "\n",
    "class PreprocessedFile:\n",
    "    def __init__(self, filename):\n",
    "        self.generator = preprocess_json(filename)\n",
    "        self.buffer = ''\n",
    "\n",
    "    def read(self, size=-1):\n",
    "        while size < 0 or len(self.buffer) < size:\n",
    "            try:\n",
    "                self.buffer += next(self.generator)\n",
    "            except StopIteration:\n",
    "                # End of generator, return what's left in the buffer\n",
    "                break\n",
    "        if size < 0:\n",
    "            result, self.buffer = self.buffer, ''\n",
    "        else:\n",
    "            result, self.buffer = self.buffer[:size], self.buffer[size:]\n",
    "        return result\n",
    "\n",
    "def get_cleaned_data(filename):\n",
    "    \"\"\"\n",
    "    Remove NumberInt and NaN from the JSON file and return a generator of objects\n",
    "    \"\"\"\n",
    "    preprocessed_file = PreprocessedFile(filename)\n",
    "    return ijson.parse(preprocessed_file)  # pass the custom file-like object to ijson.parse\n",
    "\n",
    "def process_object(objects):\n",
    "    \"\"\"\n",
    "    Process a single object from the JSON file\n",
    "    \"\"\"\n",
    "    current_item = {}\n",
    "    authors_list = []  # Initialize an empty list to collect authors\n",
    "    references_list = []  # Initialize an empty list to collect references\n",
    "    inside_authors = False  # A flag to check if we are processing the authors field\n",
    "    inside_references = False  # A flag to check if we are processing the references field\n",
    "    for _, event, value in objects:\n",
    "        if event == 'map_key':\n",
    "            current_key = value\n",
    "            if current_key == 'authors':\n",
    "                inside_authors = True  # Set flag to True when entering authors field\n",
    "            elif current_key == 'references':\n",
    "                inside_references = True  # Set flag to True when entering references field\n",
    "            else:\n",
    "                # Reset flags and assign lists to current_item when exiting authors or references fields\n",
    "                if inside_authors:\n",
    "                    inside_authors = False\n",
    "                    current_item['authors'] = authors_list\n",
    "                    authors_list = []  # Reset authors_list for next use\n",
    "                if inside_references:\n",
    "                    inside_references = False\n",
    "                    current_item['references'] = references_list\n",
    "                    references_list = []  # Reset references_list for next use\n",
    "        elif event in ('string', 'number'):\n",
    "            if inside_authors:\n",
    "                # If inside authors, append a new author dictionary to authors_list\n",
    "                authors_list.append({current_key: value})\n",
    "            elif inside_references:\n",
    "                # If inside references, append a new reference dictionary to references_list\n",
    "                references_list.append({current_key: value})\n",
    "            else:\n",
    "                current_item[current_key] = value\n",
    "        elif event == 'start_map':\n",
    "            # If inside authors or references, process the next dictionary\n",
    "            if inside_authors:\n",
    "                authors_list.append(process_object(objects))\n",
    "            elif inside_references:\n",
    "                references_list.append(process_object(objects))\n",
    "            else:\n",
    "                current_item[current_key] = process_object(objects)\n",
    "        elif event == 'end_map':\n",
    "            # Ensure authors and references lists are assigned to current_item if they are the last fields\n",
    "            if inside_authors:\n",
    "                current_item['authors'] = authors_list\n",
    "            if inside_references:\n",
    "                current_item['references'] = references_list\n",
    "            return current_item\n",
    "\n",
    "def main(filename, neo4j_uri, neo4j_user, neo4j_password):\n",
    "    driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            one_article = get_cleaned_data(filename)\n",
    "            for prefix, event, value in one_article:\n",
    "                if event == 'start_map':\n",
    "                    one_article_dict = process_object(one_article)\n",
    "                    # add articles to neo4j\n",
    "                    article_lists = prepare_article_lists(one_article_dict)\n",
    "                    if article_lists:\n",
    "                        send_articles_to_neo4j(session, article_lists)\n",
    "                    # add authors to neo4j\n",
    "                    author_lists = prepare_author_lists(one_article_dict)\n",
    "                    if author_lists:\n",
    "                        send_authors_to_neo4j(session, author_lists)\n",
    "                    # add references to neo4j\n",
    "                    references_lists = prepare_references_lists(one_article_dict)\n",
    "                    if references_lists:\n",
    "                        send_references_to_neo4j(session, references_lists)\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "# Usage\n",
    "filename1 = 'biggertest.json'\n",
    "filename2 = 'dblpv13.json'\n",
    "neo4j_uri = 'bolt://localhost:7687'\n",
    "neo4j_user = 'neo4j'\n",
    "neo4j_password = 'testtest'\n",
    "main(filename2, neo4j_uri, neo4j_user, neo4j_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['53e99784b7602d9701f3e15d', 'Timing yield estimation using statistical static timing analysis', '53f43b03dabfaedce555bf2a', 'Min Pan']\n",
      "['53e99784b7602d9701f3e15d', 'Timing yield estimation using statistical static timing analysis', '53f45ee9dabfaee43ecda842', 'Chris C. N. Chu']\n",
      "['53e99784b7602d9701f3e15d', 'Timing yield estimation using statistical static timing analysis', '53f42e8cdabfaee1c0a4274e', 'Hai Zhou']\n",
      "['53e99784b7602d9701f3e15d', 'Timing yield estimation using statistical static timing analysis', '53e9a8a9b7602d97031f6bb9']\n",
      "['53e99784b7602d9701f3e15d', 'Timing yield estimation using statistical static timing analysis', '599c7b6b601a182cd27360da']\n",
      "['53e99784b7602d9701f3e15d', 'Timing yield estimation using statistical static timing analysis', '53e9b443b7602d9703f3e52b']\n",
      "['53e99784b7602d9701f3e15d', 'Timing yield estimation using statistical static timing analysis', '53e9a6a6b7602d9702fdc57e']\n",
      "['53e99784b7602d9701f3e15d', 'Timing yield estimation using statistical static timing analysis', '599c7b6a601a182cd2735703']\n",
      "['53e99784b7602d9701f3e15d', 'Timing yield estimation using statistical static timing analysis', '53e9aad9b7602d970345afea']\n",
      "['53e99784b7602d9701f3e15d', 'Timing yield estimation using statistical static timing analysis', '5582821f0cf2bf7bae57ac18']\n",
      "['53e99784b7602d9701f3e15d', 'Timing yield estimation using statistical static timing analysis', '5e8911859fced0a24bb9a2ba']\n",
      "['53e99784b7602d9701f3e15d', 'Timing yield estimation using statistical static timing analysis', '53e9b002b7602d9703a5c932']\n"
     ]
    }
   ],
   "source": [
    "# Input dictionary\n",
    "article_dict = {'_id': '53e99784b7602d9701f3e15d',\n",
    "                'title': 'Timing yield estimation using statistical static timing analysis',\n",
    "                'authors': [{'_id': '53f43b03dabfaedce555bf2a', 'name': 'Min Pan'}, {'_id': '53f45ee9dabfaee43ecda842', 'name': 'Chris C. N. Chu'}, {'_id': '53f42e8cdabfaee1c0a4274e', 'name': 'Hai Zhou'}], 'venue': {'_id': '53a72e2020f7420be8c80142', 'name_d': 'International Symposium on Circuits and Systems', 'type': 0, 'raw': 'ISCAS (3)'}, 'year': 2005, 'keywords': 'circuit analysis', 'fos': 'Statistics', 'n_citation': 28, 'page_start': '2461', 'page_end': '2464Vol.3', 'lang': 'en', 'volume': '', 'issue': '', 'issn': '', 'isbn': '0-7803-8834-8', 'doi': '10.1109/ISCAS.2005.1465124', 'pdf': '//static.aminer.org/pdf/PDF/000/423/329/timing_yield_estimation_using_statistical_static_timing_analysis.pdf', 'url': 'http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?tp=&arnumber=1465124', 'abstract': 'As process variations become a significant problem in deep sub-micron technology, a shift from deterministic static timing analysis to statistical static timing analysis for high-performance circuit designs could reduce the excessive conservatism that is built into current timing design methods. We address the timing yield problem for sequential circuits and propose a statistical approach to handle it. We consider the spatial and path reconvergence correlations between path delays, set-up time and hold time constraints, and clock skew due to process variations. We propose a method to get the timing yield based on the delay distributions of register-to-register paths in the circuit On average, the timing yield results obtained by our approach have average errors of less than 1.0% in comparison with Monte Carlo simulation. Experimental results show that shortest path variations and clock skew due to process variations have considerable impact on circuit timing, which could bias the timing yield results. In addition, the correlation between longest and shortest path delays is not significant.', \n",
    "                'references': [{'references': '53e9a8a9b7602d97031f6bb9'},\n",
    "                               {'references': '599c7b6b601a182cd27360da'},\n",
    "                               {'references': '53e9b443b7602d9703f3e52b'},\n",
    "                               {'references': '53e9a6a6b7602d9702fdc57e'}, \n",
    "                               {'references': '599c7b6a601a182cd2735703'}, \n",
    "                               {'references': '53e9aad9b7602d970345afea'}, \n",
    "                               {'references': '5582821f0cf2bf7bae57ac18'}, \n",
    "                               {'references': '5e8911859fced0a24bb9a2ba'}, \n",
    "                               {'references': '53e9b002b7602d9703a5c932'}]}\n",
    "\n",
    "\n",
    "# Master list to collect the flattened data\n",
    "flattened_author = []\n",
    "flattened_references = []\n",
    "\n",
    "# Iterate through each author in the authors list\n",
    "for author in article_dict['authors']:\n",
    "    # Create a new list with _id, title, authors._id, and authors.name\n",
    "    author_data = [\n",
    "        article_dict['_id'],\n",
    "        article_dict['title'],\n",
    "        author['_id'],\n",
    "        author['name']\n",
    "    ]\n",
    "    # Append this list to the master list\n",
    "    flattened_author.append(author_data)  # Corrected here\n",
    "\n",
    "# Output the flattened data for authors\n",
    "for data in flattened_author:\n",
    "    print(data)\n",
    "\n",
    "# Iterate through each reference in the references list\n",
    "for references in article_dict['references']:\n",
    "    # Create a new list with _id, title, and references\n",
    "    reference_data = [\n",
    "        article_dict['_id'],\n",
    "        article_dict['title'],\n",
    "        references['references']\n",
    "    ]\n",
    "    # Append this list to the master list\n",
    "    flattened_references.append(reference_data)  # Corrected here\n",
    "\n",
    "# Output the flattened data for references\n",
    "for data in flattened_references:\n",
    "    print(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "def drop_all_constraints_and_indexes(tx):\n",
    "    \"\"\" \n",
    "    Debugging function to drop all indexes and constraints in the database \n",
    "    \"\"\"\n",
    "    # Get the list of all constraints\n",
    "    result = tx.run(\"SHOW CONSTRAINTS\")\n",
    "    for record in result:\n",
    "        # Get the constraint name\n",
    "        constraint_name = record['name']\n",
    "        # Drop the constraint\n",
    "        tx.run(f\"DROP CONSTRAINT {constraint_name} IF EXISTS\")\n",
    "\n",
    "    # Now that constraints are dropped, get the list of all indexes\n",
    "    result = tx.run(\"SHOW INDEXES\")\n",
    "    for record in result:\n",
    "        # Get the index name\n",
    "        index_name = record['name']\n",
    "        # Drop the index\n",
    "        tx.run(f\"DROP INDEX {index_name} IF EXISTS\")\n",
    "\n",
    "\n",
    "\n",
    "def neo4j_index_constraints(session):\n",
    "    with session.begin_transaction() as tx:\n",
    "        # index\n",
    "        tx.run(\"CREATE INDEX article_title_index FOR (n:Article) ON (n.title)\")\n",
    "        tx.run(\"CREATE INDEX author_name_index FOR (a:Author) ON (a.name)\")\n",
    "        # unique\n",
    "        tx.run(\"CREATE CONSTRAINT article_id_uniqueness FOR (a:Article) REQUIRE (a._id) IS UNIQUE\")\n",
    "        tx.run(\"CREATE CONSTRAINT author_id_uniqueness FOR (a:Author) REQUIRE (a._id) IS UNIQUE\")\n",
    "        # Commit the transaction at the end of the batch\n",
    "        tx.commit()\n",
    "\n",
    "def send_articles_to_neo4j(session, article_lists):\n",
    "    with session.begin_transaction() as tx:\n",
    "        for article in article_lists:\n",
    "            article_id = article[0]\n",
    "            article_title = article[1]\n",
    "            tx.run(\n",
    "                \"MERGE (a:Article {_id: $article_id}) \"\n",
    "                \"ON CREATE SET a.title = $article_title \"\n",
    "                \"ON MATCH SET a.title = $article_title\",\n",
    "                article_id=article_id,\n",
    "                article_title=article_title\n",
    "            )\n",
    "        tx.commit()  # Commit the transaction at the end of the batch\n",
    "\n",
    "def send_authors_to_neo4j(session, author_lists):\n",
    "    with session.begin_transaction() as tx:\n",
    "        for author in author_lists:\n",
    "            article_id = author[0]\n",
    "            article_title = author[1]\n",
    "            author_id = author[2]\n",
    "            author_name = author[3]\n",
    "            \n",
    "            if author_id and author_name:\n",
    "                tx.run(\"\"\"\n",
    "                    MERGE (a:Article {_id: $article_id})\n",
    "                    ON CREATE SET a.title = $article_title\n",
    "                    ON MATCH SET a.title = $article_title\n",
    "                    MERGE (authorNode:Author {_id: $author_id})\n",
    "                    SET authorNode.name = $author_name\n",
    "                    MERGE (authorNode)-[:AUTHORED]->(a)\n",
    "                \"\"\",\n",
    "                author_id=author_id,\n",
    "                author_name=author_name,\n",
    "                article_title=article_title,\n",
    "                article_id=article_id)\n",
    "        tx.commit()  # Commit the transaction at the end of the batch\n",
    "\n",
    "def send_references_to_neo4j(session, reference_lists):\n",
    "    with session.begin_transaction() as tx:\n",
    "        for reference in reference_lists:\n",
    "            article_id = reference[0]\n",
    "            reference_article_id = reference[1]\n",
    "            \n",
    "            if reference_article_id:\n",
    "                tx.run(\"\"\"\n",
    "                    MERGE (a:Article {_id: $article_id})\n",
    "                    MERGE (c:Article {_id: $reference_article_id})\n",
    "                    MERGE (a)-[:CITES]->(c)\n",
    "                \"\"\",\n",
    "                article_id=article_id,\n",
    "                reference_article_id=reference_article_id)\n",
    "        tx.commit()  # Commit the transaction at the end of the batch    \n",
    "\n",
    "def articles_generator(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        for article in ijson.items(file, 'item', use_float=True):\n",
    "            yield article\n",
    "\n",
    "def prepare_author_lists(article):\n",
    "    \"\"\"\n",
    "    Prepare a list of lists for authors\n",
    "    \"\"\"\n",
    "    # Check if 'authors' key exists in the article dictionary\n",
    "    if 'authors' not in article or not article['authors']:\n",
    "        return []  # Return an empty list if there are no authors\n",
    "    try:\n",
    "        # Master list to collect the flattened data\n",
    "        author_lists = []\n",
    "\n",
    "        # Iterate through each author in the authors list\n",
    "        for author in article['authors']:\n",
    "            # Create a new list with _id, title, authors._id, and authors.name\n",
    "            author_data = [\n",
    "                article['_id'],\n",
    "                article['title'],\n",
    "                author['_id'],\n",
    "                author['name']\n",
    "            ]\n",
    "            # Append this list to the master list\n",
    "            author_lists.append(author_data)\n",
    "        return author_lists\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def prepare_article_lists(article):\n",
    "    \"\"\"\n",
    "    Create a new list with _id and title\n",
    "    \"\"\"\n",
    "    article_data = [\n",
    "        article['_id'],\n",
    "        article['title'],\n",
    "    ]\n",
    "    return [article_data]\n",
    "\n",
    "def prepare_references_lists(article):\n",
    "    \"\"\"\n",
    "    Prepare a list of lists of references\n",
    "    \"\"\"\n",
    "    # Check if 'authors' key exists in the article dictionary\n",
    "    if 'references' not in article or not article['references']:\n",
    "        return []  # Return an empty list if there are no authors\n",
    "    try:\n",
    "        # Master list to collect the flattened data\n",
    "        reference_lists = []\n",
    "\n",
    "        # Iterate through each reference in the references list\n",
    "        for references in article['references']:\n",
    "            # Create a new list with _id and references._id\n",
    "            reference_data = [\n",
    "                article['_id'],\n",
    "                references['references']  # Assume each reference has an _id key\n",
    "            ]\n",
    "            # Append this list to the master list\n",
    "            reference_lists.append(reference_data)\n",
    "        return reference_lists\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def preprocess_json(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', line)\n",
    "            line = line.replace('NaN', 'null')\n",
    "            yield line\n",
    "\n",
    "class PreprocessedFile:\n",
    "    def __init__(self, filename):\n",
    "        self.generator = preprocess_json(filename)\n",
    "        self.buffer = ''\n",
    "\n",
    "    def read(self, size=-1):\n",
    "        while size < 0 or len(self.buffer) < size:\n",
    "            try:\n",
    "                self.buffer += next(self.generator)\n",
    "            except StopIteration:\n",
    "                # End of generator, return what's left in the buffer\n",
    "                break\n",
    "        if size < 0:\n",
    "            result, self.buffer = self.buffer, ''\n",
    "        else:\n",
    "            result, self.buffer = self.buffer[:size], self.buffer[size:]\n",
    "        return result\n",
    "\n",
    "def get_cleaned_data(filename):\n",
    "    \"\"\"\n",
    "    Remove NumberInt and NaN from the JSON file and return a generator of objects\n",
    "    \"\"\"\n",
    "    preprocessed_file = PreprocessedFile(filename)\n",
    "    return ijson.parse(preprocessed_file)  # pass the custom file-like object to ijson.parse\n",
    "\n",
    "def process_object(objects):\n",
    "    \"\"\"\n",
    "    Process a single object from the JSON file\n",
    "    \"\"\"\n",
    "    current_item = {}\n",
    "    authors_list = []  # Initialize an empty list to collect authors\n",
    "    references_list = []  # Initialize an empty list to collect references\n",
    "    inside_authors = False  # A flag to check if we are processing the authors field\n",
    "    inside_references = False  # A flag to check if we are processing the references field\n",
    "    for _, event, value in objects:\n",
    "        if event == 'map_key':\n",
    "            current_key = value\n",
    "            if current_key == 'authors':\n",
    "                inside_authors = True  # Set flag to True when entering authors field\n",
    "            elif current_key == 'references':\n",
    "                inside_references = True  # Set flag to True when entering references field\n",
    "            else:\n",
    "                # Reset flags and assign lists to current_item when exiting authors or references fields\n",
    "                if inside_authors:\n",
    "                    inside_authors = False\n",
    "                    current_item['authors'] = authors_list\n",
    "                    authors_list = []  # Reset authors_list for next use\n",
    "                if inside_references:\n",
    "                    inside_references = False\n",
    "                    current_item['references'] = references_list\n",
    "                    references_list = []  # Reset references_list for next use\n",
    "        elif event in ('string', 'number'):\n",
    "            if inside_authors:\n",
    "                # If inside authors, append a new author dictionary to authors_list\n",
    "                authors_list.append({current_key: value})\n",
    "            elif inside_references:\n",
    "                # If inside references, append a new reference dictionary to references_list\n",
    "                references_list.append({current_key: value})\n",
    "            else:\n",
    "                current_item[current_key] = value\n",
    "        elif event == 'start_map':\n",
    "            # If inside authors or references, process the next dictionary\n",
    "            if inside_authors:\n",
    "                authors_list.append(process_object(objects))\n",
    "            elif inside_references:\n",
    "                references_list.append(process_object(objects))\n",
    "            else:\n",
    "                current_item[current_key] = process_object(objects)\n",
    "        elif event == 'end_map':\n",
    "            # Ensure authors and references lists are assigned to current_item if they are the last fields\n",
    "            if inside_authors:\n",
    "                current_item['authors'] = authors_list\n",
    "            if inside_references:\n",
    "                current_item['references'] = references_list\n",
    "            return current_item\n",
    "\n",
    "def main(filename, neo4j_uri, neo4j_user, neo4j_password):\n",
    "    driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        # debug drop all indexes\n",
    "        session.execute_write(drop_all_constraints_and_indexes)\n",
    "        # optimize neo4j\n",
    "        neo4j_index_constraints(session)\n",
    "        # process articles\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            one_article = get_cleaned_data(filename)\n",
    "            # Wrap your loop with tqdm, and specify the total count of articles\n",
    "            for _, event, _ in tqdm(one_article, total=5354309, unit='article'):\n",
    "                if event == 'start_map':\n",
    "                    one_article_dict = process_object(one_article)\n",
    "                    # add authors/articles to neo4j\n",
    "                    author_lists = prepare_author_lists(one_article_dict)\n",
    "                    if author_lists:\n",
    "                        send_authors_to_neo4j(session, author_lists)\n",
    "                    else:\n",
    "                        article_lists = prepare_article_lists(one_article_dict)\n",
    "                        if article_lists:\n",
    "                            send_articles_to_neo4j(session, article_lists)\n",
    "                    # add references to neo4j\n",
    "                    references_lists = prepare_references_lists(one_article_dict)\n",
    "                    if references_lists:\n",
    "                        send_references_to_neo4j(session, references_lists)\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "# Usage\n",
    "filename = 'dblpv13.json'\n",
    "neo4j_uri = \"bolt://localhost:7687\"\n",
    "neo4j_user = 'neo4j'\n",
    "neo4j_password = 'testtest'\n",
    "\n",
    "# start\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"Processing started at {start_time}\")\n",
    "\n",
    "# process articles\n",
    "main(filename, neo4j_uri, neo4j_user, neo4j_password)\n",
    "\n",
    "# end\n",
    "end_time = datetime.datetime.now()  # <-- 3. Get the current time again\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Processing finished at {end_time}. Total time taken: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import hashlib\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def drop_all_constraints_and_indexes(tx):\n",
    "    \"\"\" \n",
    "    Debugging function to drop all indexes and constraints in the database \n",
    "    \"\"\"\n",
    "    # Get the list of all constraints\n",
    "    result = tx.run(\"SHOW CONSTRAINTS\")\n",
    "    for record in result:\n",
    "        # Get the constraint name\n",
    "        constraint_name = record['name']\n",
    "        # Drop the constraint\n",
    "        tx.run(f\"DROP CONSTRAINT {constraint_name} IF EXISTS\")\n",
    "\n",
    "    # Now that constraints are dropped, get the list of all indexes\n",
    "    result = tx.run(\"SHOW INDEXES\")\n",
    "    for record in result:\n",
    "        # Get the index name\n",
    "        index_name = record['name']\n",
    "        # Drop the index\n",
    "        tx.run(f\"DROP INDEX {index_name} IF EXISTS\")\n",
    "\n",
    "def neo4j_index_constraints(session):\n",
    "    with session.begin_transaction() as tx:\n",
    "        # index\n",
    "        tx.run(\"CREATE INDEX article_title_index FOR (n:Article) ON (n.title)\")\n",
    "        tx.run(\"CREATE INDEX author_name_index FOR (a:Author) ON (a.name)\")\n",
    "        # unique\n",
    "        tx.run(\"CREATE CONSTRAINT article_id_uniqueness FOR (a:Article) REQUIRE (a._id) IS UNIQUE\")\n",
    "        tx.run(\"CREATE CONSTRAINT author_id_uniqueness FOR (a:Author) REQUIRE (a._id) IS UNIQUE\")\n",
    "        # Commit the transaction at the end of the batch\n",
    "        tx.commit()\n",
    "\n",
    "def send_articles_to_neo4j(session, article_lists):\n",
    "    # Remove duplicates from article_lists\n",
    "    print(article_lists)\n",
    "    article_lists = remove_duplicates(article_lists)\n",
    "    print(article_lists)\n",
    "    tx = session.begin_transaction()\n",
    "    query = \"\"\"\n",
    "    MERGE (a:Article {article_id: $article_id})\n",
    "    SET a.title = $article_title\n",
    "    \"\"\"\n",
    "    for article_info in article_lists:\n",
    "        try:\n",
    "            article_id, article_title = article_info  # Unpack article_id and article_title from article_info\n",
    "        except ValueError as e:\n",
    "            print(f'Error unpacking article_info: {e}')\n",
    "            continue  # Skip to next iteration if error occurs\n",
    "\n",
    "        # Execute the query to create or update the Article node\n",
    "        tx.run(\n",
    "            query,\n",
    "            article_id=article_id,\n",
    "            article_title=article_title\n",
    "        )\n",
    "    tx.commit()\n",
    "\n",
    "def send_authors_to_neo4j(session, author_lists):\n",
    "    # Remove duplicates from author_lists\n",
    "    author_lists = remove_duplicates(author_lists)\n",
    "    with session.begin_transaction() as tx:\n",
    "        query = \"\"\"\n",
    "                MERGE (a:Article {_id: $article_id})\n",
    "                ON CREATE SET a.title = $article_title\n",
    "                ON MATCH SET a.title = $article_title\n",
    "                MERGE (authorNode:Author {_id: $author_id})\n",
    "                SET authorNode.name = $author_name\n",
    "                MERGE (authorNode)-[:AUTHORED]->(a)\n",
    "                \"\"\"\n",
    "        for author in author_lists:\n",
    "            try:\n",
    "                article_id = author[0]\n",
    "                article_title = author[1]\n",
    "                author_id = author[2]\n",
    "                author_name = author[3]\n",
    "                \n",
    "                if author_id and author_name:\n",
    "                    tx.run(\n",
    "                        query,\n",
    "                        author_id=author_id,\n",
    "                        author_name=author_name,\n",
    "                        article_title=article_title,\n",
    "                        article_id=article_id\n",
    "                        )\n",
    "            except IndexError:\n",
    "                print(f\"Unexpected data structure: {author}\")\n",
    "        tx.commit()  # Commit the transaction at the end of the batch\n",
    "\n",
    "def send_references_to_neo4j(session, reference_lists):\n",
    "    # Remove duplicates from reference_lists\n",
    "    reference_lists = remove_duplicates(reference_lists)\n",
    "    with session.begin_transaction() as tx:\n",
    "        query = \"\"\"\n",
    "                MERGE (a:Article {_id: $article_id})\n",
    "                MERGE (c:Article {_id: $reference_article_id})\n",
    "                MERGE (a)-[:CITES]->(c)\n",
    "                \"\"\"\n",
    "        for reference in reference_lists:\n",
    "            article_id = reference[0]\n",
    "            reference_article_id = reference[1]\n",
    "            \n",
    "            if reference_article_id:\n",
    "                tx.run(\n",
    "                    query,\n",
    "                    article_id=article_id,\n",
    "                    reference_article_id=reference_article_id)\n",
    "        tx.commit()  # Commit the transaction at the end of the batch    \n",
    "\n",
    "def articles_generator(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        for article in ijson.items(file, 'item', use_float=True):\n",
    "            yield article\n",
    "\n",
    "def prepare_author_lists(article):\n",
    "    #print(article)\n",
    "    \"\"\"\n",
    "    Prepare a list of lists for authors\n",
    "    [article_id, article_title, author_id, author_name]\n",
    "    \"\"\"\n",
    "    # Extract the inner dictionary from the 'article' dictionary\n",
    "    inner_dict = article.get(None, {})\n",
    "    if not inner_dict:\n",
    "        return []  # Return an empty list if there are no authors or the inner dictionary is missing\n",
    "    \n",
    "    # Check if 'authors' key exists in the inner dictionary\n",
    "    if 'authors' not in inner_dict or not inner_dict['authors']:\n",
    "        return []  # Return an empty list if there are no authors\n",
    "    \n",
    "    # Master list to collect the flattened data\n",
    "    author_lists = []\n",
    "\n",
    "    # Iterate through each author in the authors list\n",
    "    for author in inner_dict['authors']:\n",
    "        # Create a new list with _id, title, authors._id, and authors.name\n",
    "        author_data = [\n",
    "            inner_dict['_id'],\n",
    "            inner_dict['title'],\n",
    "            author.get('_id', None),  # Use None if '_id' is missing\n",
    "            author.get('name', 'Unknown Author')  # Use a default value if 'name' is missing\n",
    "        ]\n",
    "        # Append this list to the master list\n",
    "        author_lists.append(author_data)\n",
    "    \n",
    "    #print(author_lists)\n",
    "    return author_lists\n",
    "\n",
    "def prepare_article_lists(article):\n",
    "    \"\"\"\n",
    "    Create a new list with _id and title\n",
    "    [article_id, article_title]\n",
    "    \"\"\"\n",
    "    #print(article)\n",
    "    if article is None:\n",
    "        return []  # Return an empty list if the article is None or not properly constructed\n",
    "    try:\n",
    "        # Access the inner dictionary\n",
    "        inner_dict = article[None]\n",
    "        # Create a new list with _id and title\n",
    "        article_data = [\n",
    "            inner_dict['_id'],\n",
    "            inner_dict.get('title', 'Unknown title')  # Use a default value if 'title' is missing\n",
    "        ]\n",
    "        #print([article_data])\n",
    "        return [article_data]\n",
    "    except KeyError:\n",
    "        return []  # Handle the case where _id or title is missing in the article\n",
    "\n",
    "# def prepare_article_no_title_lists(article):\n",
    "#     if article is None:\n",
    "#         return []  # Return an empty list if the article is None or not properly constructed\n",
    "#     try:\n",
    "#         # Create a new list with _id and a default title\n",
    "#         article_no_title_data = [\n",
    "#             article['_id'],\n",
    "#             \"Unknown title\"\n",
    "#         ]\n",
    "#         return [article_no_title_data]\n",
    "#     except KeyError:\n",
    "#         return []  # Handle the case where _id is missing in the article\n",
    "\n",
    "def prepare_references_lists(article):\n",
    "    \"\"\"\n",
    "    Prepare a list of lists of references\n",
    "    \"\"\"\n",
    "    #print(article)\n",
    "    if article is None or None not in article:\n",
    "        return []  # Return an empty list if the article is None or not properly constructed\n",
    "\n",
    "    try:\n",
    "        references_data = []\n",
    "        data = article[None]  # Access the nested dictionary containing the article data\n",
    "        if 'references' in data:\n",
    "            references = data['references']\n",
    "            for reference in references:\n",
    "                if 'references' in reference:\n",
    "                    reference_id = reference['references']\n",
    "                    # Use the '_id' field from the article data as the article_id\n",
    "                    references_data.append([data['_id'], reference_id])\n",
    "\n",
    "        #print(\"Reference list: \", references_data)\n",
    "        return references_data\n",
    "    except KeyError:\n",
    "        return []  # Handle the case where the references structure is invalid\n",
    "\n",
    "def preprocess_json(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            line = re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', line)\n",
    "            line = line.replace('NaN', 'null')\n",
    "            yield line\n",
    "\n",
    "class PreprocessedFile:\n",
    "    def __init__(self, filename):\n",
    "        self.generator = preprocess_json(filename)\n",
    "        self.buffer = ''\n",
    "\n",
    "    def read(self, size=-1):\n",
    "        while size < 0 or len(self.buffer) < size:\n",
    "            try:\n",
    "                self.buffer += next(self.generator)\n",
    "            except StopIteration:\n",
    "                # End of generator, return what's left in the buffer\n",
    "                break\n",
    "        if size < 0:\n",
    "            result, self.buffer = self.buffer, ''\n",
    "        else:\n",
    "            result, self.buffer = self.buffer[:size], self.buffer[size:]\n",
    "        return result\n",
    "\n",
    "def get_cleaned_data(filename):\n",
    "    \"\"\"\n",
    "    Remove NumberInt and NaN from the JSON file and return a generator of objects\n",
    "    \"\"\"\n",
    "    preprocessed_file = PreprocessedFile(filename)\n",
    "    return ijson.parse(preprocessed_file)  # pass the custom file-like object to ijson.parse\n",
    "\n",
    "def process_object(objects):\n",
    "    current_item = {}\n",
    "    authors_list = []\n",
    "    references_list = []\n",
    "    inside_authors = False\n",
    "    inside_references = False\n",
    "    current_key = None  # Initialize current_key\n",
    "\n",
    "    # You can also turn objects into an iterator if it isn't already one\n",
    "    objects_iterator = iter(objects)\n",
    "\n",
    "    for obj in objects_iterator:\n",
    "        if isinstance(obj, tuple) and len(obj) == 3:\n",
    "            _, event, value = obj\n",
    "        else:\n",
    "            print(f\"Unexpected tuple structure: {obj}\")\n",
    "            continue\n",
    "\n",
    "        #print(f\"Processing event: {event}, current_key: {current_key}, value: {value}\")\n",
    "\n",
    "        if event == 'map_key':\n",
    "            current_key = value\n",
    "            if current_key == 'authors':\n",
    "                inside_authors = True\n",
    "            elif current_key == 'references':\n",
    "                inside_references = True\n",
    "            else:\n",
    "                # Reset flags and assign lists to current_item when exiting authors or references fields\n",
    "                if inside_authors:\n",
    "                    inside_authors = False\n",
    "                    current_item['authors'] = authors_list\n",
    "                    authors_list = []  # Reset authors_list for the next use\n",
    "                if inside_references:\n",
    "                    inside_references = False\n",
    "                    current_item['references'] = references_list\n",
    "                    references_list = []  # Reset references_list for the next use\n",
    "\n",
    "        elif event in ('string', 'number'):\n",
    "            if inside_authors:\n",
    "                # If inside authors, append a new author dictionary to authors_list\n",
    "                authors_list.append({current_key: value})\n",
    "            elif inside_references:\n",
    "                # If inside references, append a new reference dictionary to references_list\n",
    "                references_list.append({current_key: value})\n",
    "            else:\n",
    "                current_item[current_key] = value\n",
    "\n",
    "        elif event == 'start_map':\n",
    "            nested_object = process_object(objects_iterator)  # Process the nested object\n",
    "            if inside_authors:\n",
    "                authors_list.append(nested_object)\n",
    "            elif inside_references:\n",
    "                references_list.append(nested_object)\n",
    "            else:\n",
    "                current_item[current_key] = nested_object  # Assign the fully constructed nested object\n",
    "\n",
    "        elif event == 'end_map':\n",
    "            if inside_authors:\n",
    "                current_item['authors'] = authors_list\n",
    "            if inside_references:\n",
    "                current_item['references'] = references_list\n",
    "            return current_item\n",
    "\n",
    "    return current_item  # Return the processed object\n",
    "\n",
    "def hash_list_elements(lst):\n",
    "    # Concatenate elements of the list into a single string\n",
    "    concatenated_str = ''.join(map(str, lst))\n",
    "    # Hash the concatenated string\n",
    "    return hashlib.md5(concatenated_str.encode()).hexdigest()\n",
    "\n",
    "def process_batch(filename, BATCH_SIZE, prev_author_hash, prev_article_hash, prev_references_hash):\n",
    "    processed_articles = 0\n",
    "    author_dict_lists = []\n",
    "    article_dict_lists = []\n",
    "    references_dict_lists = []\n",
    "    \n",
    "    one_article_batch = iter(get_cleaned_data(filename))  # Convert to an iterator\n",
    "\n",
    "    while processed_articles < BATCH_SIZE:\n",
    "        try:\n",
    "            # Gather events for one article\n",
    "            events_for_one_article = []\n",
    "            for event in one_article_batch:\n",
    "                events_for_one_article.append(event)\n",
    "                if event[0] == 'item' and event[1] == 'end_map':\n",
    "                    # End of one article's map\n",
    "                    break\n",
    "\n",
    "            # Process if it's the start of an article\n",
    "            if events_for_one_article and events_for_one_article[0][1] == 'start_map':\n",
    "                one_article_dict = process_object(iter(events_for_one_article))\n",
    "\n",
    "                # Add authors/articles to your data structure here\n",
    "                author_lists = prepare_author_lists(one_article_dict)\n",
    "                if author_lists:\n",
    "                    author_dict_lists.extend(author_lists)\n",
    "\n",
    "                # Add references to your data structure here\n",
    "                references_lists = prepare_references_lists(one_article_dict)\n",
    "                if references_lists:\n",
    "                    references_dict_lists.extend(references_lists)\n",
    "            \n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        processed_articles += 1\n",
    "\n",
    "    # Hash the concatenated elements of the new lists\n",
    "    new_author_hash = hash_list_elements(author_dict_lists)\n",
    "    new_article_hash = hash_list_elements(article_dict_lists)\n",
    "    new_references_hash = hash_list_elements(references_dict_lists)\n",
    "\n",
    "    # Compare the hashes to check for differences\n",
    "    if new_author_hash != prev_author_hash:\n",
    "        # Author lists have different element orders\n",
    "        prev_author_hash = new_author_hash\n",
    "\n",
    "    if new_article_hash != prev_article_hash:\n",
    "        # Article lists have different element orders\n",
    "        prev_article_hash = new_article_hash\n",
    "\n",
    "    if new_references_hash != prev_references_hash:\n",
    "        # References lists have different element orders\n",
    "        prev_references_hash = new_references_hash\n",
    "\n",
    "    return author_dict_lists, article_dict_lists, references_dict_lists\n",
    "\n",
    "\n",
    "def remove_duplicates(lists):\n",
    "    \"\"\"\n",
    "    Remove duplicates from a list of lists\n",
    "    \"\"\"\n",
    "    return [list(item) for item in set(tuple(item) for item in lists)]\n",
    "\n",
    "# Define variables to store the previously processed lists\n",
    "def main(filename, neo4j_uri, neo4j_user, neo4j_password, BATCH_SIZE, TOTAL_ARTICLES):\n",
    "    driver = GraphDatabase.driver(neo4j_uri, auth=(neo4j_user, neo4j_password))\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        # debug drop all indexes\n",
    "        session.execute_write(drop_all_constraints_and_indexes)\n",
    "        # debug drop all tables\n",
    "        session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "        # optimize Neo4j\n",
    "        neo4j_index_constraints(session)\n",
    "\n",
    "        # Initialize tqdm with the total number of articles\n",
    "        with tqdm(total=TOTAL_ARTICLES, unit=' article') as pbar:\n",
    "            prev_author_hash = None\n",
    "            prev_article_hash = None\n",
    "            prev_references_hash = None\n",
    "\n",
    "            while pbar.n < TOTAL_ARTICLES:  # pbar.n keeps track of the number of processed articles\n",
    "                author_dict_lists, article_dict_lists, references_dict_lists = process_batch(filename, BATCH_SIZE, prev_author_hash, prev_article_hash, prev_references_hash)\n",
    "\n",
    "                # Update the tqdm progress bar\n",
    "                pbar.update(BATCH_SIZE)\n",
    "\n",
    "                if author_dict_lists:\n",
    "                    print(author_dict_lists)\n",
    "                    send_authors_to_neo4j(session, author_dict_lists)\n",
    "                    prev_author_hash = hash_list_elements(author_dict_lists)\n",
    "\n",
    "                if article_dict_lists:\n",
    "                    send_articles_to_neo4j(session, article_dict_lists)\n",
    "                    prev_article_hash = hash_list_elements(article_dict_lists)\n",
    "\n",
    "                if references_dict_lists:\n",
    "                    send_references_to_neo4j(session, references_dict_lists)\n",
    "                    prev_references_hash = hash_list_elements(references_dict_lists)\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "# Usage\n",
    "filename = 'dblpv13.json'\n",
    "neo4j_uri = \"bolt://localhost:7687\"\n",
    "neo4j_user = 'neo4j'\n",
    "neo4j_password = 'testtest'\n",
    "BATCH_SIZE = 10\n",
    "TOTAL_ARTICLES = 5354309 \n",
    "\n",
    "# start\n",
    "start_time = datetime.datetime.now()\n",
    "print(f\"Processing started at {start_time}\")\n",
    "\n",
    "# process articles\n",
    "main(filename, neo4j_uri, neo4j_user, neo4j_password, BATCH_SIZE, TOTAL_ARTICLES)\n",
    "\n",
    "# end\n",
    "end_time = datetime.datetime.now()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Processing finished at {end_time}. Total time taken: {elapsed_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/5354 [00:00<1:09:42,  1.28it/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import deque\n",
    "import ijson\n",
    "import pprint\n",
    "from neo4j import GraphDatabase\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import cProfile\n",
    "\n",
    "def main():\n",
    "    # JSON \n",
    "    def preprocess_line(line):\n",
    "        #print(f\"Processing line: {line}\")  # Debugging print\n",
    "        line = re.sub(r'NumberInt\\((\\d+)\\)', r'\\1', line)\n",
    "        return line.replace('NaN', 'null')\n",
    "\n",
    "    def preprocess_json(filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                yield preprocess_line(line)\n",
    "\n",
    "    class PreprocessedFile:\n",
    "        def __init__(self, filename):\n",
    "            self.generator = preprocess_json(filename)\n",
    "            self.buffer = deque()\n",
    "\n",
    "        def read(self, size=-1):\n",
    "            while size < 0 or sum(len(line) for line in self.buffer) < size:\n",
    "                try:\n",
    "                    self.buffer.append(next(self.generator))\n",
    "                except StopIteration:\n",
    "                    # End of generator, return what's left in the buffer\n",
    "                    break\n",
    "            if size < 0:\n",
    "                result = ''.join(self.buffer)\n",
    "                self.buffer.clear()\n",
    "            else:\n",
    "                result_list = []\n",
    "                remaining_size = size\n",
    "                while self.buffer and remaining_size > 0:\n",
    "                    line = self.buffer.popleft()\n",
    "                    result_list.append(line[:remaining_size])\n",
    "                    remaining_size -= len(line)\n",
    "                    if remaining_size < 0:\n",
    "                        self.buffer.appendleft(line[remaining_size:])\n",
    "                result = ''.join(result_list)\n",
    "            return result\n",
    "\n",
    "    def get_cleaned_data(filename):\n",
    "        \"\"\"\n",
    "        Remove NumberInt and NaN from the JSON file and return a generator of objects\n",
    "        \"\"\"\n",
    "        preprocessed_file = PreprocessedFile(filename)\n",
    "        return ijson.parse(preprocessed_file, use_float=True)\n",
    "\n",
    "    def parse_ijson_object(cleaned_data, batch_size):\n",
    "        batch = []  # This will store the current batch of articles\n",
    "        article = {}  # Initialize an empty dictionary for the current article\n",
    "        author = {}  # Temporary storage for author details\n",
    "        is_in_authors = False  # Flag to track whether we're parsing authors\n",
    "        current_key = None  # Variable to keep track of the current key\n",
    "        \n",
    "        for prefix, event, value in cleaned_data:\n",
    "            if event == 'map_key':\n",
    "                current_key = value  # Set the current key\n",
    "                continue  # Skip to the next iteration\n",
    "            \n",
    "            if (prefix, event) == ('item', 'start_map'):\n",
    "                # Start a new article\n",
    "                article = {'authors': [], 'references': []}\n",
    "            elif prefix.startswith('item.authors.item') and event == 'start_map':\n",
    "                # Start a new author\n",
    "                is_in_authors = True\n",
    "                author = {}\n",
    "            elif prefix.startswith('item.authors.item') and event == 'end_map':\n",
    "                # Finish the current author and add to the article's authors list\n",
    "                article['authors'].append(author)\n",
    "                is_in_authors = False\n",
    "            elif event in ('string', 'number'):\n",
    "                if is_in_authors:\n",
    "                    # We are within an author object\n",
    "                    author[current_key] = value\n",
    "                elif prefix == 'item.references.item':\n",
    "                    # Directly append reference IDs to the references list\n",
    "                    article['references'].append(value)\n",
    "                else:\n",
    "                    # For top-level fields, check if they are not part of the excluded fields\n",
    "                    if current_key not in ('keywords', 'fos', 'url'):\n",
    "                        article[current_key] = value\n",
    "            elif (prefix, event) == ('item', 'end_map'):\n",
    "                # End of the current article, add to batch\n",
    "                batch.append(article)\n",
    "                if len(batch) == batch_size:\n",
    "                    yield batch\n",
    "                    batch = []\n",
    "\n",
    "        # Yield any remaining articles in the last batch\n",
    "        if batch:\n",
    "            yield batch\n",
    "\n",
    "    # prepare lists\n",
    "    def prepare_data_for_unwind(ijson_articles):\n",
    "        return [\n",
    "            {\n",
    "                \"articleId\": article['_id'],\n",
    "                \"articleTitle\": article.get('title', \"Unknown Title\"),\n",
    "                \"authorId\": author.get('_id', 'Unknown Author ID'),\n",
    "                \"authorName\": author.get('name', 'Unknown Author')\n",
    "            }\n",
    "            for article in ijson_articles if (article_id := article.get('_id')) and article_id != 'null'\n",
    "            for authors in ([article.get('authors', [])] if isinstance(article.get('authors', {}), dict) else article.get('authors', []))\n",
    "            for author in (authors if isinstance(authors, list) and authors else [{'_id': None, 'name': None}])\n",
    "            if isinstance(author, dict) and author.get('_id') is not None\n",
    "        ]\n",
    "\n",
    "    # prepare neo4j\n",
    "    def neo4j_startup(uri, username, password):\n",
    "        def drop_all_constraints_and_indexes(tx):\n",
    "            \"\"\" \n",
    "            Debugging function to drop all indexes and constraints in the database \n",
    "            \"\"\"\n",
    "            # Get the list of all constraints\n",
    "            result = tx.run(\"SHOW CONSTRAINTS\")\n",
    "            for record in result:\n",
    "                # Get the constraint name\n",
    "                constraint_name = record['name']\n",
    "                # Drop the constraint\n",
    "                tx.run(f\"DROP CONSTRAINT {constraint_name} IF EXISTS\")\n",
    "\n",
    "            # Now that constraints are dropped, get the list of all indexes\n",
    "            result = tx.run(\"SHOW INDEXES\")\n",
    "            for record in result:\n",
    "                # Get the index name\n",
    "                index_name = record['name']\n",
    "                # Drop the index\n",
    "                tx.run(f\"DROP INDEX {index_name} IF EXISTS\")\n",
    "\n",
    "        def neo4j_index_constraints(session):\n",
    "            with session.begin_transaction() as tx:\n",
    "                # index\n",
    "                tx.run(\"CREATE INDEX article_title_index FOR (n:Article) ON (n.title)\")\n",
    "                tx.run(\"CREATE INDEX author_name_index FOR (a:Author) ON (a.name)\")\n",
    "                # unique\n",
    "                tx.run(\"CREATE CONSTRAINT article_id_uniqueness FOR (a:Article) REQUIRE (a._id) IS UNIQUE\")\n",
    "                tx.run(\"CREATE CONSTRAINT author_id_uniqueness FOR (a:Author) REQUIRE (a._id) IS UNIQUE\")\n",
    "                # Commit the transaction at the end of the batch\n",
    "                tx.commit()\n",
    "        \n",
    "        # Connect to Neo4j\n",
    "        driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "        \n",
    "        # Start a session and process the data in batches\n",
    "        with driver.session() as session:\n",
    "            # debug drop all indexes\n",
    "            session.execute_write(drop_all_constraints_and_indexes)\n",
    "            # debug drop all tables\n",
    "            session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "            # optimize Neo4j\n",
    "            neo4j_index_constraints(session)\n",
    "        \n",
    "        # Close the driver\n",
    "        driver.close()\n",
    "\n",
    "    # Send to neo4j\n",
    "    def send_authors_to_neo4j(uri, username, password, unwind_list):\n",
    "        # Function to send a single batch to the database\n",
    "        def send_batch(tx, batch):\n",
    "            query = \"\"\"\n",
    "            UNWIND $batch AS row\n",
    "            WITH row WHERE row.articleId IS NOT NULL AND row.authorId IS NOT NULL\n",
    "            MERGE (a:Article {_id: row.articleId})\n",
    "                ON CREATE SET a.title = row.articleTitle\n",
    "\n",
    "            // Handle authors with known IDs and names\n",
    "            FOREACH(ignoreMe IN CASE WHEN row.authorId IS NOT NULL AND row.authorName <> 'Unknown Author' THEN [1] ELSE [] END |\n",
    "                MERGE (author:Author {_id: row.authorId})\n",
    "                    ON CREATE SET author.name = row.authorName\n",
    "                MERGE (author)-[:AUTHORED]->(a)\n",
    "            )\n",
    "\n",
    "            // Handle authors with a name but no ID\n",
    "            FOREACH(ignoreMe IN CASE WHEN row.authorId IS NULL AND row.authorName <> 'Unknown Author' THEN [1] ELSE [] END |\n",
    "                CREATE (author:Author {name: row.authorName})\n",
    "                MERGE (author)-[:AUTHORED]->(a)\n",
    "            )\n",
    "\n",
    "            // Handle articles with \"Unknown Author\"\n",
    "            FOREACH(ignoreMe IN CASE WHEN row.authorName = 'Unknown Author' THEN [1] ELSE [] END |\n",
    "                MERGE (unknownAuthor:Author {name: 'Unknown Author'})\n",
    "                MERGE (unknownAuthor)-[:AUTHORED]->(a)\n",
    "            )\n",
    "            \"\"\"\n",
    "            tx.run(query, batch=batch)\n",
    "\n",
    "\n",
    "\n",
    "        # Connect to Neo4j\n",
    "        driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "        \n",
    "        # Start a session and process the data in batches\n",
    "        with driver.session() as session:\n",
    "            # send the data at once\n",
    "            session.execute_write(send_batch, unwind_list)\n",
    "\n",
    "        # Close the driver\n",
    "        driver.close()\n",
    "\n",
    "\n",
    "    # Usage\n",
    "    filename = 'petitjson.json'\n",
    "    neo4j_uri = \"bolt://localhost:7687\"\n",
    "    neo4j_user = 'neo4j'\n",
    "    neo4j_password = 'testtest'\n",
    "    BATCH_SIZE = 1000\n",
    "    TOTAL_ARTICLES = 5354309 \n",
    "\n",
    "\n",
    "\n",
    "    # Neo4j cleanup and optimization\n",
    "    neo4j_startup(neo4j_uri, neo4j_user, neo4j_password)\n",
    "\n",
    "    # Parse JSON file and get a generator of cleaned data\n",
    "    cleaned_data_generator = get_cleaned_data(filename)\n",
    "\n",
    "    # Wrap your generator with tqdm to create a progress bar\n",
    "    # The total parameter is optional; it provides an estimate of the total number of iterations (useful for generators)\n",
    "    progress_bar = tqdm(parse_ijson_object(cleaned_data_generator, BATCH_SIZE), total=TOTAL_ARTICLES//BATCH_SIZE)\n",
    "\n",
    "    # Process the data in batches\n",
    "    for articles_batch in progress_bar:\n",
    "        # Prepare the data for the UNWIND operation\n",
    "        unwind_list = prepare_data_for_unwind(articles_batch)\n",
    "        #print(f\"Prepared data for batch: {unwind_list}\")  # Debugging line\n",
    "\n",
    "        # Connect to Neo4j and send the author data\n",
    "        if unwind_list:\n",
    "            try:\n",
    "                send_authors_to_neo4j(neo4j_uri, neo4j_user, neo4j_password, unwind_list)\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")  # Debugging line\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import cProfile\n",
    "    cProfile.run('main()', 'profile_stats')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '53e99784b7602d9701f3e133', 'title': 'The relationship between canopy parameters and spectrum of winter wheat under different irrigations in Hebei Province.', 'authors': [{'_id': '53f45728dabfaec09f209538', 'name': 'Peijuan Wang'}, {'_id': '5601754345cedb3395e59457', 'name': 'Jiahua Zhang'}, {'_id': '53f38438dabfae4b34a08928', 'name': 'Donghui Xie'}, {'_id': '5601754345cedb3395e5945a', 'name': 'Yanyan Xu'}, {'_id': '53f43d25dabfaeecd6995149', 'name': 'Yun Xu'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3e151', 'title': 'A solution to the problem of touching and broken characters.', 'authors': [{'_id': '53f46797dabfaeb22f542630', 'name': 'Jairo Rocha'}, {'_id': '54328883dabfaeb4c6a8a699', 'name': 'Theo Pavlidis'}], 'references': ['53e99cf5b7602d97025ace63', '557e8a7a6fee0fe990caa63d', '53e9a96cb7602d97032c459a', '53e9b929b7602d9704515791', '557e59ebf6678c77ea222447']}\n",
      "{'_id': '53e99784b7602d9701f3e15d', 'title': 'Timing yield estimation using statistical static timing analysis', 'authors': [{'_id': '53f43b03dabfaedce555bf2a', 'name': 'Min Pan'}, {'_id': '53f45ee9dabfaee43ecda842', 'name': 'Chris C. N. Chu'}, {'_id': '53f42e8cdabfaee1c0a4274e', 'name': 'Hai Zhou'}], 'references': ['53e9a8a9b7602d97031f6bb9', '599c7b6b601a182cd27360da', '53e9b443b7602d9703f3e52b', '53e9a6a6b7602d9702fdc57e', '599c7b6a601a182cd2735703', '53e9aad9b7602d970345afea', '5582821f0cf2bf7bae57ac18', '5e8911859fced0a24bb9a2ba', '53e9b002b7602d9703a5c932']}\n",
      "{'_id': '53e99784b7602d9701f3e161', 'title': '360°', 'authors': [{'_id': '53f46946dabfaec09f24b4ed', 'name': 'Miguel Palma'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3e162', 'title': '300', 'authors': [{'_id': '53f43d95dabfaedf435b63fa', 'name': 'Maureen Squillace'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3e165', 'title': '34957+70764=105621', 'authors': [{'_id': '54484654dabfae87b7dfc077', 'name': 'Jon G. Hall'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3e922', 'title': 'International Conference on Nano/Micro Engineered and Molecular Systems', 'authors': [{'_id': '53f39e3edabfae4b34aa8c4a', 'name': 'Jungil Park'}, {'_id': '53f431bcdabfaee2a1cb41b5', 'name': 'Sunyoung Ahn'}, {'_id': '53f46ac3dabfaeee22a63eab', 'name': 'Youngmi Kim Pak'}, {'_id': '53f44f6adabfaedf435efcb8', 'name': 'James Jungho Pak'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3e4f4', 'title': '2BTextures', 'authors': [{'_id': '53f45ad4dabfaee1c0b3e206', 'name': 'Bonnie Mitchell'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3eaf2', 'title': '11MonkeysII', 'authors': [{'_id': '53f438d0dabfaeee229c1f1c', 'name': 'Naotaka Tanaka'}, {'_id': '53f47083dabfaeee22a79321', 'name': 'Mio Yamamoto'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3eaf4', 'title': '1may', 'authors': [{'_id': '53f4312edabfaee02ac9110a', 'name': 'Daniel Zdunczyk'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3eaf6', 'title': '1', 'authors': [{'_id': '560175b545cedb3395e59d80', 'name': 'C. Evans'}, {'_id': '53f42d5edabfaedce54c5021', 'name': 'S. Abu Rmeileh'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f177', 'title': '6th', 'authors': [{'_id': '53f42d84dabfaee1c0a355c4', 'name': 'Damien Perritaz'}, {'_id': '53f46fa1dabfaec09f263a80', 'name': 'Christophe Salzmann'}, {'_id': '541023addabfae450f4d5127', 'name': 'Denis Gillet'}, {'_id': '53f42b9fdabfaedf434faea8', 'name': 'Olivier Naef'}, {'_id': '53f454cddabfaec09f2009b7', 'name': 'Jacques Bapst'}, {'_id': '53f47a8ddabfaee43ed42c17', 'name': 'Frédéric Barras'}, {'_id': '54096818dabfae8faa68bc76', 'name': 'Elena Mugellini'}, {'_id': '54865672dabfae8a11fb31aa', 'name': 'Omar Abou Khaled'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f326', 'title': '458nm', 'authors': [{'_id': '53f44a4adabfaedd74dfe5c0', 'name': ' Institute of Animation, Visual Effects and Digital Postproduction'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f327', 'title': '49', 'authors': [{'_id': '53f42fd0dabfaedd74d553b0', 'name': 'Ichiro Iwano'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f4d1', 'title': '893', 'authors': [{'_id': '53f459b8dabfaeecd69f7b93', 'name': 'Anne Brotot'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f4d2', 'title': '8848', 'authors': [{'_id': '53f463b3dabfaee4dc8430d5', 'name': 'Annabel Sebag'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f352', 'title': '4IM.', 'authors': [], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f0b9', 'title': '7373170279850', 'authors': [{'_id': '53f42c7cdabfaedf43509437', 'name': 'Jean-marc Deshouillers'}, {'_id': '53f4d04ddabfaeeee2f8199d', 'name': 'François Hennecart'}, {'_id': '53f431fedabfaee43ec009a6', 'name': 'Bernard Landreau'}, {'_id': '53f46fe4dabfaeee22a770f3', 'name': 'I. Gusti Putu Purnaba'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f8c1', 'title': 'Foreword', 'authors': [{'_id': '53f49508dabfaeb4c677b4a4', 'name': 'Hiromasa Habuchi'}, {'_id': '53f43522dabfaee4dc7780b2', 'name': 'Unknown author'}, {'_id': '560175f745cedb3395e5a530', 'name': 'Unknown author'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f8c3', 'title': 'Forward', 'authors': [{'_id': '53f4d429dabfaeedd3781ccf', 'name': 'J.R. Gat'}, {'_id': '53f459b8dabfaee0d9c00acb', 'name': 'J. Kushnir'}, {'_id': '54093fbcdabfae450f46f78e', 'name': 'A. Nissenbaum'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f8d2', 'title': 'Foreword.', 'authors': [{'_id': '53f43c87dabfaedf435b42c8', 'name': 'McDonald Mark W'}, {'_id': '560175f745cedb3395e5a53c', 'name': 'McMullen Kevin P'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f411', 'title': 'Using XML to Integrate Existing Software Systems into the Web', 'authors': [{'_id': '548a2e3ddabfae9b40134fbc', 'name': 'Harry M. Sneed'}], 'references': ['53e9adbdb7602d97037be8a2', '53e9bb53b7602d9704792f33', '558aa425e4b0b32fcb37fff4', '558abd44e4b031bae1f9653a', '53e9a326b7602d9702c32229', '53e9b1d7b7602d9703c6ce7c', '558a7de784ae84d265bdee99', '53e9ae17b7602d9703828d13', '53e9aa4fb7602d97033bf9ad']}\n",
      "{'_id': '53e99784b7602d9701f3f414', 'title': '90°', 'authors': [{'_id': '53f463b3dabfaee4dc8430d5', 'name': 'Annabel Sebag'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f908', 'title': 'Foreword.', 'authors': [{'_id': '54328b5ddabfaeb4c6a8b965', 'name': 'W.R. Butt'}, {'_id': '53f4d12fdabfaef00ef80dde', 'name': 'D.R. London'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f5fe', 'title': 'Research on resource allocation for multi-tier web applications in a virtualization environment', 'authors': [{'_id': '53f46a22dabfaee0d9c3d5e5', 'name': 'Shuguo Yang'}], 'references': ['53e9a073b7602d9702957efa', '53e9ad87b7602d970377bfb5', '53e9be51b7602d9704b11381', '53e9be04b7602d9704abb31d', '53e9992bb7602d9702169236', '53e998cdb7602d97021044db', '53e9afa6b7602d97039f6054', '53e99822b7602d9702044e60']}\n",
      "{'_id': '53e99784b7602d9701f3f600', 'title': 'BUSTRAP – an efficient travel planner for metropolitans', 'authors': [{'_id': '560175ed45cedb3395e5a3a0', 'name': 'Sandeep Gupta'}, {'_id': '53f42d8bdabfaec22ba1a1e3', 'name': 'M. M. Gore'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f60c', 'title': 'Fairness', 'authors': [{'_id': '53f6339bdabfae2722c7163e', 'name': 'Nissim Francez'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f955', 'title': 'FIRST', 'authors': [{'_id': '53f4395ddabfaee2a1d02a9e', 'name': 'Michael D. Kelly'}, {'_id': '53f3aaabdabfae4b34af5123', 'name': 'Sean J. Geoghegan'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f956', 'title': 'Formatics.', 'authors': [{'_id': '548999a7dabfae9b40134ba3', 'name': 'Paul Beynon-Davies'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f957', 'title': 'Fault', 'authors': [{'_id': '53f3a30fdabfae4b34ac6874', 'name': 'S. V. Sandhya'}, {'_id': '53f42ce6dabfaec09f10dd58', 'name': 'H. A. Sanjay'}, {'_id': '53f45018dabfaee43eca4008', 'name': 'S. J. Netravathi'}, {'_id': '53f454a2dabfaeee22a2db26', 'name': 'M. V. Sowmyashree'}, {'_id': '53f42cecdabfaeb1a7b84856', 'name': 'R. N. Yogeshwari'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f958', 'title': 'Flexonics.', 'authors': [{'_id': '560175f845cedb3395e5a54e', 'name': 'John Canny'}, {'_id': '53f42ef2dabfaee1c0a47ad5', 'name': 'Jeremy Risner'}, {'_id': '540548eddabfae92b41be5a4', 'name': 'Vivek Subramanian'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f959', 'title': 'Frontmatter', 'authors': [{'_id': '53f42cf5dabfaeb2acfe425e', 'name': 'C. J. Gross'}, {'_id': '53f458aadabfaec09f20ed3a', 'name': 'W. Nazarewicz'}, {'_id': '53f45a48dabfaee2a1d82ade', 'name': 'K. P. Rykaczewski'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f95a', 'title': 'FACETS', 'authors': [{'_id': '53f4671fdabfaee43ecf9787', 'name': 'John R. Cary'}, {'_id': '53f36fd8dabfae4b349bd343', 'name': 'Ammar Hakim'}, {'_id': '53f432c5dabfaedf4355a7ff', 'name': 'Mahmood Miah'}, {'_id': '53f428b2dabfaec22b9e0df5', 'name': 'Scott Kruger'}, {'_id': '53f4375ddabfaee1c0aa5fd8', 'name': 'Alexander Pletzer'}, {'_id': '53f4693bdabfaedd74e7532c', 'name': 'Svetlana G. Shasharina'}, {'_id': '53f431f3dabfaeb2ac023e95', 'name': 'Srinath Vadlamani'}, {'_id': '53f4669fdabfaeee22a54505', 'name': 'Ronald Cohen'}, {'_id': '53f47432dabfaeecd6a41d75', 'name': 'Thomas Epperly'}, {'_id': '53f456d3dabfaee02ad5a9ac', 'name': 'Tom Rognlien'}, {'_id': '53f356f2dabfae4b3495fbcb', 'name': 'Alexei Pankin'}, {'_id': '53f45ab1dabfaeb22f511541', 'name': 'Richard Groebner'}, {'_id': '53f4359fdabfaeee229a4700', 'name': 'Satish Balay'}, {'_id': '53f44990dabfaee4dc7ddf84', 'name': 'Lois C. McInnes'}, {'_id': '562cb37445cedb3398c9befe', 'name': 'Hong Zhang'}], 'references': None}\n",
      "{'_id': '53e99784b7602d9701f3f95b', 'title': 'Fisherfaces', 'authors': [{'_id': '53f46eefdabfaee02adb698d', 'name': 'Aleix Martinez'}], 'references': None}\n"
     ]
    }
   ],
   "source": [
    "import simdjson\n",
    "\n",
    "def get_cleaned_data(filename):\n",
    "    parser = simdjson.Parser()\n",
    "    with open(filename, 'rb') as f:\n",
    "        parsed_json = parser.parse(f.read(), recursive=True)\n",
    "    return parsed_json\n",
    "\n",
    "def extract_articles(doc):\n",
    "    # Initialize an empty list to store the cleaned articles\n",
    "    cleaned_articles = []\n",
    "\n",
    "    # Loop through each article in the document\n",
    "    for article in doc:\n",
    "        # Skip the article if it has no '_id' or if it has no 'authors' or if authors have no '_id'\n",
    "        if \"_id\" not in article or \"authors\" not in article or not all(\"_id\" in author for author in article[\"authors\"]):\n",
    "            continue\n",
    "\n",
    "        # Create a dictionary for the cleaned article\n",
    "        cleaned_article = {\n",
    "            \"_id\": article[\"_id\"],\n",
    "            \"title\": article.get(\"title\", \"Unknown title\"),  # Default title if not present\n",
    "            # Create the list of authors with checks for '_id' and 'name'\n",
    "            \"authors\": [{\"_id\": author[\"_id\"], \"name\": author.get(\"name\", \"Unknown author\")} for author in article[\"authors\"]],\n",
    "            # Get the references if they exist, otherwise default to None\n",
    "            \"references\": article.get(\"references\", None)\n",
    "        }\n",
    "        # Add the cleaned article to the list\n",
    "        cleaned_articles.append(cleaned_article)\n",
    "    \n",
    "    return cleaned_articles\n",
    "\n",
    "def main():\n",
    "    filename = 'petitjson.json'\n",
    "    doc = get_cleaned_data(filename)\n",
    "    articles = extract_articles(doc)\n",
    "    for article in articles:\n",
    "        print(article)  # Or process the articles as needed\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedSymbol",
     "evalue": "Unexpected symbol 'N' at 103",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\backends\\python.py:225\u001b[0m, in \u001b[0;36mparse_value\u001b[1;34m(target, multivalue, use_float)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     number \u001b[39m=\u001b[39m to_number(symbol)\n\u001b[0;32m    226\u001b[0m     \u001b[39mif\u001b[39;00m number \u001b[39m==\u001b[39m inf:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\common.py:199\u001b[0m, in \u001b[0;36minteger_or_decimal\u001b[1;34m(str_value)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m str_value \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m str_value \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mE\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m str_value):\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39;49m(str_value)\n\u001b[0;32m    200\u001b[0m \u001b[39mreturn\u001b[39;00m decimal\u001b[39m.\u001b[39mDecimal(str_value)\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'N'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnexpectedSymbol\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\tests\\notebook_tests.ipynb Cellule 35\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X50sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdblpv13.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X50sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Process the file\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X50sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m process_file(filename)\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\tests\\notebook_tests.ipynb Cellule 35\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X50sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(filename, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X50sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Assuming the JSON data is a single array of objects\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X50sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     objects \u001b[39m=\u001b[39m ijson\u001b[39m.\u001b[39mitems(f, \u001b[39m'\u001b[39m\u001b[39mitem\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X50sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m objects:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X50sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_id: \u001b[39m\u001b[39m{\u001b[39;00mobj[\u001b[39m\"\u001b[39m\u001b[39m_id\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, title: \u001b[39m\u001b[39m{\u001b[39;00mobj[\u001b[39m\"\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\utils.py:55\u001b[0m, in \u001b[0;36mcoros2gen\u001b[1;34m(source, *coro_pipeline)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m source:\n\u001b[0;32m     54\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m         f\u001b[39m.\u001b[39;49msend(value)\n\u001b[0;32m     56\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m         \u001b[39mfor\u001b[39;00m event \u001b[39min\u001b[39;00m events:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\backends\\python.py:44\u001b[0m, in \u001b[0;36mutf8_encoder\u001b[1;34m(target)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[39mraise\u001b[39;00m common\u001b[39m.\u001b[39mIncompleteJSONError(e)\n\u001b[0;32m     43\u001b[0m \u001b[39mif\u001b[39;00m sdata:\n\u001b[1;32m---> 44\u001b[0m     send(sdata)\n\u001b[0;32m     45\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m bdata:\n\u001b[0;32m     46\u001b[0m     target\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\backends\\python.py:103\u001b[0m, in \u001b[0;36mLexer\u001b[1;34m(target)\u001b[0m\n\u001b[0;32m    101\u001b[0m             match \u001b[39m=\u001b[39m LEXEME_RE\u001b[39m.\u001b[39msearch(buf, pos)\n\u001b[0;32m    102\u001b[0m             lexeme \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mgroup()\n\u001b[1;32m--> 103\u001b[0m         send((discarded \u001b[39m+\u001b[39;49m match\u001b[39m.\u001b[39;49mstart(), lexeme))\n\u001b[0;32m    104\u001b[0m         pos \u001b[39m=\u001b[39m match\u001b[39m.\u001b[39mend()\n\u001b[0;32m    105\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    106\u001b[0m     \u001b[39m# Don't ask data from an already exhausted source\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\backends\\python.py:231\u001b[0m, in \u001b[0;36mparse_value\u001b[1;34m(target, multivalue, use_float)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtrue\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mstartswith(symbol) \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mfalse\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mstartswith(symbol) \u001b[39mor\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mnull\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mstartswith(symbol):\n\u001b[0;32m    230\u001b[0m         \u001b[39mraise\u001b[39;00m common\u001b[39m.\u001b[39mIncompleteJSONError(\u001b[39m'\u001b[39m\u001b[39mIncomplete JSON content\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 231\u001b[0m     \u001b[39mraise\u001b[39;00m UnexpectedSymbol(symbol, pos)\n\u001b[0;32m    232\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     send((\u001b[39m'\u001b[39m\u001b[39mnumber\u001b[39m\u001b[39m'\u001b[39m, number))\n",
      "\u001b[1;31mUnexpectedSymbol\u001b[0m: Unexpected symbol 'N' at 103"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "\n",
    "def process_file(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        # Assuming the JSON data is a single array of objects\n",
    "        objects = ijson.items(f, 'item')\n",
    "        for obj in objects:\n",
    "            print(f'_id: {obj[\"_id\"]}, title: {obj[\"title\"]}')\n",
    "\n",
    "# Specify the file name\n",
    "filename = 'dblpv13.json'\n",
    "\n",
    "# Process the file\n",
    "process_file(filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transaction failed and will be retried in 0.9632741611738925s (The allocation of an extra 2.0 MiB would use more than the limit 537.6 MiB. Currently using 537.0 MiB. dbms.memory.transaction.total.max threshold reached)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374b64d2f052451d90c0946af88d1a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\tests\\notebook_tests.ipynb Cellule 36\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=341'>342</a>\u001b[0m         send_data_to_neo4j(neo4j_uri, neo4j_user, neo4j_password, articles_authors_batch, articles_references_batch)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=343'>344</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=344'>345</a>\u001b[0m     \u001b[39m# run main\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=345'>346</a>\u001b[0m     \u001b[39m#cProfile.run('main()', filename='profile_output.pstats')\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=346'>347</a>\u001b[0m     main()\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\tests\\notebook_tests.ipynb Cellule 36\u001b[0m line \u001b[0;36m3\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=336'>337</a>\u001b[0m article_batches_generator \u001b[39m=\u001b[39m tqdm(article_batches_generator, total\u001b[39m=\u001b[39mtotal_batches)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=338'>339</a>\u001b[0m \u001b[39m# Loop through all the batches from the generator with a progress bar\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=339'>340</a>\u001b[0m \u001b[39mfor\u001b[39;00m articles_authors_batch, articles_references_batch \u001b[39min\u001b[39;00m article_batches_generator:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=340'>341</a>\u001b[0m     \u001b[39m#print(articles_references_batch)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=341'>342</a>\u001b[0m     send_data_to_neo4j(neo4j_uri, neo4j_user, neo4j_password, articles_authors_batch, articles_references_batch)\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\tqdm\\notebook.py:249\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[1;32m--> 249\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[0;32m    250\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    251\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m    252\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\tqdm\\std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1179\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1181\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1182\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1183\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1184\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\tests\\notebook_tests.ipynb Cellule 36\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=209'>210</a>\u001b[0m             references_batch_chunk\u001b[39m.\u001b[39mappend(references_data)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=211'>212</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m authors_batch_chunk, references_batch_chunk\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=213'>214</a>\u001b[0m \u001b[39mfor\u001b[39;00m articles_chunk \u001b[39min\u001b[39;00m chunked_iterable(cleaned_data, batch_size):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=214'>215</a>\u001b[0m     articles_authors_batch, articles_references_batch \u001b[39m=\u001b[39m process_articles_chunk(articles_chunk)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=216'>217</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(articles_authors_batch) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m batch_size:\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\tests\\notebook_tests.ipynb Cellule 36\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchunked_iterable\u001b[39m(iterable, size):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(iterable)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=159'>160</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m chunk \u001b[39m:=\u001b[39m \u001b[39mlist\u001b[39;49m(islice(it, size)):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m         \u001b[39myield\u001b[39;00m chunk\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\utils.py:53\u001b[0m, in \u001b[0;36mcoros2gen\u001b[1;34m(source, *coro_pipeline)\u001b[0m\n\u001b[0;32m     51\u001b[0m f \u001b[39m=\u001b[39m chain(events, \u001b[39m*\u001b[39mcoro_pipeline)\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 53\u001b[0m     \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m source:\n\u001b[0;32m     54\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m             f\u001b[39m.\u001b[39msend(value)\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\common.py:219\u001b[0m, in \u001b[0;36mfile_source\u001b[1;34m(f, buf_size)\u001b[0m\n\u001b[0;32m    217\u001b[0m f \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mbytes_reader(f)\n\u001b[0;32m    218\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 219\u001b[0m     data \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mread(buf_size)\n\u001b[0;32m    220\u001b[0m     \u001b[39myield\u001b[39;00m data\n\u001b[0;32m    221\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\site-packages\\ijson\\compat.py:32\u001b[0m, in \u001b[0;36mutf8reader.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\u001b[39mself\u001b[39m, n):\n\u001b[1;32m---> 32\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstr_reader\u001b[39m.\u001b[39;49mread(n)\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\tests\\notebook_tests.ipynb Cellule 36\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mwhile\u001b[39;00m size \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39msum\u001b[39m(\u001b[39mlen\u001b[39m(line) \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer) \u001b[39m<\u001b[39m size:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerator))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m         \u001b[39mbreak\u001b[39;00m  \u001b[39m# End of generator, return what's left in the buffer\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\denis.iglesias\\Documents\\GitHub\\TSM-AdvDaBa_TP22\\tests\\notebook_tests.ipynb Cellule 36\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m buffer \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     chunk \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39;49mread(buffer_size)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunk:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/denis.iglesias/Documents/GitHub/TSM-AdvDaBa_TP22/tests/notebook_tests.ipynb#X51sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         \u001b[39mif\u001b[39;00m buffer:\n",
      "File \u001b[1;32mc:\\Users\\denis.iglesias\\Miniconda3\\envs\\neo4j\\lib\\codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_buffer_decode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, errors, final):\n\u001b[0;32m    315\u001b[0m     \u001b[39m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[39m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[0;32m    317\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[1;32m--> 319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[0;32m    322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer_decode(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors, final)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import ijson\n",
    "from neo4j import GraphDatabase\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import islice\n",
    "import cProfile\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def main():\n",
    "    # # JSON \n",
    "    # def preprocess_line(line):\n",
    "    #     return line.replace('NaN', 'null').replace('NumberInt(', '').replace(')', '')\n",
    "\n",
    "    # def preprocess_json(filename):\n",
    "    #     with open(filename, 'r', encoding='utf-8') as file:\n",
    "    #         for line in file:\n",
    "    #             yield preprocess_line(line)\n",
    "\n",
    "    # class PreprocessedFile:\n",
    "    #     def __init__(self, filename):\n",
    "    #         self.generator = preprocess_json(filename)\n",
    "    #         self.buffer = deque()\n",
    "\n",
    "    #     def read(self, size=-1):\n",
    "    #         while size < 0 or sum(len(line) for line in self.buffer) < size:\n",
    "    #             try:\n",
    "    #                 self.buffer.append(next(self.generator))\n",
    "    #             except StopIteration:\n",
    "    #                 # End of generator, return what's left in the buffer\n",
    "    #                 break\n",
    "    #         if size < 0:\n",
    "    #             result = ''.join(self.buffer)\n",
    "    #             self.buffer.clear()\n",
    "    #         else:\n",
    "    #             result_list = []\n",
    "    #             remaining_size = size\n",
    "    #             while self.buffer and remaining_size > 0:\n",
    "    #                 line = self.buffer.popleft()\n",
    "    #                 result_list.append(line[:remaining_size])\n",
    "    #                 remaining_size -= len(line)\n",
    "    #                 if remaining_size < 0:\n",
    "    #                     self.buffer.appendleft(line[remaining_size:])\n",
    "    #             result = ''.join(result_list)\n",
    "    #         return result\n",
    "\n",
    "    # def get_cleaned_data(filename):\n",
    "    #     \"\"\"\n",
    "    #     Remove NumberInt and NaN from the JSON file and return a generator of objects\n",
    "    #     \"\"\"\n",
    "    #     preprocessed_file = PreprocessedFile(filename)\n",
    "    #     return ijson.items(preprocessed_file, 'item')\n",
    "\n",
    "    def preprocess_line(line):\n",
    "        return line.replace('NaN', 'null').replace('NumberInt(', '').replace(')', '')\n",
    "\n",
    "    def preprocess_json(filename, buffer_size):\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            buffer = ''\n",
    "            while True:\n",
    "                chunk = file.read(buffer_size)\n",
    "                if not chunk:\n",
    "                    if buffer:\n",
    "                        yield preprocess_line(buffer)\n",
    "                    break  # End of file\n",
    "                buffer += chunk\n",
    "                while '\\n' in buffer:\n",
    "                    line, buffer = buffer.split('\\n', 1)\n",
    "                    yield preprocess_line(line)\n",
    "\n",
    "    class PreprocessedFile:\n",
    "        def __init__(self, filename, buffer_size=65536):  # buffer_size is in bytes\n",
    "            self.generator = preprocess_json(filename, buffer_size)\n",
    "            self.buffer = deque()\n",
    "\n",
    "        def read(self, size=-1):\n",
    "            while size < 0 or sum(len(line) for line in self.buffer) < size:\n",
    "                try:\n",
    "                    self.buffer.append(next(self.generator))\n",
    "                except StopIteration:\n",
    "                    break  # End of generator, return what's left in the buffer\n",
    "            if size < 0:\n",
    "                result = ''.join(self.buffer)\n",
    "                self.buffer.clear()\n",
    "            else:\n",
    "                result_list = []\n",
    "                remaining_size = size\n",
    "                while self.buffer and remaining_size > 0:\n",
    "                    line = self.buffer.popleft()\n",
    "                    result_list.append(line[:remaining_size])\n",
    "                    remaining_size -= len(line)\n",
    "                    if remaining_size < 0:\n",
    "                        self.buffer.appendleft(line[remaining_size:])\n",
    "                result = ''.join(result_list)\n",
    "            return result\n",
    "\n",
    "    def get_cleaned_data(filename):\n",
    "        preprocessed_file = PreprocessedFile(filename)\n",
    "        return ijson.items(preprocessed_file, 'item')\n",
    "\n",
    "    # def parse_ijson_object(cleaned_data, batch_size):\n",
    "    #     def chunked_iterable(iterable, size):\n",
    "    #         it = iter(iterable)\n",
    "    #         while chunk := list(islice(it, size)):\n",
    "    #             yield chunk\n",
    "\n",
    "    #     def process_articles_chunk(articles_chunk):\n",
    "    #         authors_batch_chunk = []\n",
    "    #         references_batch_chunk = []\n",
    "    #         for article in articles_chunk:\n",
    "    #             article_id = article.get('_id')\n",
    "    #             article_title = article.get('title')\n",
    "    #             if not (article_id and article_title):\n",
    "    #                 continue  # Skip this article if it doesn't have an id or title\n",
    "\n",
    "    #             authors = article.get('authors', [])\n",
    "    #             if authors:\n",
    "    #                 for author in authors:\n",
    "    #                     author_id = author.get('_id')\n",
    "    #                     author_name = author.get('name')\n",
    "    #                     if author_id and author_name:\n",
    "    #                         entry = {\n",
    "    #                             'article': {\n",
    "    #                                 'article_id': article_id,\n",
    "    #                                 'article_title': article_title\n",
    "    #                             },\n",
    "    #                             'authors': [{\n",
    "    #                                 '_id': author_id,\n",
    "    #                                 'name': author_name\n",
    "    #                             }]\n",
    "    #                         }\n",
    "    #                         authors_batch_chunk.append(entry)\n",
    "\n",
    "    #             # Only process the article for references if there are references\n",
    "    #             references = article.get('references', [])\n",
    "    #             if references:\n",
    "    #                 references_data = {\n",
    "    #                     'article_id': article['_id'],\n",
    "    #                     'article_title': article['title'],\n",
    "    #                     'references': references\n",
    "    #                 }\n",
    "    #                 references_batch_chunk.append(references_data)\n",
    "\n",
    "    #         return authors_batch_chunk, references_batch_chunk\n",
    "\n",
    "    #     for articles_chunk in chunked_iterable(cleaned_data, batch_size):\n",
    "    #         articles_authors_batch, articles_references_batch = process_articles_chunk(articles_chunk)\n",
    "\n",
    "    #         if len(articles_authors_batch) >= batch_size:\n",
    "    #             yield articles_authors_batch, articles_references_batch\n",
    "    #             articles_authors_batch = []\n",
    "    #             articles_references_batch = []\n",
    "\n",
    "    #     if articles_authors_batch or articles_references_batch:\n",
    "    #         yield articles_authors_batch, articles_references_batch\n",
    "\n",
    "    def parse_ijson_object(cleaned_data, batch_size):\n",
    "        def chunked_iterable(iterable, size):\n",
    "            it = iter(iterable)\n",
    "            while chunk := list(islice(it, size)):\n",
    "                yield chunk\n",
    "\n",
    "        def trim_article(article):\n",
    "            trimmed_article = {key: article[key] for key in ['_id', 'title', 'references'] if key in article}\n",
    "            authors = article.get('authors')\n",
    "            \n",
    "            if authors and isinstance(authors, list):\n",
    "                trimmed_authors = [{'_id': author['_id'], 'name': author['name']} for author in authors if '_id' in author and 'name' in author]\n",
    "                if trimmed_authors:  # Check if the list is not empty\n",
    "                    trimmed_article['authors'] = trimmed_authors\n",
    "                        \n",
    "            return trimmed_article\n",
    "\n",
    "        def process_articles_chunk(articles_chunk):\n",
    "            articles_chunk = list(map(trim_article, articles_chunk))\n",
    "            authors_batch_chunk = []\n",
    "            references_batch_chunk = []\n",
    "            for article in articles_chunk:\n",
    "                article_id = article.get('_id')\n",
    "                article_title = article.get('title')\n",
    "                if not (article_id and article_title):\n",
    "                    continue  # Skip this article if it doesn't have an id or title\n",
    "\n",
    "                authors = article.get('authors', [])\n",
    "                if authors:\n",
    "                    for author in authors:\n",
    "                        author_id = author.get('_id')\n",
    "                        author_name = author.get('name')\n",
    "                        if author_id and author_name:\n",
    "                            entry = {\n",
    "                                'article': {\n",
    "                                    'article_id': article_id,\n",
    "                                    'article_title': article_title\n",
    "                                },\n",
    "                                'authors': [{\n",
    "                                    '_id': author_id,\n",
    "                                    'name': author_name\n",
    "                                }]\n",
    "                            }\n",
    "                            authors_batch_chunk.append(entry)\n",
    "\n",
    "                # Only process the article for references if there are references\n",
    "                references = article.get('references', [])\n",
    "                if references:\n",
    "                    references_data = {\n",
    "                        'article_id': article['_id'],\n",
    "                        'article_title': article['title'],\n",
    "                        'references': references\n",
    "                    }\n",
    "                    references_batch_chunk.append(references_data)\n",
    "\n",
    "            return authors_batch_chunk, references_batch_chunk\n",
    "\n",
    "        for articles_chunk in chunked_iterable(cleaned_data, batch_size):\n",
    "            articles_authors_batch, articles_references_batch = process_articles_chunk(articles_chunk)\n",
    "\n",
    "            if len(articles_authors_batch) >= batch_size:\n",
    "                yield articles_authors_batch, articles_references_batch\n",
    "                articles_authors_batch = []\n",
    "                articles_references_batch = []\n",
    "\n",
    "        if articles_authors_batch or articles_references_batch:\n",
    "            yield articles_authors_batch, articles_references_batch\n",
    "\n",
    "    # Neo4j\n",
    "    def neo4j_startup(uri, username, password):\n",
    "        def drop_all_constraints_and_indexes(tx):\n",
    "            \"\"\" \n",
    "            Debugging function to drop all indexes and constraints in the database \n",
    "            \"\"\"\n",
    "            # Get the list of all constraints\n",
    "            result = tx.run(\"SHOW CONSTRAINTS\")\n",
    "            for record in result:\n",
    "                # Get the constraint name\n",
    "                constraint_name = record['name']\n",
    "                # Drop the constraint\n",
    "                tx.run(f\"DROP CONSTRAINT {constraint_name} IF EXISTS\")\n",
    "\n",
    "            # Now that constraints are dropped, get the list of all indexes\n",
    "            result = tx.run(\"SHOW INDEXES\")\n",
    "            for record in result:\n",
    "                # Get the index name\n",
    "                index_name = record['name']\n",
    "                # Drop the index\n",
    "                tx.run(f\"DROP INDEX {index_name} IF EXISTS\")\n",
    "\n",
    "        def neo4j_index_constraints(session):\n",
    "            with session.begin_transaction() as tx:\n",
    "                # index\n",
    "                tx.run(\"CREATE INDEX article_title_index FOR (n:Article) ON (n.title)\")\n",
    "                tx.run(\"CREATE INDEX author_name_index FOR (a:Author) ON (a.name)\")\n",
    "                # unique\n",
    "                tx.run(\"CREATE CONSTRAINT article_id_uniqueness FOR (a:Article) REQUIRE (a._id) IS UNIQUE\")\n",
    "                tx.run(\"CREATE CONSTRAINT author_id_uniqueness FOR (a:Author) REQUIRE (a._id) IS UNIQUE\")\n",
    "                # Commit the transaction at the end of the batch\n",
    "                tx.commit()\n",
    "        \n",
    "        # Connect to Neo4j\n",
    "        driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "        \n",
    "        # Start a session and process the data in batches\n",
    "        with driver.session() as session:\n",
    "            # debug drop all indexes\n",
    "            session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "            session.execute_write(drop_all_constraints_and_indexes)\n",
    "            # optimize Neo4j\n",
    "            neo4j_index_constraints(session)\n",
    "        \n",
    "        # Close the driver\n",
    "        driver.close()\n",
    "\n",
    "    def send_data_to_neo4j(uri, username, password, author_lists, references_lists):\n",
    "        # Function to send a single batch to the database\n",
    "        def send_batch_author(tx, authors_batch):\n",
    "            query = \"\"\"\n",
    "            UNWIND $authors_batch AS row\n",
    "            MERGE (a:Article {_id: row.article.article_id})\n",
    "                ON CREATE SET a.title = row.article.article_title\n",
    "            WITH a, row\n",
    "            UNWIND row.authors AS authorData\n",
    "            MERGE (author:Author {_id: authorData._id})\n",
    "                ON CREATE SET author.name = authorData.name\n",
    "            MERGE (author)-[:AUTHORED]->(a)\n",
    "            \"\"\"\n",
    "\n",
    "            tx.run(query, authors_batch=authors_batch)\n",
    "\n",
    "        def send_batch_ref(tx, references_batch):\n",
    "            query = \"\"\"\n",
    "            UNWIND $references_batch AS refRow\n",
    "            MERGE (refArticle:Article {_id: refRow.article_id})\n",
    "                ON CREATE SET refArticle.title = refRow.article_title\n",
    "            WITH refArticle, refRow\n",
    "            UNWIND refRow.references AS reference\n",
    "            MERGE (referredArticle:Article {_id: reference})\n",
    "            MERGE (refArticle)-[:CITES]->(referredArticle)\n",
    "            \"\"\"\n",
    "\n",
    "            tx.run(query, references_batch=references_batch)\n",
    "\n",
    "        # Connect to Neo4j\n",
    "        driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "        \n",
    "        # Start a session and process the data in batches\n",
    "        with driver.session() as session:\n",
    "            if author_lists:\n",
    "                session.execute_write(send_batch_author, author_lists)\n",
    "            if references_lists:\n",
    "                session.execute_write(send_batch_ref, references_lists)\n",
    "\n",
    "        # Close the driver\n",
    "        driver.close()\n",
    "\n",
    "    # Usage\n",
    "    filename = 'dblpv13.json'\n",
    "    neo4j_uri = \"bolt://localhost:7687\"\n",
    "    neo4j_user = 'neo4j'\n",
    "    neo4j_password = 'testtest'\n",
    "    BATCH_SIZE = 10000\n",
    "    TOTAL_ARTICLES = 5354309\n",
    "\n",
    "    # Neo4j cleanup and optimization\n",
    "    neo4j_startup(neo4j_uri, neo4j_user, neo4j_password)\n",
    "\n",
    "    # Parse JSON file and get a generator of cleaned data\n",
    "    cleaned_data_generator = get_cleaned_data(filename)\n",
    "    \n",
    "    # Estimate the total number of batches for the progress bar\n",
    "    total_batches = TOTAL_ARTICLES // BATCH_SIZE\n",
    "    if TOTAL_ARTICLES % BATCH_SIZE != 0:\n",
    "        total_batches += 1  # Account for a partially-filled final batch\n",
    "\n",
    "    # Create the generator\n",
    "    article_batches_generator = parse_ijson_object(cleaned_data_generator, BATCH_SIZE)\n",
    "\n",
    "    # Wrap the generator with tqdm, specifying the total number of batches\n",
    "    article_batches_generator = tqdm(article_batches_generator, total=total_batches)\n",
    "\n",
    "    # Loop through all the batches from the generator with a progress bar\n",
    "    for articles_authors_batch, articles_references_batch in article_batches_generator:\n",
    "        #print(articles_references_batch)\n",
    "        send_data_to_neo4j(neo4j_uri, neo4j_user, neo4j_password, articles_authors_batch, articles_references_batch)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # run main\n",
    "    #cProfile.run('main()', filename='profile_output.pstats')\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5647a5ad30a14022a23fcaeb472e87e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transaction failed and will be retried in 1.0412633066474275s (Couldn't connect to localhost:7687 (resolved to ()):\n",
      "Failed to establish connection to ResolvedIPv6Address(('::1', 7687, 0, 0)) (reason [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée)\n",
      "Failed to establish connection to ResolvedIPv4Address(('127.0.0.1', 7687)) (reason [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée))\n",
      "Transaction failed and will be retried in 2.372332866952515s (Couldn't connect to localhost:7687 (resolved to ()):\n",
      "Failed to establish connection to ResolvedIPv6Address(('::1', 7687, 0, 0)) (reason [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée)\n",
      "Failed to establish connection to ResolvedIPv4Address(('127.0.0.1', 7687)) (reason [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée))\n",
      "Transaction failed and will be retried in 4.369530984257658s (Couldn't connect to localhost:7687 (resolved to ()):\n",
      "Failed to establish connection to ResolvedIPv6Address(('::1', 7687, 0, 0)) (reason [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée)\n",
      "Failed to establish connection to ResolvedIPv4Address(('127.0.0.1', 7687)) (reason [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée))\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import ijson\n",
    "from neo4j import GraphDatabase\n",
    "from tqdm.notebook import tqdm\n",
    "from itertools import islice\n",
    "import cProfile\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def main():\n",
    "    # JSON \n",
    "    def preprocess_line(line):\n",
    "        return line.replace('NaN', 'null').replace('NumberInt(', '').replace(')', '')\n",
    "\n",
    "    def preprocess_json(filename, buffer_size):\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            buffer = ''\n",
    "            while True:\n",
    "                chunk = file.read(buffer_size)\n",
    "                if not chunk:\n",
    "                    if buffer:\n",
    "                        yield preprocess_line(buffer)\n",
    "                    break  # End of file\n",
    "                buffer += chunk\n",
    "                while '\\n' in buffer:\n",
    "                    line, buffer = buffer.split('\\n', 1)\n",
    "                    yield preprocess_line(line)\n",
    "\n",
    "    class PreprocessedFile:\n",
    "        def __init__(self, filename, buffer_size=1048576):  # buffer_size is in bytes\n",
    "            self.generator = preprocess_json(filename, buffer_size)\n",
    "            self.buffer = deque()\n",
    "\n",
    "        def read(self, size=-1):\n",
    "            while size < 0 or sum(len(line) for line in self.buffer) < size:\n",
    "                try:\n",
    "                    self.buffer.append(next(self.generator))\n",
    "                except StopIteration:\n",
    "                    break  # End of generator, return what's left in the buffer\n",
    "            if size < 0:\n",
    "                result = ''.join(self.buffer)\n",
    "                self.buffer.clear()\n",
    "            else:\n",
    "                result_list = []\n",
    "                remaining_size = size\n",
    "                while self.buffer and remaining_size > 0:\n",
    "                    line = self.buffer.popleft()\n",
    "                    result_list.append(line[:remaining_size])\n",
    "                    remaining_size -= len(line)\n",
    "                    if remaining_size < 0:\n",
    "                        self.buffer.appendleft(line[remaining_size:])\n",
    "                result = ''.join(result_list)\n",
    "            return result\n",
    "\n",
    "    def get_cleaned_data(filename):\n",
    "        preprocessed_file = PreprocessedFile(filename)\n",
    "        return ijson.items(preprocessed_file, 'item')\n",
    "\n",
    "    def parse_ijson_object(cleaned_data, batch_size):\n",
    "        def chunked_iterable(iterable, size):\n",
    "            it = iter(iterable)\n",
    "            while chunk := list(islice(it, size)):\n",
    "                yield chunk\n",
    "\n",
    "        def trim_article(article):\n",
    "            trimmed_article = {key: article[key] for key in ['_id', 'title', 'references'] if key in article}\n",
    "            authors = article.get('authors')\n",
    "            \n",
    "            if authors and isinstance(authors, list):\n",
    "                trimmed_authors = [{'_id': author['_id'], 'name': author['name']} for author in authors if '_id' in author and 'name' in author]\n",
    "                if trimmed_authors:  # Check if the list is not empty\n",
    "                    trimmed_article['authors'] = trimmed_authors\n",
    "                        \n",
    "            return trimmed_article\n",
    "\n",
    "        def process_articles_chunk(articles_chunk):\n",
    "            articles_chunk = list(map(trim_article, articles_chunk))\n",
    "            authors_batch_chunk = []\n",
    "            references_batch_chunk = []\n",
    "            for article in articles_chunk:\n",
    "                article_id = article.get('_id')\n",
    "                article_title = article.get('title')\n",
    "                if not (article_id and article_title):\n",
    "                    continue  # Skip this article if it doesn't have an id or title\n",
    "\n",
    "                authors = article.get('authors', [])\n",
    "                if authors:\n",
    "                    for author in authors:\n",
    "                        author_id = author.get('_id')\n",
    "                        author_name = author.get('name')\n",
    "                        if author_id and author_name:\n",
    "                            entry = {\n",
    "                                'article': {\n",
    "                                    'article_id': article_id,\n",
    "                                    'article_title': article_title\n",
    "                                },\n",
    "                                'authors': [{\n",
    "                                    '_id': author_id,\n",
    "                                    'name': author_name\n",
    "                                }]\n",
    "                            }\n",
    "                            authors_batch_chunk.append(entry)\n",
    "\n",
    "                # Only process the article for references if there are references\n",
    "                references = article.get('references', [])\n",
    "                if references:\n",
    "                    references_data = {\n",
    "                        'article_id': article['_id'],\n",
    "                        'article_title': article['title'],\n",
    "                        'references': references\n",
    "                    }\n",
    "                    references_batch_chunk.append(references_data)\n",
    "\n",
    "            return authors_batch_chunk, references_batch_chunk\n",
    "\n",
    "        for articles_chunk in chunked_iterable(cleaned_data, batch_size):\n",
    "            articles_authors_batch, articles_references_batch = process_articles_chunk(articles_chunk)\n",
    "\n",
    "            if len(articles_authors_batch) >= batch_size:\n",
    "                yield articles_authors_batch, articles_references_batch\n",
    "                articles_authors_batch = []\n",
    "                articles_references_batch = []\n",
    "\n",
    "        if articles_authors_batch or articles_references_batch:\n",
    "            yield articles_authors_batch, articles_references_batch\n",
    "\n",
    "    # Neo4j\n",
    "    def neo4j_startup(uri, username, password):\n",
    "        def drop_all_constraints_and_indexes(tx):\n",
    "            \"\"\" \n",
    "            Debugging function to drop all indexes and constraints in the database \n",
    "            \"\"\"\n",
    "            # Get the list of all constraints\n",
    "            result = tx.run(\"SHOW CONSTRAINTS\")\n",
    "            for record in result:\n",
    "                # Get the constraint name\n",
    "                constraint_name = record['name']\n",
    "                # Drop the constraint\n",
    "                tx.run(f\"DROP CONSTRAINT {constraint_name} IF EXISTS\")\n",
    "\n",
    "            # Now that constraints are dropped, get the list of all indexes\n",
    "            result = tx.run(\"SHOW INDEXES\")\n",
    "            for record in result:\n",
    "                # Get the index name\n",
    "                index_name = record['name']\n",
    "                # Drop the index\n",
    "                tx.run(f\"DROP INDEX {index_name} IF EXISTS\")\n",
    "\n",
    "        def neo4j_index_constraints(session):\n",
    "            with session.begin_transaction() as tx:\n",
    "                # index\n",
    "                tx.run(\"CREATE INDEX article_title_index FOR (n:Article) ON (n.title)\")\n",
    "                tx.run(\"CREATE INDEX author_name_index FOR (a:Author) ON (a.name)\")\n",
    "                # unique\n",
    "                tx.run(\"CREATE CONSTRAINT article_id_uniqueness FOR (a:Article) REQUIRE (a._id) IS UNIQUE\")\n",
    "                tx.run(\"CREATE CONSTRAINT author_id_uniqueness FOR (a:Author) REQUIRE (a._id) IS UNIQUE\")\n",
    "                # Commit the transaction at the end of the batch\n",
    "                tx.commit()\n",
    "        \n",
    "        # Connect to Neo4j\n",
    "        driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "        \n",
    "        # Start a session and process the data in batches\n",
    "        with driver.session() as session:\n",
    "            # debug drop all indexes\n",
    "            session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "            session.execute_write(drop_all_constraints_and_indexes)\n",
    "            # optimize Neo4j\n",
    "            neo4j_index_constraints(session)\n",
    "        \n",
    "        # Close the driver\n",
    "        driver.close()\n",
    "\n",
    "    def send_data_to_neo4j(uri, username, password, author_lists, references_lists):\n",
    "        # Function to send a single batch to the database\n",
    "        def send_batch_author(tx, authors_batch):\n",
    "            query = \"\"\"\n",
    "            UNWIND $authors_batch AS row\n",
    "            MERGE (a:Article {_id: row.article.article_id})\n",
    "                ON CREATE SET a.title = row.article.article_title\n",
    "            WITH a, row\n",
    "            UNWIND row.authors AS authorData\n",
    "            MERGE (author:Author {_id: authorData._id})\n",
    "                ON CREATE SET author.name = authorData.name\n",
    "            MERGE (author)-[:AUTHORED]->(a)\n",
    "            \"\"\"\n",
    "\n",
    "            tx.run(query, authors_batch=authors_batch)\n",
    "\n",
    "        def send_batch_ref(tx, references_batch):\n",
    "            query = \"\"\"\n",
    "            UNWIND $references_batch AS refRow\n",
    "            MERGE (refArticle:Article {_id: refRow.article_id})\n",
    "                ON CREATE SET refArticle.title = refRow.article_title\n",
    "            WITH refArticle, refRow\n",
    "            UNWIND refRow.references AS reference\n",
    "            MERGE (referredArticle:Article {_id: reference})\n",
    "            MERGE (refArticle)-[:CITES]->(referredArticle)\n",
    "            \"\"\"\n",
    "\n",
    "            tx.run(query, references_batch=references_batch)\n",
    "\n",
    "        # Connect to Neo4j\n",
    "        driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "        \n",
    "        # Start a session and process the data in batches\n",
    "        with driver.session() as session:\n",
    "            if author_lists:\n",
    "                session.execute_write(send_batch_author, author_lists)\n",
    "            if references_lists:\n",
    "                session.execute_write(send_batch_ref, references_lists)\n",
    "\n",
    "        # Close the driver\n",
    "        driver.close()\n",
    "\n",
    "    # Usage\n",
    "    filename = 'dblpv13.json'\n",
    "    neo4j_uri = \"bolt://localhost:7687\"\n",
    "    neo4j_user = 'neo4j'\n",
    "    neo4j_password = 'testtest'\n",
    "    BATCH_SIZE = 10000\n",
    "    TOTAL_ARTICLES = 5354309\n",
    "\n",
    "    # Neo4j cleanup and optimization\n",
    "    neo4j_startup(neo4j_uri, neo4j_user, neo4j_password)\n",
    "\n",
    "    # Parse JSON file and get a generator of cleaned data\n",
    "    cleaned_data_generator = get_cleaned_data(filename)\n",
    "    \n",
    "    # Estimate the total number of batches for the progress bar\n",
    "    total_batches = TOTAL_ARTICLES // BATCH_SIZE\n",
    "    if TOTAL_ARTICLES % BATCH_SIZE != 0:\n",
    "        total_batches += 1  # Account for a partially-filled final batch\n",
    "\n",
    "    # Create the generator\n",
    "    article_batches_generator = parse_ijson_object(cleaned_data_generator, BATCH_SIZE)\n",
    "\n",
    "    # Wrap the generator with tqdm, specifying the total number of batches\n",
    "    article_batches_generator = tqdm(article_batches_generator, total=total_batches)\n",
    "\n",
    "    # Loop through all the batches from the generator with a progress bar\n",
    "    for articles_authors_batch, articles_references_batch in article_batches_generator:\n",
    "        #print(articles_references_batch)\n",
    "        send_data_to_neo4j(neo4j_uri, neo4j_user, neo4j_password, articles_authors_batch, articles_references_batch)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # run main\n",
    "    #cProfile.run('main()', filename='profile_output.pstats')\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
